<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>YOLO Series</title>
    <link href="/2024/07/23/Yolo/"/>
    <url>/2024/07/23/Yolo/</url>
    
    <content type="html"><![CDATA[<h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h3><p>结构：Backbone+Neck+Head</p><p><img src="/2024/07/23/Yolo/yolov8.png" alt="yolov8结构图"></p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Object Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>YOLO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OpenCV</title>
    <link href="/2024/07/18/OpenCV/"/>
    <url>/2024/07/18/OpenCV/</url>
    
    <content type="html"><![CDATA[<h2 id="OpenCV"><a href="#OpenCV" class="headerlink" title="OpenCV"></a>OpenCV</h2><p><strong>OpenCV</strong>的全称是Open Source Computer Vision Library，是一个跨平台的计算机视觉库。OpenCV是由<a href="https://zh.wikipedia.org/wiki/%E8%8B%B1%E7%89%B9%E5%B0%94">英特尔公司</a>发起并参与开发。用于开发实时的图像处理、计算机视觉以及模式识别程序。该程序库也可以使用英特尔公司的<a href="https://zh.wikipedia.org/w/index.php?title=%E9%9B%86%E6%88%90%E6%80%A7%E8%83%BD%E5%9F%BA%E5%85%83&action=edit&redlink=1">IPP</a>进行加速处理。</p><h3 id="1-模板匹配-Template-Matching"><a href="#1-模板匹配-Template-Matching" class="headerlink" title="1. 模板匹配 Template Matching"></a>1. 模板匹配 Template Matching</h3><p>OpenCV中的模板匹配是一种在图像中搜索和查找模板图像位置的技术。其基本思想是通过移动模板图像并计算每个位置上的相似度来找到最佳匹配位置。</p><h4 id="1-1-原理"><a href="#1-1-原理" class="headerlink" title="1.1 原理"></a>1.1 原理</h4><p>模板匹配的基本原理是滑动窗口法。具体步骤如下：</p><ol><li>将模板图像放在待匹配图像的不同位置。</li><li>计算模板图像与待匹配图像当前窗口的相似度。</li><li>找到相似度最高的位置，该位置即为最佳匹配位置。</li></ol><h4 id="1-2-不同的模板匹配方法"><a href="#1-2-不同的模板匹配方法" class="headerlink" title="1.2 不同的模板匹配方法"></a>1.2 不同的模板匹配方法</h4><p>OpenCV提供了几种不同的模板匹配方法，每种方法都有其独特的相似度计算方式。主要的方法包括：</p><p><strong>cv2.TM_SQDIFF</strong>:</p><blockquote><p>计算平方差（Sum of Squared Differences, SSD）。</p><p>匹配程度是所有方法中最低的。值越小，匹配越好。</p><p>公式: $R(x,y)&#x3D;∑x′,y′(T(x′,y′)−I(x+x′,y+y′))^2$</p><p><img src="/2024/07/18/OpenCV/image-20240717130135699.png" alt="公式"></p></blockquote><p><strong>cv2.TM_SQDIFF_NORMED</strong>:</p><blockquote><p>归一化平方差（Normalized Sum of Squared Differences, NSSD）。</p><p>与TM_SQDIFF相似，但结果被归一化。</p><p>公式: $R(x, y) &#x3D; \frac{\sum_{x’, y’} (T(x’, y’) - I(x + x’, y + y’))^2}{\sqrt{\sum_{x’, y’} T(x’, y’)^2 \cdot \sum_{x’, y’} I(x + x’, y + y’)^2}}$</p></blockquote><p><strong>cv2.TM_CCORR</strong>:</p><blockquote><p>计算相关性（Cross-Correlation, CC）。</p><p>匹配值越大，匹配程度越好。</p><p>公式: $R(x, y) &#x3D; \sum_{x’, y’} (T(x’, y’) \cdot I(x + x’, y + y’))$</p></blockquote><p><strong>cv2.TM_CCORR_NORMED</strong>:</p><blockquote><p>归一化相关性（Normalized Cross-Correlation, NCC）。</p><p>与TM_CCORR相似，但结果被归一化。</p><p>公式: $R(x, y) &#x3D; \frac{\sum_{x’, y’} (T(x’, y’) \cdot I(x + x’, y + y’))}{\sqrt{\sum_{x’, y’} T(x’, y’)^2 \cdot \sum_{x’, y’} I(x + x’, y + y’)^2}}$</p></blockquote><p><strong>cv2.TM_CCOEFF</strong>:</p><blockquote><ul><li>计算相关系数（Cross-Correlation Coefficient, CCC）。</li><li>匹配值越大，匹配程度越好。</li><li>公式: $R(x, y) &#x3D; \sum_{x’, y’} (T(x’, y’) - \bar{T}) \cdot (I(x + x’, y + y’) - \bar{I})$</li><li>其中，$\bar{T}$ 和 $\bar{I}$分别是模板和窗口的平均值。</li></ul></blockquote><p><strong>cv2.TM_CCOEFF_NORMED</strong>:</p><blockquote><p>归一化相关系数（Normalized Cross-Correlation Coefficient, NCCC）。</p><p>与TM_CCOEFF相似，但结果被归一化。</p><p>公式: $R(x, y) &#x3D; \frac{\sum_{x’, y’} (T(x’, y’) - \bar{T}) \cdot (I(x + x’, y + y’) - \bar{I})}{\sqrt{\sum_{x’, y’} (T(x’, y’) - \bar{T})^2 \cdot \sum_{x’, y’} (I(x + x’, y + y’) - \bar{I})^2}}$</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Develop</category>
      
      <category>OpenCV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>OpenCV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Reinforcement Learning</title>
    <link href="/2024/07/18/ReinforcementLearning/"/>
    <url>/2024/07/18/ReinforcementLearning/</url>
    
    <content type="html"><![CDATA[<h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><p><strong>强化学习的目标：找到最优策略</strong></p><p><img src="/2024/07/18/ReinforcementLearning/image-20240715174712469.png" alt="Reinforcement Learning基本脉络"></p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>以grid-world [网格世界] 为例：</p><p><img src="/2024/07/18/ReinforcementLearning/image-20240715181615315.png" alt="grid-world：位置为S1-S9"></p><h4 id="1-State"><a href="#1-State" class="headerlink" title="1. State"></a>1. State</h4><p>agent相对于environment的状态，在grid-world中是指agent的<code>location</code>。</p><blockquote><p>在FrozenLake这个小游戏中就是以0-15表示所处位置来表示State，</p></blockquote><p>更复杂的情况可以其它state，比如agent的速度、加速度等等。把所有状态放到一起就得到了<strong>State space</strong>。表示为$S&#x3D;{s_i} $</p><h4 id="2-Action"><a href="#2-Action" class="headerlink" title="2. Action"></a>2. Action</h4><p>对于每一个状态，有一系列可采取的行动。在grid-world中是指<code>up</code>、<code>down</code>、<code>left</code>、<code>right</code>、<code>stay</code>这五个动作。把所有动作放到一起就得到了<strong>Action space</strong>。特别的是，不同状态下的Action Space是不一样的，因此表示为$A(s_i)&#x3D;{a_i} $</p><h4 id="3-State-transition"><a href="#3-State-transition" class="headerlink" title="3. State transition"></a>3. State transition</h4><p>当采取一个Action时，agent从一个状态<code>s1</code>移动到另一个状态<code>s2</code>。</p><h5 id="3-1-Tabular"><a href="#3-1-Tabular" class="headerlink" title="3.1 Tabular"></a>3.1 <strong>Tabular</strong></h5><p>我们可以用表格的方式来罗列出所有可能性，但是有个局限，它只能表示确定的情况，如果agent向上走可能被弹到s1，s4，甚至被连续弹到s7这些情况，就无法用表格的方法。</p><h5 id="3-2-State-transition-probability"><a href="#3-2-State-transition-probability" class="headerlink" title="3.2 State transition probability"></a>3.2 <strong>State transition probability</strong></h5><p>引入条件概率来描述state transition，下列公式还是用的确定性例子。<br>$$<br>p(s_2|s_1,a2)&#x3D;1 \<br>p(s_i|s_1,a2)&#x3D;0<br>$$</p><h4 id="4-Policy"><a href="#4-Policy" class="headerlink" title="4. Policy"></a>4. Policy</h4><p>告诉agent，如果我在状态$S_i$，我应该采取什么action。</p><p>用数学的表达方式–条件概率来描述它，在强化学习中 $\pi$ 表示为策略，下面的式子表示在$s_1$的状况下，采取各个action的概率：<br>$$<br>\pi(a_1|s_1)&#x3D;0\<br>\pi(a_2|s_1)&#x3D;0.5\<br>\pi(a_3|s_1)&#x3D;0.5\<br>\pi(a_4|s_1)&#x3D;0\<br>\pi(a_5|s_1)&#x3D;0<br>$$</p><h4 id="5-Reward"><a href="#5-Reward" class="headerlink" title="5. Reward"></a>5. Reward</h4><p>采取了action后得到的一个实数，正数表示鼓励，负数表示惩罚。</p><h5 id="5-1-Tabular"><a href="#5-1-Tabular" class="headerlink" title="5.1 Tabular"></a>5.1 <strong>Tabular</strong></h5><p>对于一些确定的状况（执行一个action一定会得到确定的reward），可以用表格表示法。</p><p>例子见补充-【Q表–FrozenLake游戏】</p><h5 id="5-2-Conditional-probability"><a href="#5-2-Conditional-probability" class="headerlink" title="5.2 Conditional probability"></a>5.2 <strong>Conditional probability</strong></h5><p>对于一些不确定的情况，可以用数学的表达方式–条件概率来表示。</p><p>比如当前在$s_1$，如果我们选择了$a_1$，得到reward为-1的概率为：$p(r&#x3D;1|s_1,a_1)$</p><h4 id="6-Trajectory"><a href="#6-Trajectory" class="headerlink" title="6. Trajectory"></a>6. Trajectory</h4><p>表示一条state-action-reward链</p><p><img src="/2024/07/18/ReinforcementLearning/image-20240716140222610.png" alt="state-action-reward"></p><h4 id="7-Return"><a href="#7-Return" class="headerlink" title="7. Return"></a>7. Return</h4><p>是把沿着一条Trajectory所得到的所有reward加起来。</p><p>Discounted Return：</p><p>如果我们到达target（s9）后，策略还在不断运行，此时采取保持不动的动作（s5），Trajectory如下所示：</p><p><img src="/2024/07/18/ReinforcementLearning/image-20240716142143668.png" alt="image-20240716142143668"></p><p>则return为：$0+0+0+1+1+1+1+…+1&#x3D;\infty$，这个return会<strong>发散</strong>。</p><p>Discounted Return引入一个Discounted rate：$\gamma \in [0,1) $<br>$$<br>Discounted Return &#x3D; 0+\gamma0+\gamma^20+\gamma^31+\gamma^41+\dots \<br>&#x3D;\gamma^3(1+\gamma+\gamma^2+\dots)\<br>&#x3D;\gamma^3(\frac{1}{1-\gamma})<br>$$</p><ol><li>使得return不再发散</li><li>平衡更远未来和更近未来得到的reward</li></ol><h4 id="8-Episode"><a href="#8-Episode" class="headerlink" title="8. Episode"></a>8. Episode</h4><p>可以理解为一个包含<code>terminal states</code>的trajectory。</p><h4 id="9-Markov-decision-process（MDP）"><a href="#9-Markov-decision-process（MDP）" class="headerlink" title="9. Markov decision process（MDP）"></a>9. Markov decision process（MDP）</h4><p>Key elements of MDP: </p><ol><li><p>Sets:</p><blockquote><ul><li>State: the set of states $S$</li><li>Action: the set of actions $A$ is associated for state $s\in S$</li><li>Reward: the set of rewards $R(s,a)$</li></ul></blockquote></li><li><p>Probability distribution</p><blockquote><ul><li>State transition probabilty: $p(s’|s,a)$</li><li>Reward probability: $p(r|s,a)$</li></ul></blockquote></li><li><p>Policy</p><blockquote><p>$\pi(a|s)$</p></blockquote></li><li><p>Markov property：简而言之，下一状态只和我当前的状态和选择的动作有关。</p><p><img src="/2024/07/18/ReinforcementLearning/image-20240716144810109.png" alt="Markov property: state and reward"></p></li></ol><p>​当Policy确定时，Markov decision process就是Markov process</p><h3 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h3><h4 id="1-为什么Return是重要的？"><a href="#1-为什么Return是重要的？" class="headerlink" title="1. 为什么Return是重要的？"></a>1. 为什么Return是重要的？</h4><p>以下图为例，s1处有三种策略，哪一种是最好的？直观来说，第一个策略是最好的，因为它不会进入到forbidden area。第二种最差，第三种中间。</p><p><img src="/2024/07/18/ReinforcementLearning/image-20240716174935512.png" alt="s1处的三种Policy"></p><p>那我们能不能<strong>用数学来描述</strong>这一种直观？-答案就是：<strong>Return</strong>。它能够评估策略。<br>$$<br>return_1 &#x3D; 0+\gamma1+\gamma^21+\dots &#x3D; \frac{\gamma}{1-\gamma}\<br>return_2 &#x3D; -1+\gamma1+\gamma^21+\dots &#x3D; -1 + \frac{\gamma}{1-\gamma}\<br>return_3 &#x3D; 0.5(-1 + \frac{\gamma}{1-\gamma})  + 0.5(\frac{\gamma}{1-\gamma})&#x3D;-0.5+\frac{\gamma}{1-\gamma}\<br>$$<br><strong>怎么计算Return？</strong></p><p>上面的计算是基于return的定义。</p><p><img src="/2024/07/18/ReinforcementLearning/image-20240717151717838.png" alt="一条Trajectory"></p><p>这里我们用 $v_i$ 来表示从 $s_i$ 出发的return：</p><p>$$<br>v_1 &#x3D; r_1+\gamma r_2+\gamma^2 r_3+\dots<br>$$<br><strong>Bootstrapping方法</strong>：从不同状态出发得到的return依赖于从其他状态得到的return：</p><p><img src="/2024/07/18/ReinforcementLearning/image-20240717152406412.png" alt="Bootstrapping"></p><p>这个式子看上去很奇怪，我要得到v1就得得到v4，而得到v4又要得到v1，很矛盾，但是用矩阵的思想去解决就很简单：</p><p><img src="/2024/07/18/ReinforcementLearning/image-20240717152635582.png" alt="matrix-vector"></p><p>上述这个式子就是Bellman Equation，但这个是针对确定性情况，对于一般情况，之后会介绍一般化的Bellman Equation。</p><h4 id="2-State-Value"><a href="#2-State-Value" class="headerlink" title="2. State Value"></a>2. State Value</h4><p>首先考虑一个采取动作是 $A_t$ 的单步过程：<br>$$<br>S_t \to R_{t+1},S_{t+1}$$<br>$$<br>这一步中的所有的这些跳跃，都是由<strong>条件概分布</strong>决定的：</p><p><img src="/2024/07/18/ReinforcementLearning/image-20240717153528466.png" alt="probability distributions"></p><p>上述是一个单步的过程，可以延伸到求解一条trajectory的 discounted return ：$G_t$：<br>$$<br>G_t &#x3D; R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dots<br>$$<br><strong>state value就是 $G_t$ 的期望值</strong>，它的计算公式是：<br>$$<br>v_\pi(s)&#x3D;\mathbb{E}[G_t|S_t&#x3D;s]<br>$$<br>他是一个基于状态和策略的函数，其中：</p><ul><li>$v(s)$ 表示从状态 $s$ 出发得到的return；</li><li>$\pi $表示policy</li></ul><h4 id="3-return和state-value有什么区别和联系？"><a href="#3-return和state-value有什么区别和联系？" class="headerlink" title="3. return和state value有什么区别和联系？"></a>3. return和state value有什么区别和联系？</h4><p>return是对单个trajectory，而state value是对多个可行的trajectory得到的return求平均值。</p><h4 id="4-Bellman-Equation"><a href="#4-Bellman-Equation" class="headerlink" title="4. Bellman Equation"></a>4. Bellman Equation</h4><p><strong>如何推导Bellman equation？</strong><br>$$<br>v_\pi(s)&#x3D;\mathbb{E}[G_t|S_t&#x3D;s]&#x3D;\mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t&#x3D;s]&#x3D;\mathbb{E}[R_{t+1}|S_t&#x3D;s]+\gamma \mathbb{E}[G_{t+1}|S_t&#x3D;s]<br>$$<br>在状态 s 下能获得的reward的均值：<br>$$<br>\mathbb{E}[R_{t+1}|S_t&#x3D;s] &#x3D; \sum_a\pi(a|s)\mathbb{E}[R_{t+1}|S_t&#x3D;s,A_t&#x3D;a]&#x3D;\sum_a\pi(a|s)\sum_r p(r|s,a)r<br>$$<br>在状态 s 下能获得的下一个状态 s’ 的return的均值：<br>$$<br>\mathbb{E}[G_{t+1}|S_t&#x3D;s] &#x3D; \sum_{s’}\mathbb{E}[G_{t+1}|S_{t+1}&#x3D;s’]p(s’|s)&#x3D;\sum_{s’}v_\pi(s’)\sum_ap(s’|s,a)\pi(a|s)<br>$$<br><strong>最终的Bellman equation如下所示：它描述了不同状态的state value之间的关系：</strong></p><blockquote><p>它并不只是一个”式子“，如果我们有n个状态，最终我们就会有n个这样的式子，通过这n个式子我们就能把state value求解出来</p></blockquote><p><img src="/2024/07/18/ReinforcementLearning/image-20240717165709791.png" alt="Bellman equation"></p><h4 id="5-Bellman-Equation：Matrix-vector-form"><a href="#5-Bellman-Equation：Matrix-vector-form" class="headerlink" title="5. Bellman Equation：Matrix-vector form"></a>5. Bellman Equation：Matrix-vector form</h4><p><strong>如何求解Bellman Equation？</strong><br>$$<br>v_\pi(s_i)&#x3D;r_\pi(s_i)+\gamma\sum_{s_j}p_\pi(s_j|s_i)v_\pi(s_j)&#x3D;r_\pi+\gamma P_\pi v_\pi<br>$$<br><img src="/2024/07/18/ReinforcementLearning/image-20240717174251267.png" alt="Bellman Equation"></p><p>用矩阵向量的形式表示bellman equation的求解过程，下面有一个例子：</p><p><img src="/2024/07/18/ReinforcementLearning/image-20240717174647038.png" alt="image-20240717174647038"></p><p><strong>求解State Value:</strong></p><p>为什么要求解state value？</p><blockquote><p><strong>policy evaluation</strong>：给定一个policy，求解出对应的state values叫做policy evaluation。</p><p>通过state value来评估策略的好坏，我们才能进一步改进策略，所以求解bellman equation进而求解state value是十分重要的。</p></blockquote><p>怎么求解state value？</p><blockquote><p>$v_\pi&#x3D;r_\pi+\gamma P_\pi v_\pi \to v_\pi&#x3D;(I-\gamma P_\pi)^{-1}r_\pi$，我们可以通过求逆矩阵来求解，但是在实际中，当状态空间比较大时，矩阵的维数也比较大，求逆计算量高。</p><p>在实际中我们经常用一种迭代的方法：$v_{k+1}&#x3D;r_\pi+\gamma P_\pi v_k$，当$k\to\infty$时，$x_k\to v_\pi$，$v_\pi$就是真实的state value</p></blockquote><p>在强化学习中，特别是对于马尔可夫决策过程（Markov Decision Process, MDP），状态值函数 ( V(s) ) 的计算可以通过迭代方法来进行，这种方法称为<strong>值迭代</strong>（Value Iteration）或<strong>策略迭代</strong>（Policy Iteration）。迭代法的基本思想是逐步更新状态值，直到收敛到一个稳定的值，这比直接求解逆矩阵更高效，尤其是在状态空间较大时。</p><h3 id="值迭代法"><a href="#值迭代法" class="headerlink" title="值迭代法"></a>值迭代法</h3><p>值迭代法是一种迭代算法，用于找到最优值函数 ( V^* ) 和最优策略。其基本思想是反复应用Bellman方程更新每个状态的值，直到值函数收敛。</p><p>Bellman最优方程定义为：<br>[ V^*(s) &#x3D; \max_{a \in \mathcal{A}} \left[ R(s, a) + \gamma \sum_{s’} P(s’ | s, a) V^*(s’) \right] ]</p><p>值迭代法的步骤如下：</p><ol><li><p><strong>初始化值函数：</strong><br>初始化每个状态 ( s ) 的值 ( V(s) )，可以全部设为零或其他初始值。</p></li><li><p><strong>更新值函数：</strong><br>对每个状态 ( s ) ，根据当前的值函数 ( V(s) ) ，使用Bellman方程更新：<br>[ V(s) \leftarrow \max_{a \in \mathcal{A}} \left[ R(s, a) + \gamma \sum_{s’} P(s’ | s, a) V(s’) \right] ]</p></li><li><p><strong>检查收敛性：</strong><br>重复步骤2，直到值函数的变化小于一个预设的阈值，即：<br>[ \max_{s} |V_{new}(s) - V_{old}(s)| &lt; \epsilon ]</p></li><li><p><strong>提取最优策略：</strong><br>一旦值函数收敛，最优策略可以通过选择每个状态 ( s ) 下的最优动作 ( a ) 得到：<br>[ \pi^*(s) &#x3D; \arg\max_{a \in \mathcal{A}} \left[ R(s, a) + \gamma \sum_{s’} P(s’ | s, a) V^*(s’) \right] ]</p></li></ol><h3 id="策略迭代法"><a href="#策略迭代法" class="headerlink" title="策略迭代法"></a>策略迭代法</h3><p>策略迭代法通过反复执行策略评估和策略改进两个步骤来找到最优策略。</p><ol><li><p><strong>策略评估：</strong><br>给定一个策略 ( \pi ) ，计算其对应的值函数 ( V^\pi(s) ) ，直到收敛。使用Bellman期望方程进行更新：<br>[ V^\pi(s) &#x3D; R(s, \pi(s)) + \gamma \sum_{s’} P(s’ | s, \pi(s)) V^\pi(s’) ]</p></li><li><p><strong>策略改进：</strong><br>给定当前的值函数 ( V^\pi ) ，更新策略，使得每个状态下选择的动作最大化值函数：<br>[ \pi_{new}(s) &#x3D; \arg\max_{a \in \mathcal{A}} \left[ R(s, a) + \gamma \sum_{s’} P(s’ | s, a) V^\pi(s’) \right] ]</p></li><li><p><strong>重复上述步骤：</strong><br>反复执行策略评估和策略改进，直到策略不再改变，即达到最优策略。</p></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>迭代法（无论是值迭代还是策略迭代）通过逐步更新状态值或策略，最终收敛到最优解。这种方法避免了直接求解逆矩阵带来的巨大计算量，特别适用于状态空间较大的情况。</p><p>例如，值迭代法通过反复应用Bellman最优方程，逐步逼近最优值函数，而策略迭代法通过交替进行策略评估和策略改进，最终找到最优策略。这两种迭代方法在实践中广泛应用于强化学习任务中。</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><h3 id="Q表–FrozenLake游戏"><a href="#Q表–FrozenLake游戏" class="headerlink" title="Q表–FrozenLake游戏"></a>Q表–FrozenLake游戏</h3><p>关于Q表的实际应用，以FrozenLake这一个小游戏为例：使用ε-贪心策略选择动作 <code>a</code>。该策略在探索和利用之间进行权衡。</p><p>![FrozenLake](ReinforcementLearning&#x2F;屏幕截图 2024-07-16 110659.png)</p><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gymnasium <span class="hljs-keyword">as</span> gym<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> time<br><br>env = gym.make(<span class="hljs-string">&#x27;FrozenLake-v1&#x27;</span>, render_mode = <span class="hljs-string">&#x27;human&#x27;</span>)<br>s = env.reset()<br>env.render()<br><br><span class="hljs-comment">#Epsilon-Greedy approach for Exploration and Exploitation of the state-action spaces</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">epsilon_greedy</span>(<span class="hljs-params">Q,s,na</span>):<br>    epsilon = <span class="hljs-number">0.1</span><br>    p = np.random.uniform(low=<span class="hljs-number">0</span>,high=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">if</span> p &gt; epsilon:<br>        <span class="hljs-comment"># say here,initial policy = for each state consider the action having highest Q-value</span><br>        <span class="hljs-keyword">return</span> np.argmax(Q[s,:])<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> env.action_space.sample()<br><br><span class="hljs-comment"># Q-Learning Implementation</span><br><span class="hljs-comment"># Initializing Q-table with zeros</span><br>Q = np.zeros([env.observation_space.n,env.action_space.n])<br><br><span class="hljs-comment"># set hyperparameters</span><br>lr = <span class="hljs-number">0.5</span> <span class="hljs-comment">#learning rate</span><br>y = <span class="hljs-number">0.9</span> <span class="hljs-comment">#discount factor lambda</span><br>eps = <span class="hljs-number">100000</span> <span class="hljs-comment">#total episodes being 100000</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(eps):<br>    s = env.reset()[<span class="hljs-number">0</span>]<br>    t = <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;当前迭代数：<span class="hljs-subst">&#123;i&#125;</span>&quot;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;当前Q值表：<span class="hljs-subst">&#123;Q&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">while</span>(<span class="hljs-literal">True</span>):<br>        a = epsilon_greedy(Q,s,env.action_space.n)<br>        s_,r,t,_,_ = env.step(a)<br>        <span class="hljs-keyword">if</span> (r==<span class="hljs-number">0</span>):<br>            <span class="hljs-keyword">if</span> t==<span class="hljs-literal">True</span>:<br>                r = -<span class="hljs-number">5</span> <span class="hljs-comment"># to give negative rewards when holes turn up</span><br>                Q[s_] = np.ones(env.action_space.n)*r <span class="hljs-comment">#in terminal state Q value equals the reward</span><br>            <span class="hljs-keyword">else</span>:<br>                r = -<span class="hljs-number">1</span> <span class="hljs-comment"># to give negative rewards to avoid long routes</span><br>        <span class="hljs-keyword">if</span> (r==<span class="hljs-number">1</span>):<br>                r = <span class="hljs-number">100</span><br>                Q[s_] = np.ones(env.action_space.n)*r <span class="hljs-comment">#in terminal state Q value equals the reward</span><br>        Q[s,a] = Q[s,a] + lr * (r + y*np.<span class="hljs-built_in">max</span>(Q[s_,a]) - Q[s,a])<br>        s = s_<br>        <span class="hljs-keyword">if</span> (t == <span class="hljs-literal">True</span>) :<br>            <span class="hljs-keyword">break</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Q-table&quot;</span>)<br><span class="hljs-built_in">print</span>(Q)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output after learning&quot;</span>)<br><span class="hljs-comment">#learning ends with the end of the above loop of several episodes above</span><br><span class="hljs-comment">#let&#x27;s check how much our agent has learned</span><br>s = env.reset()<br>env.render()<br><span class="hljs-keyword">while</span>(<span class="hljs-literal">True</span>):<br>    a = np.argmax(Q[s[<span class="hljs-number">0</span>]])<br>    s_,r,t,_ = env.step(a)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;===============&quot;</span>)<br>    env.render()<br>    s = s_<br>    <span class="hljs-keyword">if</span>(t==<span class="hljs-literal">True</span>) :<br>        <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure><h6 id="Q-learning的更新公式"><a href="#Q-learning的更新公式" class="headerlink" title="Q-learning的更新公式"></a>Q-learning的更新公式</h6><hr><p>Q-learning的更新公式是基于贝尔曼方程（Bellman Equation），用于估计在给定状态下采取某个动作的长期回报。下面详细解释为什么Q-learning的更新公式是这样的。</p><p><strong>1. 贝尔曼方程</strong></p><p>贝尔曼方程描述了在某个状态下的动作价值（Q值）如何与该状态下的即时奖励和未来状态的最大Q值相关联。贝尔曼方程形式如下：<br>$$<br>Q(s, a) &#x3D; \mathbb{E}[r + \gamma \max_{a’} Q(s’, a’) | s, a]<br>$$<br>其中：</p><ul><li>$Q(s,a)$ 是在状态 $s$ 下采取动作 $a$ 的动作价值（Q值）。</li><li>$r$ 是即时奖励。</li><li>$\gamma$ 是折扣因子（介于0和1之间），它决定了未来奖励在当前决策中的重要性。</li><li>$s′$是动作 $a$ 在状态$s$ 下导致的下一个状态。</li><li>$\max_{a’} Q(s’, a’)$是在状态$ s’ $下采取最佳动作 $a’ $的最大Q值。</li></ul><p><strong>Q-learning更新公式</strong></p><p>Q-learning是一种无模型的强化学习算法，通过交互环境并更新Q表来逼近贝尔曼方程。其更新公式如下：<br>$$<br>Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a’} Q(s’, a’) - Q(s, a) \right)<br>$$<br>其中：</p><ul><li>$\alpha$ 是学习率，控制更新步长的大小。</li><li><strong>新信息</strong>：当agent在状态 s下采取动作 a 并观察到即时奖励 r 和下一个状态 s′ 时，它获得了关于Q值的新信息，即$r + \gamma \max_{a’} Q(s’, a’)$。</li><li><strong>TD误差</strong>：通过计算当前Q值与新信息的差值，我们得到了时间差分（Temporal Difference，TD）误差： $\delta &#x3D; r + \gamma \max_{a’} Q(s’, a’) - Q(s, a)$</li><li><strong>更新Q值</strong>：使用TD误差来调整Q值： $Q(s,a)←Q(s,a)+αδ$</li></ul>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Reinforcement Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Reinforcement Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Algorithm-Sort</title>
    <link href="/2024/07/07/Algorithm/"/>
    <url>/2024/07/07/Algorithm/</url>
    
    <content type="html"><![CDATA[<h2 id="Algorithm-Sort"><a href="#Algorithm-Sort" class="headerlink" title="Algorithm-Sort"></a>Algorithm-Sort</h2><h3 id="排序算法总结"><a href="#排序算法总结" class="headerlink" title="排序算法总结"></a>排序算法总结</h3><p><img src="/2024/07/07/Algorithm/v2-519970e854c90f9b901947963316dd64_1440w-20240707192544329.png" alt="排序算法基础总结"></p><p><img src="/2024/07/07/Algorithm/0.png" alt="复杂度和稳定性"></p><hr><h4 id="1-冒泡排序"><a href="#1-冒泡排序" class="headerlink" title="1.冒泡排序"></a>1.冒泡排序</h4><p>时间复杂度$O(N^2)$空间复杂度$O(1)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 数组后面的数为排好序的数，一直交换直到把当前最大的数交换到最后</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">bubble_sort</span>(<span class="hljs-params">nums</span>):<br>    n = <span class="hljs-built_in">len</span>(nums)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>      <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>:n-<span class="hljs-number">1</span>-i):<br>        <span class="hljs-keyword">if</span> nums[j] &gt; nums[j+<span class="hljs-number">1</span>]:<br>          <span class="hljs-comment"># 交换</span><br>          nums[j],num[j+<span class="hljs-number">1</span>] = nums[j+<span class="hljs-number">1</span>],nums[j]<br>    <span class="hljs-keyword">return</span> nums<br></code></pre></td></tr></table></figure><h4 id="2-选择排序"><a href="#2-选择排序" class="headerlink" title="2.选择排序"></a>2.选择排序</h4><h4 id="3-插入排序"><a href="#3-插入排序" class="headerlink" title="3.插入排序"></a>3.插入排序</h4><p>时间复杂度$O(N^2)$空间复杂度$O(1)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 数组前面的数为排好序的，每次在前面不断交换直到找到当前数的位置</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">insertion_sort</span>(<span class="hljs-params">nums</span>):<br>    n = <span class="hljs-built_in">len</span>(nums)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n):<br>        j = i<br>        <span class="hljs-keyword">while</span> j &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> nums[j - <span class="hljs-number">1</span>] &gt; nums[j]:<br>            nums[j], nums[j - <span class="hljs-number">1</span>] = nums[j - <span class="hljs-number">1</span>], nums[j]<br>            j -= <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> nums<br></code></pre></td></tr></table></figure><h4 id="4-希尔排序"><a href="#4-希尔排序" class="headerlink" title="4.希尔排序"></a>4.希尔排序</h4><h4 id="5-归并排序"><a href="#5-归并排序" class="headerlink" title="5.归并排序"></a>5.归并排序</h4><h4 id="6-快速排序"><a href="#6-快速排序" class="headerlink" title="6.快速排序"></a>6.快速排序</h4><blockquote><p>下面的实现使用额外的数组来存储分区结果，因此空间复杂度除了递归调用栈的空间外，还包括分区数组的空间:</p><p>空间复杂度为 O(n)（额外数组）+ O(log n)（递归调用栈）&#x3D; O(n)。</p><p>时间复杂度为 O(log n) （分区操作树的高度）*  O(n)（每层的分区操作）&#x3D;  O(nlog n)</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">quick_sort</span>(<span class="hljs-params">nums</span>):<br>    <span class="hljs-comment"># 实现快排</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(nums) &lt;= <span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> nums<br>    <span class="hljs-comment"># 选取基准数</span><br>    pivot = nums[<span class="hljs-built_in">len</span>(nums)//<span class="hljs-number">2</span>]<br>    <span class="hljs-comment"># 实现分区</span><br>    left = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> nums <span class="hljs-keyword">if</span> x &lt; pivot]<br>    middle = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> nums <span class="hljs-keyword">if</span> x == pivot]<br>    right = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> nums <span class="hljs-keyword">if</span> x &gt; pivot]<br>    <span class="hljs-comment"># 将左右半区递归进行排序</span><br>    <span class="hljs-keyword">return</span> quick_sort(left) + middle + quick_sort(right)<br></code></pre></td></tr></table></figure><blockquote><p>使用原地分区的快速排序实现，避免了使用额外的数组，从而优化了空间复杂度：</p><p>空间复杂度为 O(log n)（递归调用栈）。</p><p>时间复杂度为 O(log n) （分区操作树的高度）*  O(n)（每层的分区操作）&#x3D;  O(nlog n)</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">quick_sort</span>(<span class="hljs-params">nums, low, high</span>):<br>  <span class="hljs-keyword">if</span> low &lt; high:<br>    pi = partition(nums, low, high)<br>    <span class="hljs-comment"># 递归实现左右分区快排</span><br>    quick_sort(nums, low, pi-<span class="hljs-number">1</span>)<br>    quick_sort(nums, pi+<span class="hljs-number">1</span>, high)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partition</span>(<span class="hljs-params">nums, low, high</span>):<br>  <span class="hljs-comment"># 选取基准数</span><br>  pivot = nums[high]<br>  i = low<br>  <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(low,high):<br>    <span class="hljs-keyword">if</span> nums[j] &lt; pivot:<br>      nums[i],nums[j] = nums[j],nums[i]<br>      i += <span class="hljs-number">1</span><br>  nums[i],nums[high] = nums[high],nums[i]<br>  <span class="hljs-keyword">return</span> i<br></code></pre></td></tr></table></figure><h4 id="7-堆排序"><a href="#7-堆排序" class="headerlink" title="7.堆排序"></a>7.堆排序</h4><blockquote><p>堆排序的主要思想是将数组转化为一个大顶堆，然后通过不断将堆顶（最大值）与<strong>未排序部分</strong>的最后一个元素交换并重新调整堆来实现排序。</p><p>从小到大排序，建立大根堆；从大到小排序，建立小根堆。</p><p>大根堆：每个结点的值都大于等于其孩子结点的值；小根堆：每个结点的值都小于等于其孩子结点的值。</p></blockquote><p>排序过程：</p><blockquote><p><strong>1.构建大顶堆</strong>：将数组转化为一个大顶堆。</p><p><strong>2.交换堆顶元素和末尾元素</strong>：将堆顶（最大元素）与未排序部分的最后一个元素交换，并缩小堆的范围。</p><p><strong>3.调整堆</strong>：调整交换后的堆，使其重新成为大顶堆。</p><p><strong>4.重复步骤2和步骤3</strong>，直到堆的范围缩小到1。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">heapify</span>(<span class="hljs-params">arr, h_size, h_index</span>):<br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    调整堆，使其满足大顶堆的性质</span><br><span class="hljs-string">    :param arr: 数组</span><br><span class="hljs-string">    :param h_size: 堆的大小</span><br><span class="hljs-string">    :param h_index: 要调整的节点索引</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 左右结点的索引值</span><br>    left = <span class="hljs-number">2</span>*(h_index) + <span class="hljs-number">1</span><br>    right = <span class="hljs-number">2</span>*(h_index) + <span class="hljs-number">2</span><br>    <span class="hljs-comment"># 先设置最大值为当前结点值</span><br>    maxIndex = h_index<br>    <span class="hljs-comment"># 判断左右结点值是否大于它，进行交换调整</span><br>    <span class="hljs-keyword">if</span> left &lt; h_size <span class="hljs-keyword">and</span> arr[left] &gt; arr[maxIndex]:<br>        maxIndex = left<br>    <span class="hljs-keyword">if</span> right &lt; h_size <span class="hljs-keyword">and</span> arr[right] &gt; arr[maxIndex]:<br>        maxIndex = right<br>    <span class="hljs-keyword">if</span> maxIndex != h_index:<br>        arr[h_index], arr[maxIndex] = arr[maxIndex], arr[h_index]<br>        <span class="hljs-comment"># 递归调整被交换的子树</span><br>        heapify(arr, h_size, maxIndex)<br>        <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">heap_sort</span>(<span class="hljs-params">arr</span>):<br>    <span class="hljs-comment"># 创建大根堆</span><br>    n = <span class="hljs-built_in">len</span>(arr)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n//<span class="hljs-number">2</span>-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):<br>        heapify(arr,n,i)<br><br>    <span class="hljs-comment"># 逐步将最大值交换末尾，然后动态调整剩余堆，使其保持大根堆的性质</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n-<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,-<span class="hljs-number">1</span>):<br>        <span class="hljs-comment"># 将最大值交换到末尾</span><br>        arr[i],arr[<span class="hljs-number">0</span>] = arr[<span class="hljs-number">0</span>],arr[i]<br>        <span class="hljs-comment"># 调整剩余大根堆</span><br>        heapify(arr,i,<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>使用迭代方法来避免递归调用栈，使其空间复杂度保持$O(1)$，迭代方法并不会造成时间复杂度的增加，是因为迭代深度还是类似于堆的高度，所以为$O(\log n)$：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">heapify</span>(<span class="hljs-params">arr, n, i</span>):<br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        largest = i  <span class="hljs-comment"># 初始化最大值为当前节点</span><br>        left = <span class="hljs-number">2</span> * i + <span class="hljs-number">1</span>  <span class="hljs-comment"># 左子节点</span><br>        right = <span class="hljs-number">2</span> * i + <span class="hljs-number">2</span>  <span class="hljs-comment"># 右子节点</span><br><br>        <span class="hljs-comment"># 如果左子节点存在且大于当前最大值，则更新最大值为左子节点</span><br>        <span class="hljs-keyword">if</span> left &lt; n <span class="hljs-keyword">and</span> arr[left] &gt; arr[largest]:<br>            largest = left<br>        <span class="hljs-comment"># 如果右子节点存在且大于当前最大值，则更新最大值为右子节点</span><br>        <span class="hljs-keyword">if</span> right &lt; n <span class="hljs-keyword">and</span> arr[right] &gt; arr[largest]:<br>            largest = right<br>        <span class="hljs-comment"># 如果最大值不是当前节点，则交换并继续调整子树</span><br>        <span class="hljs-keyword">if</span> largest != i:<br>            arr[i], arr[largest] = arr[largest], arr[i]<br>            i = largest<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Algorithm</category>
      
      <category>Basic</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM-Transformer</title>
    <link href="/2024/07/04/LLM-Transformer/"/>
    <url>/2024/07/04/LLM-Transformer/</url>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>Transformer 模型是由谷歌在2017 年提出并首先应用于机器翻译的神经网络模型结构。Transformer 结构完全通过注意力机制完成对源语言序列和目标语言序列全局依赖的建模。</p><p><strong>参考文章：</strong></p><p>本文只是做一些学习记录总结，内容全部来源于作者<a href="https://www.zhihu.com/people/lemonround">猛猿</a>以下文章，写的非常详细：</p><p><a href="https://zhuanlan.zhihu.com/p/454482273%E3%80%81https://zhuanlan.zhihu.com/p/456863215%E7%AD%89">https://zhuanlan.zhihu.com/p/454482273、https://zhuanlan.zhihu.com/p/456863215等</a></p><p><strong>结构图：</strong></p><p><img src="/2024/07/04/LLM-Transformer/1740792-20200714150528222-1369583497.png" alt="Transformer结构"></p><h3 id="1-位置编码"><a href="#1-位置编码" class="headerlink" title="1. 位置编码"></a>1. 位置编码</h3><hr><p>在Transformer的encoder和decoder的输入层中，使用了Positional Encoding，使得最终的输入满足：</p><p>$input &#x3D; Input\ Embedding + Positional\ Encoding$</p><h4 id="1-1-为什么需要位置编码："><a href="#1-1-为什么需要位置编码：" class="headerlink" title="1.1 为什么需要位置编码："></a>1.1 为什么需要位置编码：</h4><p>在self-attention模型中，输入是一整排的tokens，对于人来说，我们很容易观察到tokens的位置信息，但是因为self-attention的运算是无向的，无法分辩tokens位置信息，因此我们要想办法，把tokens的位置信息，喂给模型。</p><h4 id="1-2-如何构造位置编码"><a href="#1-2-如何构造位置编码" class="headerlink" title="1.2 如何构造位置编码"></a>1.2 如何构造位置编码</h4><p>一些方法，比如 用整型值标记位置 &#x2F; 用[0,1]范围标记位置 &#x2F; 用二进制向量标记 &#x2F; 用周期函数sin表示….等，都有其不足之处和局限性。</p><p>但我们有一些希望实现的目标：</p><blockquote><ul><li>位置的表示应该是有界的；</li><li>模型可以处理在训练时没见过的句子长度；</li><li>序列长度不同时，tokens的相对位置应该保持一致；</li><li>希望位置编码在连续空间内而不是离散的；</li><li>不同的位置向量是可以通过线性转换得到的。</li></ul></blockquote><h5 id="1-2-1用周期函数-Sin-表示Position-Encoding"><a href="#1-2-1用周期函数-Sin-表示Position-Encoding" class="headerlink" title="1.2.1用周期函数 Sin 表示Position Encoding:"></a>1.2.1<strong>用周期函数 Sin 表示Position Encoding:</strong></h5><p>我们把位置向量当中的每一个元素都用一个sin函数来表示，则第t个token的位置向量可以表示为：<br>$$<br>PE_t &#x3D; [\sin(\frac{1}{2^0}t), \sin(\frac{1}{2^1}t),\cdots,\sin(\frac{1}{2^{i-1}}t),\cdots,\sin(\frac{1}{2^{d_{model}-1}}t)]<br>$$<br>结合下图，来理解一下这样设计的含义，图中每一行表示一个$PE_t$ ，每一列表示$PE_t$ 中的第<code>i</code>个元素。<code>d_model</code>表示token的维度。</p><blockquote><p>旋钮用于调整精度，越往右边的旋钮，需要调整的精度越大，因此指针移动的步伐越小。每一排的旋钮都在上一排的基础上进行调整（函数中 t 的作用）。通过频率$\frac{1}{2^{i-1}}$来控制sin函数的波长，频率不断减小，则波长不断变大，此时sin函数对t的变动越不敏感，以此来达到越向右的旋钮，指针移动步伐越小的目的。 这也类似于二进制编码，每一位上都是0和1的交互，越往低位走（越往左边走），交互的频率越慢。</p></blockquote><p><img src="/2024/07/04/LLM-Transformer/v2-eb3d818037adb892dafcf26a6ef433cc_1440w.webp" alt="用周期函数Sin表示位置编码"></p><p>由于sin是周期函数，因此从纵向来看，如果函数的频率偏大，引起波长偏短，则不同t下的位置向量可能出现重合的情况。比如在下图中(d_model &#x3D; 3），图中的点表示每个token的位置向量，颜色越深，token的位置越往后，在频率偏大的情况下，位置响亮点连成了一个闭环，靠前位置（黄色）和靠后位置（棕黑色）竟然靠得非常近：</p><p><img src="/2024/07/04/LLM-Transformer/v2-a76698f2225c5ed7bc49b2259548cf2a_1440w.webp" alt="d_model = 3下的sin函数"></p><p>为了避免这种情况，我们尽量将函数的波长拉长。一种简单的解决办法是同一把所有的频率都设成一个非常小的值。因此在transformer的论文中，采用了$\frac{1}{10000^{i&#x2F;{d_{model}-1}}}$这个频率（这里 i 其实不是表示第 i 个位置，但是大致意思差不多，下面会细说） </p><p>总结一下，到这里我们把位置向量表示为：其中$w_i$等于$\frac{1}{10000^{i&#x2F;{d_{model}-1}}}$<br>$$<br>PE_t &#x3D; [\sin(w_0t), \sin(w_1t),\cdots,\sin(w_{i-1}t),\cdots,\sin(w_{d_{model}-1}t)]<br>$$</p><h5 id="1-2-2-用sin和cos交替来表示位置"><a href="#1-2-2-用sin和cos交替来表示位置" class="headerlink" title="1.2.2 用sin和cos交替来表示位置"></a>1.2.2 用sin和cos交替来表示位置</h5><p>目前为止，我们的位置向量实现了如下功能： </p><blockquote><ul><li>每个token的向量唯一（每个sin函数的频率足够小）；</li><li>位置向量的值是有界的，且位于连续空间中。模型在处理位置向量时更容易泛化，即更好处理长度和训练数据分布不一致的序列（sin函数本身的性质）。</li></ul></blockquote><p>那现在我们对位置向量再提出一个要求：<strong>不同的位置向量是可以通过线性转换得到的</strong>。这样，我们不仅能表示一个token的绝对位置，还可以表示一个token的相对位置，即我们想要：<br>$$<br>PE_{t+\triangle{t}}&#x3D;T_{\triangle{t}}*PE_t<br>$$<br>这里，$T$ 表示一个线性变换矩阵。观察这个目标式子，联想到在向量空间中一种常用的线形变换—旋转。在这里，我们将t想象为一个角度，那么$\triangle{t}$就是其旋转的角度，则上面的式子可以进一步写成：</p><p><img src="/2024/07/04/LLM-Transformer/image-20240704221004298.png" alt="向量空间-旋转"></p><p>有了这个构想，我们就可以把原来元素全都是sin函数的 做一个替换，我们让位置两两一组，分别用sin和cos的函数对来表示它们，则现在我们有：<br>$$<br>PE_t &#x3D; \sin(w_0t),\cos(w_0t),\sin(w_1t),\cos(w_1t)\cdots,\sin(w_{i-1}t),\cos(w_{i-1}t),\cdots,\sin(w_{d_{model}-1}t)\cos(w_{d_{model}-1}t)<br>$$<br>在这样的表示下，我们可以很容易用一个线性变换，把$PE_t$转变为$PE_{t+\Delta t}$ ，具体公式<a href="https://zhuanlan.zhihu.com/p/454482273">参考文章</a>。</p><h4 id="1-3-Transformer中位置编码方法：Sinusoidal-functions"><a href="#1-3-Transformer中位置编码方法：Sinusoidal-functions" class="headerlink" title="1.3 Transformer中位置编码方法：Sinusoidal functions"></a>1.3 Transformer中位置编码方法：Sinusoidal functions</h4><p>定义： </p><ul><li><code>t</code> 是这个token在序列中的实际位置（例如第一个token为1，第二个token为2…） </li><li>$PE_t\in\mathbb{R^d}$是这个token的位置向量， $PE_t^{(i)}$表示这个位置向量里的第<code>i</code>个元素 </li><li>$d_{model}$是这个token的维度（在论文中，是512)</li></ul><p>$PE_t^{(i)}$则可以如下表示，这里$w_k&#x3D;\frac{1}{10000^{2k&#x2F;{d_{model}-1}}}$，$k&#x3D;0,1,2,3,\dots,\frac{d_{model}}{2}-1$<br>$$<br>PE_t^{(i)} &#x3D; \begin{cases}<br>\sin(w_kt) &amp; \text{if } i &#x3D; 2k \<br>\cos(w_kt) &amp; \text{if } i &#x3D; 2k+1<br>\end{cases}<br>$$<br>这个意思和式子(5)中的意思是一模一样的，把512维的向量两两一组，每组都是一个sin和一个cos，这两个函数共享同一个频率$w_i$ ，一共有256组，由于我们从0开始编号，所以最后一组编号是255。sin&#x2F;cos函数的波长（由$w_i$决定）则从$2\pi$增长到 $2\pi*10000$</p><h4 id="1-4-Transformer位置编码可视化"><a href="#1-4-Transformer位置编码可视化" class="headerlink" title="1.4 Transformer位置编码可视化"></a>1.4 Transformer位置编码可视化</h4><p>下图是一串序列长度为50，位置编码维度为128的位置编码可视化结果：</p><blockquote><p>可以发现，由于sin&#x2F;cos函数的性质，位置向量的每一个值都位于[-1, 1]之间。同时，纵向来看，图的右半边几乎都是蓝色的，这是因为越往后的位置，频率越小，波长越长，所以不同的t对最终的结果影响不大。而越往左边走，颜色交替的频率越频繁。</p></blockquote><p><img src="/2024/07/04/LLM-Transformer/v2-b6c64586260ebed24339052adec7bca8_1440w.webp" alt="img"></p><h4 id="1-5-Transformer位置编码的重要性质"><a href="#1-5-Transformer位置编码的重要性质" class="headerlink" title="1.5 Transformer位置编码的重要性质"></a>1.5 Transformer位置编码的重要性质</h4><ol><li><p>两个位置编码的点积(dot product)仅取决于偏移量$\Delta t$ ，即两个位置编码的点积可以反应出两个位置编码间的距离。</p><p><img src="/2024/07/04/LLM-Transformer/image-20240707215149249.png" alt="两个位置编码的点积仅取决于偏移量证明公式"></p></li><li><p>位置向量的点积可以用于表示<strong>距离</strong>(distance-aware)，但是它却不能用来表示位置的<strong>方向性</strong>(lack-of-directionality)。</p></li><li><p>在Transformer的论文中，比较了用positional encoding和learnable position embedding(让模型自己学位置参数）两种方法，得到的结论是两种方法对模型最终的衡量指标差别不大。不过在后面的BERT中，已经改成用learnable position embedding的方法了，也许是因为positional encoding在进attention层后一些优异性质消失的原因（猜想）。Positional encoding有一些想象+实验+论证的意味，而编码的方式也不只这一种，比如把sin和cos换个位置，依然可以用来编码。</p></li></ol><h3 id="2-Self-Attention（自注意力机制）"><a href="#2-Self-Attention（自注意力机制）" class="headerlink" title="2.Self-Attention（自注意力机制）"></a>2.Self-Attention（自注意力机制）</h3><hr><h4 id="2-1-Attention构造"><a href="#2-1-Attention构造" class="headerlink" title="2.1 Attention构造"></a>2.1 Attention构造</h4><p>在RNN当中，tokens是一个一个被喂给模型的。比如在x3的位置，模型要等x1和x2的信息都处理完成后，才可以处理x3。</p><p><img src="/2024/07/04/LLM-Transformer/u=3242639576,199183504&fm=253&fmt=auto&app=138&f=JPEG.jpeg" alt="RNN"></p><p>这种构造造成的缺点：</p><ol><li><p><strong>Sequential operations的复杂度随着序列长度的增加而增加。</strong><br>这是指模型下一步计算的等待时间，在RNN中为O(N)。该复杂度越大，模型并行计算的能力越差，反之则反。</p></li><li><p><strong>Maximum Path length的复杂度随着序列长度的增加而增加。</strong><br>这是指信息从一个数据点传送到另一个数据点所需要的距离，在RNN中同样为O(N)，距离越大，则在传送的过程中越容易出现信息缺失的情况，即数据点对于远距离处的信息，是很难“看见”的。</p></li></ol><p>那么，在处理序列化数据的时候，是否有办法，在提升模型的并行运算能力的同时，对于序列中的每个token，也能让它不损失信息地看见序列里的其他tokens呢？ Attention就作为一种很好的改进办法出现了。每个token生成其对应的输出的过程是同时进行的，计算不需要等待。</p><p><img src="/2024/07/04/LLM-Transformer/v2-9b6041b7d63b4e73325ed34898b9d5fd_r.jpg" alt="Self Attention"></p><h4 id="2-2-Attention计算过程图解"><a href="#2-2-Attention计算过程图解" class="headerlink" title="2.2 Attention计算过程图解"></a>2.2 Attention计算过程图解</h4><p><strong>query，key，value的产生：</strong></p><p>假设batch_size&#x3D;1，输入序列X的形状为(seq_len &#x3D; 4, d_model &#x3D; 6)，则对于这串序列，我们产生三个参数矩阵：$W^Q、W^K、W^V$ 。通过上述的矩阵乘法，我们可以得到最终的结果Q，K，V。如下图所示：</p><p><img src="/2024/07/04/LLM-Transformer/v2-ab15fd18a31f3a63ef100688e9a5747f_1440w.webp" alt="query，key，value的产生"></p><p>一般来说， $W^Q$和$W^K$都同样使用$k_{dim}$， $W^V$使用$v_{dim}$。$k_{dim}$和$v_{dim}$不一定要相等，但在transformer的原始论文中，采用的策略是：$K_{dim}&#x3D;V_{dim}&#x3D;d_{model}&#x2F;&#x2F;n_{heads}$，其中$n_{heads}$为self-attention的头数。</p><p><strong>query，key，value的意义：</strong> </p><ul><li><p>query向量类比于询问。某个token问：“其余的token都和我有多大程度的相关呀？” </p><blockquote><p>Query用于与Key矩阵进行点积操作，以计算当前输入与所有输入之间的相似度或关联程度。这个相似度决定了当前输入关注其他输入的程度。</p></blockquote></li><li><p>key向量类比于索引。某个token说：“我把每个询问内容的回答都压缩了下装在我的key里” </p><blockquote><p>Key矩阵代表所有输入中每个单词（或输入）的特征，用于匹配Query以计算相似度。通过点积操作，Key帮助Query找到相关性高的输入。</p></blockquote></li><li><p>value向量类比于回答。某个token说：“我把我自身涵盖的信息又抽取了一层装在我的value里” </p><blockquote><p>Value矩阵代表所有输入中每个单词（或输入）的实际信息值，它是最终的加权结果。Value提供实际的信息内容，它会根据计算得到的注意力分数进行加权平均，从而得到每个输入的最终表示。这些加权后的值将被用于后续的层或输出。</p></blockquote></li></ul><p><img src="/2024/07/04/LLM-Transformer/v2-980e1c0abb79f7d16785a59d55618991_r.jpg" alt="Attention计算过程"></p><p>以图中的token a2为例： </p><ul><li>它产生一个query，每个query都去和别的token的key做“<strong>某种方式</strong>”的计算，得到的结果我们称为attention score（即为图中的$\alpha_{2,i}’$）。则一共得到四个attention score（attention score又可以被称为attention weight）。 </li><li>将这四个score再分别乘上每个token的value，我们会得到四个抽取信息完毕的向量。 </li><li>将这四个向量相加，就是最终a2过attention模型后所产生的结果b2。</li></ul><h4 id="2-3-Attention-Score-计算"><a href="#2-3-Attention-Score-计算" class="headerlink" title="2.3 Attention Score 计算"></a>2.3 Attention Score 计算</h4><p>$$<br>Attention(Q,K,V)&#x3D;softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$</p><p>其中$d_k$就是$k_{dim}$，而$softmax(\frac{QK^T}{\sqrt{d_k}})$就是Attention Score矩阵，我们来详细看下这个矩阵的计算过程:</p><p><img src="/2024/07/04/LLM-Transformer/v2-a5856289c63896fbb4dd9e7151bbde67_1440w.webp" alt="Attention Score矩阵"></p><p><strong>（勘误：紫色方框中的下标有误 )</strong></p><p>在softmax之后，attention score矩阵的<strong>每一行表示一个token</strong>，<strong>每一列表示该token和对应位置token的$\alpha$值</strong>，因为进行了softmax，每一行的$\alpha$值相加等于1。</p><p>论文中所采用的是scaled dot-product，因为乘上了因子$1&#x2F;\sqrt{d_k}$​ ，<font color="red">乘因子$1&#x2F;\sqrt{d_k}$ 的原因</font><strong>是为了使得在softmax的过程中，梯度下降得更加稳定，避免因为梯度过小而造成模型参数更新的停滞</strong>。下面我们通过数学证明，来解释这个结论：</p><hr><p>假设输入向量 ( q ) 和 ( k ) 的各个分量是独立同分布的随机变量，均值为0，方差为1，则q、k的点积的期望值和方差为：</p><p>期望：$\mathbb{E}[q \cdot k] &#x3D; 0$；方差：$\text{Var}(q \cdot k) &#x3D; d_k$</p><p>如果不进行缩放，点积的方差为 ( $d_k$ )，随着 ( $d_k$ ) 增大，点积的值会变得越来越大，这会导致Softmax函数的输入值也变大，从而使得Softmax函数的梯度变得很小，如下所示。通过乘以 ($ \frac{1}{\sqrt{d_k}}$ )，点积的方差变为1，这样可以使得点积的值在一个较小的范围内波动，保持数值稳定性，避免梯度消失问题。加上下图进行理解：</p><p><img src="/2024/07/04/LLM-Transformer/image-20240707231309184.png" alt="对于第i个token的j和j‘处的梯度计算"></p><hr><h4 id="2-4-Masked-Attention"><a href="#2-4-Masked-Attention" class="headerlink" title="2.4 Masked Attention"></a>2.4 Masked Attention</h4><p>在transformer的decoder层中，我们用到了masked attention。在序列生成任务（如机器翻译或文本生成）中，模型在生成第<code>t</code>个位置的单词时，不应该看到第<code>t+1</code>个位置及其之后的单词。Masked Attention通过掩码来阻止模型在计算当前位置的注意力权重时访问未来的位置，从而避免了信息泄露。</p><p>具体来说，在实现Masked Attention时，通常会构建一个上三角形矩阵掩码，这个掩码矩阵的下三角部分为1（允许注意），上三角部分为0（屏蔽注意）。在计算注意力权重时，这个掩码矩阵会被加到注意力分数上，通过对屏蔽位置施加一个大的负数，从而在softmax计算时将这些位置的权重归零。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">masked_attention</span>(<span class="hljs-params">Q, K, V, mask</span>):<br>    scores = torch.matmul(Q, K.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) / (K.size(-<span class="hljs-number">1</span>) ** <span class="hljs-number">0.5</span>)<br>    scores = scores.masked_fill(mask == <span class="hljs-number">0</span>, <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;-inf&#x27;</span>))<br>    attention_weights = F.softmax(scores, dim=-<span class="hljs-number">1</span>)<br>    output = torch.matmul(attention_weights, V)<br>    <span class="hljs-keyword">return</span> output<br><br><span class="hljs-comment"># 示例输入</span><br>Q = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)  <span class="hljs-comment"># (batch_size, seq_length, d_k)</span><br>K = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br>V = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br>mask = torch.tril(torch.ones(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)).unsqueeze(<span class="hljs-number">0</span>).repeat(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)  <span class="hljs-comment"># 下三角掩码</span><br><br>output = masked_attention(Q, K, V, mask)<br></code></pre></td></tr></table></figure><h4 id="2-5-Multihead-Attention"><a href="#2-5-Multihead-Attention" class="headerlink" title="2.5 Multihead Attention"></a>2.5 Multihead Attention</h4><p>多头注意力通过多个并行的注意力头来处理输入，每个注意力头可以关注输入序列的不同部分或不同的特征子空间。这样，模型能够捕获更丰富、更细粒度的信息，而不是单一头注意力所能提供的有限视角。</p><p>不同的注意力头可以学习到不同类型的关系和模式，这使得模型在处理复杂的序列关系时更具灵活性和表达力。例如，一个注意力头可能专注于短距离依赖关系，而另一个注意力头可能专注于长距离依赖关系。</p><blockquote><p>在NLP中，这种模式识别同样重要。比如第一个head用来识别词语间的指代关系(某个句子里有一个单词it，这个it具体指什么呢），第二个head用于识别词语间的时态关系（看见yesterday就要用过去式）等等。 </p></blockquote><p>具体实现上，多头注意力通过以下步骤实现：</p><ol><li><p><strong>线性变换</strong>：输入的查询（Q）、键（K）、值（V）向量分别经过线性变换，得到多个头的Q‘、K’、V‘。</p></li><li><p><strong>独立计算注意力</strong>：对每个头的Q、K、V分别计算注意力权重，并对V进行加权求和。</p></li><li><p><strong>连接和线性变换</strong>：将所有头的输出连接在一起，并通过一个线性变换得到最终的输出。</p></li></ol><p>示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiheadAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, num_heads</span>):<br>        <span class="hljs-built_in">super</span>(MultiheadAttention, self).__init__()<br>        <span class="hljs-keyword">assert</span> d_model % num_heads == <span class="hljs-number">0</span><br>        self.d_k = d_model // num_heads<br>        self.num_heads = num_heads<br><br>        self.linear_Q = nn.Linear(d_model, d_model)<br>        self.linear_K = nn.Linear(d_model, d_model)<br>        self.linear_V = nn.Linear(d_model, d_model)<br>        self.linear_out = nn.Linear(d_model, d_model)<br><br>        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, Q, K, V, mask=<span class="hljs-literal">None</span></span>):<br>        batch_size = Q.size(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># 线性变换并分割成多头</span><br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        view操作将Q的形状从 (batch_size, seq_length, d_model) 转换为 (batch_size, seq_length, num_heads, d_k)，</span><br><span class="hljs-string">        其中 d_k = d_model // num_heads。具体来说，这一步通过view将d_model维度拆分成两个维度：num_heads和d_k。</span><br><span class="hljs-string">        transpose操作交换了第1维和第2维，使得Q的形状变为 (batch_size, num_heads, seq_length, d_k)。</span><br><span class="hljs-string">        这是因为我们希望将多个头分开处理，而不是在序列长度维度上处理。</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        Q = self.linear_Q(Q).view(batch_size, -<span class="hljs-number">1</span>, self.num_heads, self.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        K = self.linear_K(K).view(batch_size, -<span class="hljs-number">1</span>, self.num_heads, self.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        V = self.linear_V(V).view(batch_size, -<span class="hljs-number">1</span>, self.num_heads, self.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><br>        <span class="hljs-comment"># 计算注意力</span><br>        scores = torch.matmul(Q, K.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) / (self.d_k ** <span class="hljs-number">0.5</span>)<br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            scores = scores.masked_fill(mask == <span class="hljs-number">0</span>, <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;-inf&#x27;</span>))<br>        attention_weights = torch.nn.functional.softmax(scores, dim=-<span class="hljs-number">1</span>)<br>        attention_output = torch.matmul(attention_weights, V)<br><br>        <span class="hljs-comment"># 连接多头输出并进行线性变换</span><br>        attention_output = attention_output.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(batch_size, -<span class="hljs-number">1</span>, self.num_heads * self.d_k)<br>        output = self.linear_out(attention_output)<br><br>        <span class="hljs-keyword">return</span> output<br><br><span class="hljs-comment"># 示例输入</span><br>d_model = <span class="hljs-number">512</span><br>num_heads = <span class="hljs-number">8</span><br><span class="hljs-comment"># (batch_size, seq_length, d_model)</span><br>Q = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">10</span>, d_model) <br>K = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">10</span>, d_model)<br>V = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">10</span>, d_model)<br>mask = <span class="hljs-literal">None</span><br><br>multihead_attention = MultiheadAttention(d_model, num_heads)<br>output = multihead_attention(Q, K, V, mask)<br></code></pre></td></tr></table></figure><h3 id="3-Layer-Normalization（层归一化）"><a href="#3-Layer-Normalization（层归一化）" class="headerlink" title="3.Layer Normalization（层归一化）"></a>3.Layer Normalization（层归一化）</h3><h4 id="3-1-Batch-Normalization"><a href="#3-1-Batch-Normalization" class="headerlink" title="3.1 Batch Normalization"></a>3.1 Batch Normalization</h4><p>Batch Normalization（以下简称BN）的方法最早由<a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1502.03167.pdf">Ioffe&amp;Szegedy</a>在2015年提出，主要用于解决在深度学习中产生的内部协变量偏移-ICS（Internal Covariate Shift）的问题。在BasicKnowledge那篇文章中我们详细介绍了Batch Normalization的原理和作用。Batch Normalization被广泛用在CNN任务上来处理图像，针对文本任务，它有以下缺点：</p><blockquote><ol><li>“长短不一”：文本中某些位置没有足够的batch的数据，使得计算出来的$\mu$ 、$\sigma ^2$产生偏差。</li><li>测试集中出现比训练集更长的数据，由于BN在训练中累积 ，在测试中使用累计的经验统计量的原因，导致测试集中多出来的数据没有相应的统计量以供使用。 （<strong>在实际应用中，通常会对语言类的数据设置一个max_len，多裁少pad，这时没有上面所说的这个问题。但这里我们讨论的是理论上的情况，即理论上，诸如Transformer这样的模型，是支持任意长度的输入数据的</strong>）。</li><li>在 RNN 中，每个时间步的数据依赖于前面时间步的隐藏状态。Batch Normalization 对当前时间步的数据进行归一化时引入了整个批次的信息，可能会破坏这种依赖关系。例如，某个时间步的输入特征会因批次内其他时间步的均值和方差的变化而变化，从而影响隐藏状态的更新。这破坏了 RNN 模型逐步构建时间依赖性的能力。</li></ol></blockquote><h4 id="3-2-Layer-Normalization"><a href="#3-2-Layer-Normalization" class="headerlink" title="3.2 Layer Normalization"></a>3.2 Layer Normalization</h4><p><strong>做法：</strong></p><p>Layer Normalization整体做法类似于BN，不同的是LN不是在特征维度间进行标准化操作（横向操作），而是在整条数据间进行标准化操作（纵向操作）。以图像任务为例：</p><blockquote><p>BN：对一个batch里同一channel上的所有数据求取均值和方差</p><p>LN：在一张图片里所有channel的pixel范围内计算均值和方差</p></blockquote><p>以下图为例：$Z^i_j$：<code>i</code>表示第几个样本，<code>j</code>表示维度</p><p><img src="/2024/07/04/LLM-Transformer/v2-bd1fa944470997e03e89a64e9ce2b469_1440w.webp" alt="Batch Normalization"></p><p><img src="/2024/07/04/LLM-Transformer/v2-839ecea737782bc8a8af321f2d4f8633_1440w.webp" alt="Layer Normalization"></p><p><strong>优点：</strong></p><p>LN使得各条数据间在进行标准化的时候相互独立，因此LN在训练和测试过程中是一致的。LN不需要保留训练过程中的 ，每当来一条数据时，对这条数据的指定范围内单独计算所需统计量即可。</p><h4 id="3-3-Transformer-LN改进方法：Pre-LN"><a href="#3-3-Transformer-LN改进方法：Pre-LN" class="headerlink" title="3.3 Transformer LN改进方法：Pre-LN"></a>3.3 Transformer LN改进方法：Pre-LN</h4><p>原始transformer中，采用的是Post-LN，即LN在residule block（图中addtion）之后。<a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/2002.04745.pdf">Xiong et al. (2020)</a>中提出了一种更优Pre-LN的架构，即LN在residule block之前，它能和Post-LN达到相同甚至更好的训练结果，同时规避了在训练Post-LN中产生的种种问题。两种架构的具体形式可以见下图。</p><p><img src="/2024/07/04/LLM-Transformer/v2-b71299f375bd3e5ec3dd86051b3d2cb8_1440w.webp" alt="Post-LN和Pre-LN"></p><p>Post-LN 是原始 Transformer 论文中的默认设置。在这种情况下，学习率 warm-up 策略显得尤为重要，因为 Post-LN 的训练可能会不稳定，尤其是深层模型中。Warm-up 可以帮助缓解这种不稳定性，具体原因如下：</p><blockquote><p><strong>训练初期稳定性</strong>：在模型训练初期，参数初始化较为随机，直接使用较大的学习率可能导致梯度过大，参数更新幅度过大。Warm-up 可以平缓地增加学习率，使模型逐步适应训练数据，减少训练初期的震荡。</p><p><strong>梯度问题</strong>：Post-LN 中，归一化层在残差连接之后，可能导致梯度传播过程中逐层缩小。Warm-up 阶段的低学习率可以在一定程度上防止梯度消失或爆炸问题。</p></blockquote><h3 id="4-ResNet（残差网络）"><a href="#4-ResNet（残差网络）" class="headerlink" title="4. ResNet（残差网络）"></a>4. ResNet（残差网络）</h3><p>弄清楚一个问题：<strong>为什么「嵌入&#x2F;每一层输出」需要再次输入到 Add &amp; Norm 层？</strong></p><blockquote><p><strong>梯度传递的稳定性</strong>：残差连接使得梯度可以直接通过加法路径传递，从而缓解深层网络中的梯度消失问题。</p><p><strong>模型收敛性和训练速度</strong>：归一化操作确保了每一层的输入分布是稳定的，有助于模型的快速收敛。</p><p><strong>信息保留</strong>：通过残差连接，输入信息不会在每一层中丢失，而是持续传递到更深层次的网络中。</p></blockquote><p>下列代码示例中输入嵌入 x 直接加到多头注意力层的输出 sublayer_output 上，然后进行层归一化，输出的形状与输入相同。通过这种设计，确保了每一层的输出在传递过程中保持稳定并保留原始信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">AddNorm</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-built_in">super</span>(AddNorm, self).__init__()<br>        self.norm = nn.LayerNorm(d_model)<br>        self.dropout = nn.Dropout(dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, sublayer_output</span>):<br>        <span class="hljs-keyword">return</span> self.norm(x + self.dropout(sublayer_output))<br><br><span class="hljs-comment"># 示例输入</span><br>batch_size = <span class="hljs-number">2</span><br>seq_length = <span class="hljs-number">10</span><br>d_model = <span class="hljs-number">512</span><br><br>x = torch.rand(batch_size, seq_length, d_model)  <span class="hljs-comment"># 输入嵌入</span><br>sublayer_output = torch.rand(batch_size, seq_length, d_model)  <span class="hljs-comment"># 多头注意力层的输出</span><br><br><span class="hljs-comment"># Add &amp; Norm 层</span><br>add_norm = AddNorm(d_model)<br>output = add_norm(x, sublayer_output)<br><br><span class="hljs-built_in">print</span>(output.shape)  <span class="hljs-comment"># (batch_size, seq_length, d_model)</span><br></code></pre></td></tr></table></figure><h3 id="6-其他"><a href="#6-其他" class="headerlink" title="6. 其他"></a>6. 其他</h3><h4 id="6-1-weight-tying"><a href="#6-1-weight-tying" class="headerlink" title="6.1 weight tying"></a>6.1 weight tying</h4><p>在Transformer模型中，weight tying 是一种减少模型参数数量的方法。具体而言，它是在多个不同的层之间共享权重。这种技术可以显著降低模型的参数数量，从而减小模型的复杂性和内存占用，同时还能帮助模型在训练时更好地泛化。</p><p>在Transformer模型中，最常见的weight tying场景有两个：</p><ol><li><p>Embedding层和输出层之间的weight tying：</p><p>在自然语言处理（NLP）任务中，输入的词嵌入（word embeddings）和输出的词概率分布（通常通过softmax层得到）都需要一个词汇表大小的矩阵。通常情况下，这两个矩阵是独立的，但通过weight tying，这两个矩阵可以共享同一个权重。</p><p>这种共享可以通过以下方式实现：假设输入的词嵌入矩阵为  E ，输出层的权重矩阵为  W ，则通过将  W  设置为  $E^T $ 来实现共享。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, d_model</span>):<br>        <span class="hljs-built_in">super</span>(TransformerModel, self).__init__()<br>        self.embedding = nn.Embedding(vocab_size, d_model)<br>        self.transformer = nn.Transformer(d_model)<br>        self.fc_out = nn.Linear(d_model, vocab_size)<br><br>        <span class="hljs-comment"># Weight tying: set the output layer weights to be the same as the embedding layer weights</span><br>        self.fc_out.weight = self.embedding.weight<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, src, tgt</span>):<br>        src_emb = self.embedding(src)<br>        tgt_emb = self.embedding(tgt)<br>        transformer_output = self.transformer(src_emb, tgt_emb)<br>        output = self.fc_out(transformer_output)<br>        <span class="hljs-keyword">return</span> output<br><br><span class="hljs-comment"># 示例参数</span><br>vocab_size = <span class="hljs-number">10000</span>  <span class="hljs-comment"># 词汇表大小</span><br>d_model = <span class="hljs-number">512</span>       <span class="hljs-comment"># 嵌入维度</span><br><br><span class="hljs-comment"># 创建模型实例</span><br>model = TransformerModel(vocab_size, d_model)<br></code></pre></td></tr></table></figure></li><li><p>Encoder和Decoder之间的weight tying：</p><p>在一些Seq2Seq模型（如翻译模型）中，编码器和解码器可以使用相同的权重。这种共享可以通过将编码器和解码器的相应层设置为相同的权重来实现。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>LLM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM-BERT</title>
    <link href="/2024/07/03/LLM-BERT/"/>
    <url>/2024/07/03/LLM-BERT/</url>
    
    <content type="html"><![CDATA[<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>LLM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BERT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM-CLIP</title>
    <link href="/2024/07/03/LLM-CLIP/"/>
    <url>/2024/07/03/LLM-CLIP/</url>
    
    <content type="html"><![CDATA[<h3 id="CLIP"><a href="#CLIP" class="headerlink" title="CLIP"></a>CLIP</h3>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>LLM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CLIP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AIGC-StableDiffusion</title>
    <link href="/2024/06/28/AIGC-StableDiffusion/"/>
    <url>/2024/06/28/AIGC-StableDiffusion/</url>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h2 id="AIGC–Stable-Diffusion"><a href="#AIGC–Stable-Diffusion" class="headerlink" title="AIGC–Stable Diffusion"></a>AIGC–Stable Diffusion</h2><p><strong>Stable Diffusion</strong>是一个基于Latent Diffusion Model（LDM）的文转图AI模型，其使用了<strong>CLIP ViT-L&#x2F;14的文本编码器</strong>，能够通过文本提示调整图像。它在运行时将成像过程分离成“<strong>扩散 （diffusion）</strong>”的过程，从有噪声的情况开始，逐渐改善图像，直到完全没有噪声，逐步接近所提供的文本描述。</p><p><strong>参考文章：</strong></p><blockquote><ol><li><p>在文章写完去学源码时候才找到这两篇文章，写的非常牛逼，非常细节！！文章中许多图、想法来源于它：</p><p><a href="https://zhuanlan.zhihu.com/p/613337342%E3%80%81https://zhuanlan.zhihu.com/p/615310965">https://zhuanlan.zhihu.com/p/613337342、https://zhuanlan.zhihu.com/p/615310965</a></p></li><li><p>这三篇文章介绍DM模型还有其他内容等非常深入浅出，难以理解的数学公式介绍的非常详细，非常牛逼！！</p><p><a href="https://zhuanlan.zhihu.com/p/637815071%E3%80%81https://zhuanlan.zhihu.com/p/650394311%E3%80%81https://zhuanlan.zhihu.com/p/655568910">https://zhuanlan.zhihu.com/p/637815071、https://zhuanlan.zhihu.com/p/650394311、https://zhuanlan.zhihu.com/p/655568910</a></p></li><li><p>其他文章：</p><p><a href="http://shiyanjun.cn/archives/2212.html%E3%80%81https://jalammar.github.io/illustrated-stable-diffusion/">http://shiyanjun.cn/archives/2212.html、https://jalammar.github.io/illustrated-stable-diffusion/</a></p></li></ol></blockquote><p><strong>Latent Diffusion Model文章摘要：</strong></p><p><img src="/2024/06/28/AIGC-StableDiffusion/image-20240630102333319.png" alt="Latent Diffusion Model Abstract"></p><h3 id="1-模型结构"><a href="#1-模型结构" class="headerlink" title="1. 模型结构"></a>1. 模型结构</h3><hr><p><strong>结构图：</strong></p><p><img src="/2024/06/28/AIGC-StableDiffusion/s.png" alt="Stable Diffusion文生图模型简单框架"></p><p><img src="/2024/06/28/AIGC-StableDiffusion/image-20240630195921339.png" alt="Stable Diffusion文生图模型框架"></p><p><strong>模块</strong>：</p><blockquote><p><strong>CLIP Text Encoder</strong> : 提取输入的text的embedding , <font color="red">通过 cross attention 方式</font>送入扩散模型的 UNet 中作为 condition，注入语义信息，在训练时以context作为condition，使用cross attention机制来更好的学习文本与图像的匹配关系；</p><p><strong>Image Information Creator</strong></p><ul><li><p><strong>Image Encoder</strong> : 基于 VAE Eecoder 将图像压缩到 latent 空间，用向量表示；</p></li><li><p><strong>UNet+Scheduler</strong> : 扩散模型的主体，用来实现文本引导下的 latent 去噪生成；</p></li></ul><p><strong>Image Decode</strong> : 根据得到的 Latent Image，基于 VAE Decoder 生成最终的图像。。</p></blockquote><p><strong>过程：</strong></p><blockquote><p><strong>首先</strong>，输入 Prompt 提示词 “paradise, cosmic, beach”，经过 <strong>CLIP Text Encoder</strong> 组件的处理，将输入的 Prompt 提示词转换成 77×768 的 Token Embeddings，<code>该 Embeddings 输入到 Image Information Creator 组件</code>；</p><p><font color="green">这里如果有别的输入信息，比如Semantic Map（语义图）、Text（文本）、Representations（表示）、Images（图像）等，经过不同组件的处理，然后通过 cross attention 方式送入扩散模型的 UNet 中作为 condition。</font></p><p><strong>然后</strong>，Random image information tensor 是由一个 Latent Seed（Gaussian noise ~ N(0, 1)） 随机生成的 64×64 大小的图片表示，它表示一个完全的噪声图片，<code>作为 Image Information Creator 组件的另一个初始输入</code>；</p><p><strong>接着</strong>，通过 Image Information Creator 组件的处理（该过程称为 Diffusion），<code>生成一个包含图片信息的 64×64 的 Latent Image</code>，该输出包含了前面输入 Prompt 提示词所具有的语义信息的图片的信息；</p><p><strong>最后</strong>，上一步生成的 Latent Image 经过 Image Decoder 组件处理后<code>生成最终的和输入 Prompt 提示词相关的 512×512 大小的图片输出</code>。</p></blockquote><h3 id="2-具体实现"><a href="#2-具体实现" class="headerlink" title="2. 具体实现"></a>2. 具体实现</h3><hr><h4 id="2-0-到底什么是扩散-Diffusion-？"><a href="#2-0-到底什么是扩散-Diffusion-？" class="headerlink" title="2.0 到底什么是扩散(Diffusion)？"></a>2.0 <strong>到底什么是扩散(Diffusion)？</strong></h4><p>扩散是发生在“<strong>Image Information Creator</strong>”组件内部的过程。有了表示输入文本的标记嵌入「<strong>Token embeddings</strong>」和随机起始图像信息数组「<strong>Random image information tensor</strong>」，该过程会生成图像解码器用来绘制最终图像的信息数组。</p><h4 id="2-1-CLIP-Text-Encoder"><a href="#2-1-CLIP-Text-Encoder" class="headerlink" title="2.1 CLIP Text Encoder"></a>2.1 CLIP Text Encoder</h4><p>Stable Diffusion所使用的是OpenAI的CLIP的预训练模型，CLIP 「Contrastive Language-Image Pretraining」是一个多模态模型，能够同时理解和生成图像和文本。它可以将图像和文本进行对比学习，从而在各种多模态任务中表现出色。</p><p>CLIP需要的数据为图像及其描述，数据集中大约包含4亿张图像及其描述。CLIP整体模型如下所示：</p><p><img src="/2024/06/28/AIGC-StableDiffusion/image-20240628165159974.png" alt="CLIP模型"></p><ul><li><p>(1). <strong>对比预训练</strong> 「Contrastive pre-training」</p><blockquote><p>CLIP 是图像编码器和文本编码器的组合，使用两个编码器对数据分别进行编码。然后使用余弦距离比较结果嵌入，刚开始训练时，即使文本描述与图像是相匹配的，它们之间的相似性肯定也是很低的。随着模型的不断更新，在整个数据集中重复该过程，编码器对图像和文本编码得到的嵌入会逐渐相似。</p></blockquote></li><li><p>(2). <strong>从标签文本创建数据集分类器</strong> 「Create dataset classifier from label text」</p><blockquote><p><strong>作用</strong>：</p><ol><li><p><strong>构建分类器</strong>：在传统的图像分类任务中，通常需要为每个类别训练一个专门的分类器。<font color="red">CLIP 通过文本编码器将标签转换为描述性文本，并进一步转换为特征向量，从而构建一个零样本分类器。这些特征向量可以看作是每个类别的“代表”，可以直接用于分类任务。</font></p></li><li><p><strong>零样本学习</strong>：通过这种方式，CLIP 可以在没有看到任何特定类别的训练样本的情况下，依靠标签文本的描述来进行分类。这种能力被称为“零样本学习”，即模型可以识别从未见过的类别。</p></li></ol><p><strong>为什么需要这一步</strong>：</p><ol><li><p><strong>减少数据依赖</strong>：在实际应用中，获取每个类别的大量标注数据是非常困难的。通过从标签文本创建分类器，CLIP 能够在没有特定训练数据的情况下，利用现有的文本描述来实现分类。</p></li><li><p><strong>提高模型泛化能力</strong>：传统分类器只能处理训练时见过的类别，而 CLIP 的这种方法使其能够处理新类别，提高模型的泛化能力和适用范围。</p></li></ol></blockquote></li><li><p>(3). <strong>用于零样本预测</strong> 「Use for zero-shot prediction」</p><blockquote><p><strong>作用：</strong></p><ol><li><p><strong>实际应用</strong>：在实际应用中，将待分类的图像输入图像编码器，得到图像的特征向量，然后将这个特征向量与之前生成的类别特征向量进行匹配，找到最接近的类别。这一过程就是零样本预测的核心。</p></li><li><p><strong>扩展性</strong>：这一过程使得 CLIP 可以处理任意数量和类型的类别，只需提供相应的文本描述即可，无需重新训练模型。</p></li></ol><p><strong>为什么需要这一步</strong>：</p><ol><li><p><strong>实现分类任务</strong>：这一步是将 CLIP 应用于实际分类任务的关键，通过匹配图像和文本特征向量，实现图像分类。</p></li><li><p><strong>灵活性和扩展性</strong>：这种方法使得模型可以处理新出现的类别和变化多端的任务，只需要相应的文本描述即可，无需重新训练。这大大提高了模型的灵活性和扩展性。</p></li></ol></blockquote></li></ul><h4 id="2-2-Image-Information-Creator"><a href="#2-2-Image-Information-Creator" class="headerlink" title="2.2 Image Information Creator"></a>2.2 <strong>Image Information Creator</strong></h4><h5 id="2-2-1-Image-Encoder「VQ-VAE中的编码器部分」"><a href="#2-2-1-Image-Encoder「VQ-VAE中的编码器部分」" class="headerlink" title="2.2.1 Image Encoder「VQ-VAE中的编码器部分」"></a>2.2.1 Image Encoder「<strong>VQ-VAE</strong>中的编码器部分」</h5><p>在Stable Diffusion模型中，Image Encoder（图像编码器）是一个关键组件，用于将图像转换为潜在空间（latent space）中的表示，实现信息的有效压缩和重构。这些表示之后用于生成高质量的图像。</p><ul><li><p>架构</p><blockquote><ol><li><strong>卷积层</strong>：一系列的卷积层，用于逐步提取图像的局部特征。</li><li><strong>池化层</strong>：减少特征图的空间维度，从而降低计算复杂度。</li><li><strong>编码层</strong>：将提取到的特征转换为潜在向量表示。</li></ol></blockquote></li><li><p>工作流程</p><blockquote><ol><li><strong>输入图像</strong>：输入图像首先经过预处理步骤，例如归一化和大小调整。</li><li><strong>卷积层处理</strong>：经过多个卷积层和池化层，提取图像的特征并逐步降低空间维度。</li><li><strong>潜在表示生成</strong>：通过编码层，将提取的特征转换为潜在空间中的向量表示。</li></ol></blockquote></li></ul><p>这些潜在向量表示是扩散模型操作的核心，它们在扩散过程中逐步反向扩散（denoising），最终生成高质量的图像。Image Encoder在这个过程中扮演了至关重要的角色，确保输入图像能够被有效地压缩和表示，从而使得扩散过程能够生成准确且高质量的图像。</p><h5 id="2-2-2-UNet-Scheduler"><a href="#2-2-2-UNet-Scheduler" class="headerlink" title="2.2.2 UNet+Scheduler"></a>2.2.2 UNet+Scheduler</h5><p>当输入的图像、 文本提示词等信息经过不同组件分别被转换成 Image Embeddings、Token Embeddings… 之后，后续的操作就开始进入 Latent Space 中，通过向量来表示并进行各种处理操作，得到了包含 “原始图像 + 提示词” 信息的图像向量数据信息（Latent Image）。</p><p>下图中前向过程（$\varepsilon$为VAE的编码器）<strong>是我们生成数据以训练噪声预测器的方式</strong>。训练完成后，我们可以通过运行反向过程（$\mathbf{D}$为VAE的解码器）来生成图像。</p><p><img src="/2024/06/28/AIGC-StableDiffusion/image-20240629232700236.png" alt="图2"></p><p> Image Embeddings、Token Embeddings 等是如何在 UNet 网络中整合在一起的：</p><p><img src="/2024/06/28/AIGC-StableDiffusion/image-20240630110811534.png" alt="Image Embeddings、Token Embeddings 如何在UNet网络中整合在一起"></p><p>UNet网络具体结构如下：</p><p><img src="/2024/06/28/AIGC-StableDiffusion/v2-1a60fadfd1b8cb1b41bad5f7deddf526_1440w.webp" alt="UNet网络"></p><p>ResBlock+SpatialTransformer具体结构示意图：</p><p><img src="/2024/06/28/AIGC-StableDiffusion/v2-11ab7515941c3b343b28488d89b59ed0_r.jpg" alt="ResBlock+SpatialTransformer"></p><p><strong>相关组件解释：</strong></p><ol><li><p><code>time step embedding</code>是由<u>时间步信息</u>编码成的一个高维向量，以便与其他特征进行结合和处理。以下是详细解释：</p><p>这种嵌入方法的目的是将时间步长信息注入到模型中，使模型能够在生成过程中考虑时间维度上的变化。这种方法在稳定扩散模型中非常重要，因为它能够帮助模型在每个时间步长上进行准确的预测和生成。</p><p>生成 <code>timestep_embedding</code> 的方法如下：</p><ol><li><p><strong>Sine 和 Cosine 位置编码</strong>：</p><p>这种方法类似于 Transformer 模型中使用的位置编码。它使用正弦和余弦函数将时间步长嵌入到一个高维空间中。具体步骤如下：对每个时间步长 ( t ) 生成一个向量，其中每个维度对应一个特定的频率。使用正弦和余弦函数对这些频率进行编码。</p><p>具体公式如下：其中( t ) 是时间步长；( i ) 是向量的维度索引；( d ) 是向量的维度。</p><p>$\text{PE}_{(t, 2i)} &#x3D; \sin \left( \frac{t}{10000^{2i&#x2F;d}} \right) $</p><p>$\text{PE}_{(t, 2i+1)} &#x3D; \cos \left( \frac{t}{10000^{2i&#x2F;d}} \right) $</p></li><li><p><strong>线性变换</strong>：<br>生成的正弦和余弦位置编码向量通常会通过一个线性变换层，以适应模型的输入维度。</p></li><li><p><strong>结合噪声预测</strong>：<br>在噪声预测模型中，时间步长嵌入有助于模型理解不同时间步长下的噪声分布，从而更好地进行预测和生成。</p></li></ol></li></ol><h4 id="2-3-Image-Decoder「VQ-VAE中的解码器部分」"><a href="#2-3-Image-Decoder「VQ-VAE中的解码器部分」" class="headerlink" title="2.3 Image Decoder「VQ-VAE中的解码器部分」"></a>2.3 Image Decoder「<strong>VQ-VAE</strong>中的解码器部分」</h4><p>最后要把这个Latent Image，基于 VAE Decoder 从 Latent Space 再映射到 Pixel Space，得到我们最终需要生成的视觉图像。</p><h3 id="3-DDPM、DDIM、PLMS"><a href="#3-DDPM、DDIM、PLMS" class="headerlink" title="3. DDPM、DDIM、PLMS"></a>3. DDPM、DDIM、PLMS</h3><hr><h4 id="3-1-DDPM-denoising-diffusion-probabilistic-models"><a href="#3-1-DDPM-denoising-diffusion-probabilistic-models" class="headerlink" title="3.1 DDPM (denoising diffusion probabilistic models)"></a>3.1 DDPM (denoising diffusion probabilistic models)</h4><p>在上面所说的几个博客中已经介绍的非常详细了，而且其中的数学公式在大牛的博客中介绍的也非常详细了，但是推导过程依然有些懵懂，常看常新叭。主要解释了以下几个问题：</p><ol><li><p>最大似然估计</p><p>最大似然估计（Maximum Likelihood Estimation, MLE）是一种统计方法，用于估计模型参数，使得在给定观察数据的情况下，模型生成这些数据的概率最大。简单来说，<strong>MLE 通过最大化似然函数来找到最符合已知数据的模型参数</strong>。</p><p><strong>基本思想:</strong></p><p>假设我们有一个数据集 ${x_1, x_2, …, x_m}$，这些数据点是从某个概率分布 $P_{\text{data}}(x)$中抽取出来的。我们想要用一个参数化的模型 $P_\theta(x)$来近似这个分布，其中 ( $\theta$ ) 是模型的参数。</p><blockquote><p>最大似然估计的目标函数是对数似然函数的和，即：$\sum_{i&#x3D;1}^{m} \log P_\theta(x_i) $</p></blockquote><p><strong>在DDPM中公式推导步骤:</strong></p><p><img src="/2024/06/28/AIGC-StableDiffusion/image-20240704113802655.png" alt="最小化KL散度"></p><p>第一步：$[ \arg\max_\theta \int P_{\text{data}}(x) \log P_\theta(x) , dx ]$，这个积分表示在整个数据分布上的<strong>对数似然函数「$\log P_{\theta}(x)$」的期望</strong>。</p><p>第二步：$[\arg\max_\theta E_{x \sim P_{\text{data}}}[\log P_\theta(x)]]$，利用期望的定义，我们可以<strong>把积分表示成期望的形式</strong>。</p><blockquote><p>这里， $( E_{x \sim P_{\text{data}}} )$ 表示根据数据分布 $( P_{\text{data}}(x) )$ 的期望。</p></blockquote><p>第三步：$[ \arg\max_\theta \sum_{i&#x3D;1}^{m} \log P_\theta(x_i) ]$，在实际操作中，我们没有 $P_{\text{data}} $ 的精确形式，而是只有一个有限的数据样本 ${x_1, x_2, …, x_m}$。因此，我们<strong>使用样本均值来近似期望</strong>，这一步的近似就是在数据样本上的对数似然的求和。通过最大化这个和，我们就可以找到最符合数据的模型参数 $( \theta )$。</p></li><li><p>ELBO「Evidence Lower Bound」</p><p>$argmax_{\theta}\prod_{i&#x3D;1}^{m}P_\theta(x_i)$ 的本质就是要使得连乘中的每一项最大，也等同于使得$\log P_{\theta}(x)$最大。所以我们进一步来拆解 。</p><p><img src="/2024/06/28/AIGC-StableDiffusion/image-20240704122722262.png" alt="ELBO"></p></li><li><p>重参数「Reparamterization」技术</p><p>之前想得到<code>t</code>时刻的加噪图像，需要：$q(x_t|x_{t-1})$；通过重参数技术：$q(x_t|x_0)$</p><p>具体来说，在重参数的表达下，第t个时刻的输入图片可以表示为：$x_t &#x3D; \sqrt{\bar{\alpha_t}}x_0+\sqrt{1-\bar{\alpha_t}}\epsilon$</p><p>也就是说，我们每次只需要对符合高斯分布的噪声 $\epsilon$ 采样，就能得到 t 时刻的加噪图</p><p>“从一个带参数的分布中进行采样”转变到“从一个确定的分布中进行采样”，解决了梯度无法传递的问题。</p></li></ol><h3 id="4-补充知识"><a href="#4-补充知识" class="headerlink" title="4. 补充知识"></a>4. 补充知识</h3><hr><h4 id="4-1-VQ-VAE（Vector-Quantized-Variational-Autoencoder）"><a href="#4-1-VQ-VAE（Vector-Quantized-Variational-Autoencoder）" class="headerlink" title="4.1 VQ-VAE（Vector Quantized Variational Autoencoder）"></a>4.1 VQ-VAE（Vector Quantized Variational Autoencoder）</h4><p>关于 VQ-VAE「Vector Quantized Variational Autoencoder，VQ-VAE」，是一种将离散表示学习引入自编码器框架的模型。它结合了传统变分自编码器（VAE）和向量量化技术，使模型能够在离散的潜在空间中进行编码和解码。这张图描述了向量量化变分自编码器的工作原理:</p><p><img src="/2024/06/28/AIGC-StableDiffusion/image-20240701164621499.png" alt="VQ-VAE"></p><p><strong>左侧部分：</strong></p><blockquote><ul><li><strong>输入图像</strong>：首先输入的是一张狗的图像。</li><li><strong>编码器（Encoder）</strong>：输入图像经过卷积神经网络（CNN）编码器，生成编码表示$ z_e(x) $。</li><li><strong>离散化（Discretization）</strong>：编码表示$z_e(x) $被映射到最近的嵌入点$ e_2 $，形成离散表示$z_q(x) $。</li><li><strong>量化（Quantization）</strong>：离散表示$z_q(x)$通过查找表将编码表示映射到最近的嵌入向量，形成量化表示。</li></ul></blockquote><p><strong>右侧部分:</strong></p><blockquote><ul><li><strong>嵌入空间（Embedding Space）</strong>：展示了嵌入空间的可视化。在这个空间中，编码器输出$ z_e(x) $被映射到最近的嵌入点$e_2$。</li><li><strong>梯度更新（Gradient Update）</strong>：红色箭头表示梯度$\nabla_{z}L$的方向，嵌入空间中的点通过梯度更新，推动编码器改变输出，从而优化整个模型。</li></ul></blockquote><p>其中，Encoder 能够将一个图像压缩到低维空间表示，在 Stable Diffusion 模型中，将原始输入图像通过 Encode 转换成 Latent Space 中的向量表示 Latent Image；</p><p>Decoder 能够将一个压缩表示的图像向量数据转换成高维空间表示，在 Stable Diffusion 模型中将 Latent Space 中图像的向量表示 Latent Image 通过 Decode 转换成 Pixel Space 中的视觉图像。</p><p><strong>其中一些概念：</strong></p><ol><li><p>连续表示（Continuous Representation）</p><p>输入数据：假设输入数据  x  是一张图像。</p><p>编码器（Encoder）：编码器  E  是一个神经网络，通常由多个卷积层组成。它将输入图像  x  映射到一个连续的在表示 $z_e &#x3D; E(x)$，这里， $z_e$  是一个连续值的向量，表示图像的特征。</p></li><li><p>向量表示（Vector Representation）</p><p>码本（Codebook）：码本  e  是一个固定大小的向量集合，记为  ${e_1, e_2, \ldots, e_K}$ ，其中  K  是码本向量的数量。每个向量  $e_i$  通常是随机初始化的，并在训练过程中更新。</p></li><li><p>量化表示（Quantized Representation）</p><p>向量量化（Vector Quantization）：向量量化的目的是将连续的潜在表示  $z_e$  映射到离散的码本向量 $ e_i $。具体来说，对于每个  $z_e$ ，找到与其最接近的码本向量 $ e_i $，并用  $e_i $ 代替 $ z_e $。</p><p>量化过程如下：$z_q &#x3D; \text{argmin}_{e_i} | z_e - e_i |$，这里，$ z_q $ 是量化后的离散潜在表示，它与最近的码本向量  $e_i$相等。</p></li><li><p>解码过程（Decoding Process）</p><p>解码器（Decoder）：解码器  D  将量化后的离散潜在表示  z_q  映射回输入数据空间，重建输入图像：$\hat{x} &#x3D; D(z_q)$，这里， $\hat{x}$  是解码器生成的重建图像。</p></li></ol><p><strong>损失函数（Loss Function）:</strong></p><p>VQ-VAE 的损失函数由三部分组成：</p><ol><li><p>重构损失（Reconstruction Loss）：衡量原始输入  x  和重建输出  \hat{x}  之间的差异，通常使用均方误差（MSE）。<br>$$<br>\mathcal{L}_{\text{recon}} &#x3D; | x - \hat{x} |^2<br>$$</p></li><li><p>向量量化损失（Vector Quantization Loss）：鼓励码本向量  $e_i $ 接近连续潜在表示  $z_e $<br>$$<br>\mathcal{L}_{\text{vq}} &#x3D; | \text{sg}[z_e] - e_i |^2<br>$$<br>其中，$\text{sg}$ 表示停止梯度（stop gradient），即在反向传播时不计算其梯度。</p></li><li><p>承诺损失（Commitment Loss）：鼓励连续潜在表示  $z_e$  稳定，以减少解码器的训练难度。<br>$$<br>\mathcal{L}_{\text{commit}} &#x3D; \beta | z_e - \text{sg}[e_i] |<br>$$<br>其中，$\beta$ 是超参数，控制承诺损失的权重。</p></li><li><p>总损失函数是上述三部分损失的加权和：</p></li></ol><p>$$<br>\mathcal{L} &#x3D; \mathcal{L}{\text{recon}} + \mathcal{L}{\text{vq}} + \mathcal{L}_{\text{commit}}<br>$$</p><h4 id="4-2-什么叫做转换为潜在空间中的向量表示，它和提取的图像特征不是一回事吗？"><a href="#4-2-什么叫做转换为潜在空间中的向量表示，它和提取的图像特征不是一回事吗？" class="headerlink" title="4.2 什么叫做转换为潜在空间中的向量表示，它和提取的图像特征不是一回事吗？"></a>4.2 什么叫做转换为潜在空间中的向量表示，它和提取的图像特征不是一回事吗？</h4><p>将图像转换为潜在空间中的向量表示是特征提取的进一步抽象和压缩过程。这一过程的主要目的是将高维的、冗长的特征图压缩为低维的紧凑表示，同时保留图像的核心信息，便于后续的生成和操作。</p><ul><li><p>图像特征提取</p><blockquote><p>图像特征提取是指从输入图像中提取出能够代表图像内容的重要信息。这通常是通过一系列的卷积操作和其他神经网络层来实现的。特征提取的主要目标是捕捉图像的局部和全局信息，包括颜色、纹理、形状等。</p></blockquote></li><li><p><strong>潜在空间中的向量表示</strong></p><blockquote><p>潜在空间中的向量表示是<font color="red"><strong>通过对图像特征进一步处理</strong></font>得到的。这些表示通常是低维的紧凑向量，包含了图像的高层次抽象信息。潜在空间中的向量表示的主要目的是将图像数据压缩到一个更易处理和操作的形式，同时保留图像的关键信息。</p></blockquote></li><li><p>例子</p><blockquote><p>假设我们有一张猫的图像：</p><ol><li><strong>特征提取</strong>：卷积神经网络提取出猫的边缘、纹理、耳朵形状等特征，形成一个高维的特征图。</li><li><strong>潜在表示</strong>：特征图通过编码器压缩为一个低维向量。这个向量可能表示为[0.8, -1.2, 0.3, …]，其中的每个值都对应着图像的一些高层次特征，例如猫的种类、颜色、姿态等。</li></ol></blockquote></li></ul><h4 id="4-3-时间步骤（timestep）和条件信息（context-embedding）"><a href="#4-3-时间步骤（timestep）和条件信息（context-embedding）" class="headerlink" title="4.3 时间步骤（timestep）和条件信息（context embedding）"></a>4.3 时间步骤（timestep）和条件信息（context embedding）</h4><p>在 Stable Diffusion 模型中，时间步骤（timestep）和条件信息（如文本嵌入）被注入到不同的网络组件中，这是为了最大化它们在生成过程中的作用和效率。</p><ul><li>时间步骤加入到 ResBlock:</li></ul><blockquote><p>时间步骤通常通过时间嵌入的形式加入到残差块（ResBlock）中。通俗点来说，由于模型每一步的去噪都用的是同一个模型，所以我们必须告诉模型，现在进行的是哪一步去噪。因此我们要引入timestep，允许模型在每一层都根据当前的时间步骤调整其处理方式。timestep的表达方法类似于Transformer中的位置编码，将一个常数转换为一个向量，再和我们的输入图片进行相加。</p></blockquote><ul><li>条件信息加入到 Spatial Transformer:</li></ul><blockquote><p>条件信息，如文本嵌入，通常通过空间变换器（Spatial Transformer），也就是交叉注意力机制，加入到模型中。<font color="red">Spatial Transformer允许模型在图像特征和条件信息之间建立复杂的对应关系。通过使用交叉注意力，模型可以根据图像的每个部分和条件信息之间的关系来动态调整图像特征</font>。这种方式特别适合处理与空间位置相关的信息，因为它可以让模型关注条件信息中与当前正在生成的图像区域最相关的部分。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>AIGC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Stable Diffusion</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LeetCode</title>
    <link href="/2024/06/28/LeetCode/"/>
    <url>/2024/06/28/LeetCode/</url>
    
    <content type="html"><![CDATA[<h3 id="LeetCode-热题100"><a href="#LeetCode-热题100" class="headerlink" title="LeetCode 热题100"></a>LeetCode 热题100</h3><hr><p>[TOC]</p><hr><h3 id="Hashtable"><a href="#Hashtable" class="headerlink" title="Hashtable"></a>Hashtable</h3><h4 id="1-1-两数之和"><a href="#1-1-两数之和" class="headerlink" title="1. 1:两数之和"></a>1. 1:两数之和</h4><p># Hash表# 数组</p><blockquote><p>给定一个整数数组 <code>nums</code> 和一个整数目标值 <code>target</code>，请你在该数组中找出 <strong>和为目标值</strong> <code>target</code> 的那 <strong>两个</strong> 整数，并返回它们的数组下标。</p></blockquote><p><strong>解决：</strong></p><ul><li><p>暴力枚举法空间复杂度$O(1)$时间复杂度$O(N^2)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">twoSum</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        n = <span class="hljs-built_in">len</span>(nums)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i + <span class="hljs-number">1</span>, n):<br>                <span class="hljs-keyword">if</span> nums[i] + nums[j] == target:<br>                    <span class="hljs-keyword">return</span> [i, j]        <br>        <span class="hljs-keyword">return</span> []<br><br></code></pre></td></tr></table></figure></li><li><p>Hash法空间复杂度$O(N)$时间复杂度$O(N)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">twoSum</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span></span>):<br>       hashtable = <span class="hljs-built_in">dict</span>()<br>       <span class="hljs-keyword">for</span> i, num <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(nums):<br>           <span class="hljs-keyword">if</span> target - num <span class="hljs-keyword">in</span> hashtable:<br>               <span class="hljs-keyword">return</span> [hashtable[target-num], i]<br>           hashtable[nums[i]] = i<br>       <span class="hljs-keyword">return</span> []<br></code></pre></td></tr></table></figure></li></ul><p><strong>总结：</strong></p><ol><li><p><code>enumerate()</code>是python的内置函数，用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">enumerate</span>(sequence, [start=<span class="hljs-number">0</span>])<br>* sequence: 一个序列、迭代器或其他支持迭代对象。<br>* start: 下标起始位置。<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">seasons = [<span class="hljs-string">&#x27;Spring&#x27;</span>, <span class="hljs-string">&#x27;Summer&#x27;</span>, <span class="hljs-string">&#x27;Fall&#x27;</span>, <span class="hljs-string">&#x27;Winter&#x27;</span>]<br><br><span class="hljs-built_in">list</span>(<span class="hljs-built_in">enumerate</span>(seasons))<br>[(<span class="hljs-number">0</span>, <span class="hljs-string">&#x27;Spring&#x27;</span>), (<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;Summer&#x27;</span>), (<span class="hljs-number">2</span>, <span class="hljs-string">&#x27;Fall&#x27;</span>), (<span class="hljs-number">3</span>, <span class="hljs-string">&#x27;Winter&#x27;</span>)]<br><span class="hljs-built_in">list</span>(<span class="hljs-built_in">enumerate</span>(seasons, start=<span class="hljs-number">1</span>))       <span class="hljs-comment"># 下标从 1 开始</span><br>[(<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;Spring&#x27;</span>), (<span class="hljs-number">2</span>, <span class="hljs-string">&#x27;Summer&#x27;</span>), (<span class="hljs-number">3</span>, <span class="hljs-string">&#x27;Fall&#x27;</span>), (<span class="hljs-number">4</span>, <span class="hljs-string">&#x27;Winter&#x27;</span>)]<br></code></pre></td></tr></table></figure></li></ol><h4 id="2-149-字母异位词分组"><a href="#2-149-字母异位词分组" class="headerlink" title="2. 149:字母异位词分组"></a>2. 149:字母异位词分组</h4><p># Hash表# 数组# 字符串</p><blockquote><p>给你一个字符串数组，请你将 <strong>字母异位词</strong> 组合在一起。可以按任意顺序返回结果列表。</p><p><strong>字母异位词</strong> 是由重新排列源单词的所有字母得到的一个新单词。</p></blockquote><p><strong>解决：</strong></p><ul><li><p>Hash表时间复杂度$O(nk\log{k})$空间复杂度$O(nk)$</p><p>其中 <em>n</em> 是 <em>strs</em> 中的字符串的数量，<em>k</em> 是 <em>strs</em> 中的字符串的的最大长度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">groupAnagrams</span>(<span class="hljs-params">self, strs: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]</span>):<br>        hashtable = collections.defaultdict(<span class="hljs-built_in">list</span>)<br>        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> strs:<br>            key = <span class="hljs-string">&quot;&quot;</span>.join(<span class="hljs-built_in">sorted</span>(s))<br>            hashtable[key].append(s)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">list</span>(hashtable.values())<br><br></code></pre></td></tr></table></figure></li><li><p>计数时间复杂度$O(n(k+26)$空间复杂度$O(n(k+26))$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">groupAnagrams</span>(<span class="hljs-params">self, strs</span>):<br>        hashtable = collections.defaultdict(<span class="hljs-built_in">list</span>)<br>        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> strs:<br>            count = [<span class="hljs-number">0</span>] * <span class="hljs-number">26</span><br>            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> s:<br>                count[<span class="hljs-built_in">ord</span>(c)-<span class="hljs-built_in">ord</span>(<span class="hljs-string">&quot;a&quot;</span>)] += <span class="hljs-number">1</span><br>            hashtable[<span class="hljs-built_in">tuple</span>(count)].append(s)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">list</span>(hashtable.values())<br></code></pre></td></tr></table></figure></li></ul><p><strong>总结：</strong></p><ol><li><p>在这里使用<code>hashtable=&#123;&#125;</code>可能会有一些keyError等问题，需要我们多写一些代码，而使用<code>defaultdict</code>可以避免 ：</p><p><code>defaultdict</code> 的构造函数接受一个工厂函数作为参数，这个工厂函数<strong>在访问不存在的键时</strong>会被调用生成默认值。例如，<code>defaultdict(list) </code>表示当访问的键不存在时，会自动生成一个空列表作为默认值。</p></li><li><p>Map、HashTable、Dict等的区别和联系</p></li></ol><h4 id="3-128-最长连续序列"><a href="#3-128-最长连续序列" class="headerlink" title="3. 128:最长连续序列"></a>3. 128:最长连续序列</h4><p># Hash表# 数组</p><blockquote><p>给定一个未排序的整数数组 <code>nums</code> ，找出数字连续的最长序列（不要求序列元素在原数组中连续）的长度。</p><p>请你设计并实现时间复杂度为 <code>O(n)</code> 的算法解决此问题。</p></blockquote><p><strong>解决：</strong></p><ul><li><p>数组排序时间复杂度$O(n\log{n})$空间复杂度$O(1)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">longestConsecutive</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>)-&gt;<span class="hljs-built_in">int</span>:<br>        <span class="hljs-keyword">if</span> nums == []:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        nums_sorted = <span class="hljs-built_in">sorted</span>(nums)<br>        max_length = <span class="hljs-number">1</span><br>        tmp = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums_sorted) - <span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">if</span> nums_sorted[i+<span class="hljs-number">1</span>] == nums_sorted[i]+<span class="hljs-number">1</span>:<br>                tmp += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">elif</span> nums_sorted[i+<span class="hljs-number">1</span>] == nums_sorted[i]:<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">else</span>:<br>                tmp=<span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> tmp&gt;max_length:<br>                max_length = tmp<br>        <span class="hljs-keyword">return</span> max_length<br></code></pre></td></tr></table></figure></li><li><p>Hash表时间复杂度$O(N)$空间复杂度$O(N)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">longestConsecutive</span>(<span class="hljs-params">self, nums</span>):<br>        nums_set = <span class="hljs-built_in">set</span>(nums)<br>        longest_streak = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># 非常聪明的将时间复杂度高的‘排序’替换为‘查找’</span><br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> nums_set:<br>            <span class="hljs-keyword">if</span> n-<span class="hljs-number">1</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> nums_set:<br>                current_num = n<br>                current_streak = <span class="hljs-number">1</span><br><br>                <span class="hljs-keyword">while</span> current_num+<span class="hljs-number">1</span> <span class="hljs-keyword">in</span> nums_set:<br>                    current_num += <span class="hljs-number">1</span><br>                    current_streak += <span class="hljs-number">1</span><br>                <br>                longest_streak = <span class="hljs-built_in">max</span>(current_streak, longest_streak)<br>        <span class="hljs-keyword">return</span> longest_streak<br></code></pre></td></tr></table></figure></li></ul><p><strong>总结：</strong></p><ol><li><p>在Python中，<code>set()</code> 是一种数据结构，用于存储不重复的元素集合。</p><p>集合（set）是一种无序的数据结构，不支持通过索引来访问元素。集合中的元素是无序的，因此没有像列表那样的索引机制。</p><pre><code class="hljs">集合（Set）在底层是通过哈希表实现的。哈希表通过哈希函数将元素映射到一个存储桶，理论上可以在常数时间内完成插入、删除和查找操作。因此，查找一个元素是否在集合中，平均时间复杂度是 O(1)。</code></pre><p>列表（List）</p><pre><code class="hljs">列表（List）是一个有序集合，元素按顺序存储。为了查找一个元素是否在列表中，最坏情况下需要遍历整个列表，检查每个元素。因此，查找操作的时间复杂度是 O(n)，其中 n 是列表中的元素数量。</code></pre></li></ol><h3 id="双指针"><a href="#双指针" class="headerlink" title="双指针"></a>双指针</h3><hr><h4 id="4-283-移动零"><a href="#4-283-移动零" class="headerlink" title="4. 283:移动零"></a>4. 283:移动零</h4><p># 数组# 双指针</p><blockquote><p>给定一个数组 <code>nums</code>，编写一个函数将所有 <code>0</code> 移动到数组的末尾，同时保持非零元素的相对顺序。</p></blockquote><p><strong>解决：</strong></p><ul><li>双指针时间复杂度$O(N)$空间复杂度$O(1)$</li></ul><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">moveZeroes</span>(<span class="hljs-params">self, nums</span>):<br>        left = right = <span class="hljs-number">0</span><br>        num_len = <span class="hljs-built_in">len</span>(nums) <br><span class="hljs-comment"># right指针遍历整个列表</span><br>        <span class="hljs-keyword">while</span> right &lt; num_len:<br>          <span class="hljs-comment"># right指针的目标就是寻找非零值与left指针交换，当找到则两个指针同时右移</span><br>            <span class="hljs-keyword">if</span> nums[right] != <span class="hljs-number">0</span>:<br>                nums[left], nums[right] = nums[right], nums[left]<br>                left += <span class="hljs-number">1</span><br>            right += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p><strong>总结：</strong></p><ol><li><p>这道题的难度被归为简单，解决方法标签为双指针。但是我很疑惑python中如何实现C++中传统的指针功能，ChatGPT回答如下：</p><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs brainfuck"><span class="hljs-comment">在 Python 中，虽然没有直接的指针概念，但你可以通过几种方式实现类似于 C</span><span class="hljs-literal">++</span> <span class="hljs-comment">中指针的功能。主要方法包括使用对象的引用、可变数据类型以及通过封装类来模拟指针行为。</span><br><span class="hljs-comment">1</span><span class="hljs-string">.</span><span class="hljs-comment">在 Python 中，所有变量实际上都是对象的引用（类似于指针）。通过传递对象的引用，可以实现类似指针的功能。</span><br><span class="hljs-comment">2</span><span class="hljs-string">.</span><span class="hljs-comment">Python 的可变数据类型（如列表、字典和自定义对象）可以用于模拟指针的行为，因为它们可以在函数调用中被修改。</span><br><span class="hljs-comment">3</span><span class="hljs-string">.</span><span class="hljs-string">.</span><span class="hljs-string">.</span><span class="hljs-string">.</span><br></code></pre></td></tr></table></figure></li><li><p>这道题其实还有一些解决办法，比如找到<code>0</code>后面的数都往前移动、或者暴力对列表remove&#x2F;append等，但是时间复杂度会很高。</p><p>这个双指针的解决办法说实话很巧妙，但是理解起来有点复杂，评论区说可以按<u>快排的原理</u>去理解，在这里梳理一下：</p><ul><li>当L和R指针指向同一个位置的时候，此时还没有遇到<code>0</code>，交换位置无所谓；</li><li>当L和R指针不指向同一个位置的时候，就一定是遇到了<code>0</code>，此后，L指针一定是指向<code>0</code>，它的左边是处理好的数据。</li></ul></li></ol><h4 id="5-11-盛最多水的容器"><a href="#5-11-盛最多水的容器" class="headerlink" title="5. 11:盛最多水的容器"></a>5. 11:盛最多水的容器</h4><p># 贪心# 双指针# 数组 </p><blockquote><p>给定一个长度为 <code>n</code> 的整数数组 <code>height</code> 。有 <code>n</code> 条垂线，第 <code>i</code> 条线的两个端点是 <code>(i, 0)</code> 和 <code>(i, height[i])</code> 。</p><p>找出其中的两条线，使得它们与 <code>x</code> 轴共同构成的容器可以容纳最多的水。返回容器可以储存的最大水量。</p><p><strong>说明：</strong>你不能倾斜容器。</p></blockquote><p><strong>解决：</strong></p><ul><li>双指针时间复杂度$O(N)$空间复杂度$O(1)$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">maxArea</span>(<span class="hljs-params">self, height</span>):<br>        lens = <span class="hljs-built_in">len</span>(height)<br>        left = <span class="hljs-number">0</span><br>        right = lens-<span class="hljs-number">1</span><br><br>        maxArea = <span class="hljs-number">0</span><br>        curArea = <span class="hljs-number">0</span><br>        <br>        <span class="hljs-keyword">while</span> left != right:<br>            <span class="hljs-keyword">if</span> height[left] &lt;= height[right]:<br>                curArea = height[left] * (right-left)<br>                left += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                curArea = height[right] * (right-left)<br>                right -=<span class="hljs-number">1</span><br>            <br>            maxArea = <span class="hljs-built_in">max</span>(maxArea,curArea)<br>        <span class="hljs-keyword">return</span> maxArea<br></code></pre></td></tr></table></figure><p><strong>总结：</strong></p><ol><li>这道题也可以用暴力解法，双层循环遍历两次列表，判断每一组值，但是时间复杂度为$O(N^2)$。</li><li>一开始两个指针一个指向开头一个指向结尾，此时容器的底是最大的，接下来随着指针向内移动，会造成容器的底变小，这种情况下我们想要让指针移动后的容器面积增大，就要使移动后的容器的高尽量大，所以我们选择指针所指的高较小的那个指针进行移动，这样我们就保留了容器较高的那条边，放弃了较小的那条边，以获得有更高的边的机会。</li></ol><h4 id="6-15-三数之和"><a href="#6-15-三数之和" class="headerlink" title="6. 15:三数之和"></a>6. 15:三数之和</h4><p># 排序# 双指针# 数组</p><blockquote><p>给你一个整数数组 <code>nums</code> ，判断是否存在三元组 <code>[nums[i], nums[j], nums[k]]</code> 满足 <code>i != j</code>、<code>i != k</code> 且 <code>j != k</code> ，同时还满足 <code>nums[i] + nums[j] + nums[k] == 0</code> 。请你返回所有和为 <code>0</code> 且不重复的三元组。</p><p><strong>注意：</strong>答案中不可以包含重复的三元组。</p></blockquote><p><strong>解决：</strong></p><ul><li><p>双指针时间复杂度$O(N^2)$空间复杂度$O(1)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">threeSum</span>(<span class="hljs-params">self, nums</span>):<br>        nums_sort = <span class="hljs-built_in">sorted</span>(nums)<br>        res = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums_sort)):<br>            <span class="hljs-keyword">if</span> nums_sort[i] &gt; <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-comment"># 去重</span><br>            <span class="hljs-keyword">if</span> i &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> nums_sort[i] == nums_sort[i-<span class="hljs-number">1</span>]:<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-comment"># 双指针</span><br>            l = i+<span class="hljs-number">1</span><br>            r = <span class="hljs-built_in">len</span>(nums_sort)-<span class="hljs-number">1</span><br>            <span class="hljs-keyword">while</span> l &lt; r:<br>                s = nums_sort[l] + nums_sort[r] + nums_sort[i]<br>                <span class="hljs-keyword">if</span> s == <span class="hljs-number">0</span>:<br>                    res.append([nums_sort[i],nums_sort[l],nums_sort[r]])<br>                    r -= <span class="hljs-number">1</span><br>                    <span class="hljs-comment"># 去重</span><br>                    <span class="hljs-keyword">while</span> l&lt;r <span class="hljs-keyword">and</span> nums_sort[r]== nums_sort[r+<span class="hljs-number">1</span>]:r -= <span class="hljs-number">1</span><br>                    l += <span class="hljs-number">1</span><br>                    <span class="hljs-comment"># 去重</span><br>                    <span class="hljs-keyword">while</span> l&lt;r <span class="hljs-keyword">and</span> nums_sort[l]== nums_sort[l-<span class="hljs-number">1</span>]:l += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">elif</span> s &gt; <span class="hljs-number">0</span>:<br>                    r -= <span class="hljs-number">1</span><br>                    <span class="hljs-comment"># 去重</span><br>                    <span class="hljs-keyword">while</span> l&lt;r <span class="hljs-keyword">and</span> nums_sort[r]== nums_sort[r+<span class="hljs-number">1</span>]:r -= <span class="hljs-number">1</span> <br>                <span class="hljs-keyword">elif</span> s &lt; <span class="hljs-number">0</span>:<br>                    l += <span class="hljs-number">1</span><br>                    <span class="hljs-comment"># 去重</span><br>                    <span class="hljs-keyword">while</span> l&lt;r <span class="hljs-keyword">and</span> nums_sort[l]== nums_sort[l-<span class="hljs-number">1</span>]:l += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure></li></ul><p><strong>总结：</strong></p><ol><li>双指针解法中 <code>l = i+1</code> 而不是 <code>l = 0</code> 的原因是，如果每次从0开始会造成重复解，<code>for</code>循环遍历每个数，与每个数有关的解都已经被列出，遍历到其他数时，不应该再包含之前已经处理好的值。</li><li>为什么不能用set方法先直接去掉重复的数字？因为像<code>[-1,-1,2]</code>这样的例子就不可行。</li></ol><h4 id="7-42-接雨水"><a href="#7-42-接雨水" class="headerlink" title="7. 42:接雨水"></a>7. <font color="red">42:接雨水</font></h4><p># 栈# 数组# 双指针# 动态规划#单调栈</p><blockquote><p>给定 <code>n</code> 个非负整数表示每个宽度为 <code>1</code> 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。</p></blockquote><p><img src="/2024/06/28/LeetCode/image-20240706170317206.png" alt="接雨水示例1"></p><p><strong>解决：</strong></p><ul><li><p>动态规划「按列」时间复杂度$O(N)$空间复杂度$O(N)$</p><p>对于下标<code> i</code>，下雨后水能到达的最大高度等于下标<code>i</code>两边的最大高度的最小值，下标 <code>i </code>处能接的雨水量等于下标 <code>i </code>处的水能到达的最大高度减去<code> height[i]</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">朴素的做法是对于数组 height 中的每个元素，分别向左和向右扫描并记录左边和右边的最大高度，然后计算每个下标位置能接的雨水量。该做法需要对每个下标位置使用 O(n) 的时间向两边扫描并得到最大高度，因此总时间复杂度是 O(n^2)。</span><br><span class="hljs-string">使用动态规划的方法，可以在 O(n) 的时间内预处理得到每个位置两边的最大高度:</span><br><span class="hljs-string">leftMax[i] = max(leftMax[i-1], height[i])</span><br><span class="hljs-string">rightMax[i] = max(rightMax[i+1], height[i])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">trap</span>(<span class="hljs-params">self, height</span>):<br>        n = <span class="hljs-built_in">len</span>(height)<br>        ans = <span class="hljs-number">0</span><br>        leftMax = [height[<span class="hljs-number">0</span>]] + [<span class="hljs-number">0</span>] * (n-<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n):<br>            leftMax[i] = <span class="hljs-built_in">max</span>(leftMax[i-<span class="hljs-number">1</span>], height[i])<br><br>        rightMax = [<span class="hljs-number">0</span>] * (n-<span class="hljs-number">1</span>) + [height[n-<span class="hljs-number">1</span>]]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):<br>            rightMax[i] = <span class="hljs-built_in">max</span>(rightMax[i+<span class="hljs-number">1</span>], height[i])<br>        <br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            ans += <span class="hljs-built_in">min</span>(leftMax[i], rightMax[i]) - height[i]<br>        <br>        <span class="hljs-keyword">return</span> ans<br></code></pre></td></tr></table></figure></li><li><p>单调栈「按行」时间复杂度$O(N)$空间复杂度$O(N)$</p><p>维护一个单调栈，单调栈存储的是下标，满足从栈底到栈顶的下标对应的数组 height 中的元素递减。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">1. 从左到右遍历数组，遍历到下标 i 时，如果栈内至少有两个元素，记栈顶元素为 top，top 的下面一个元素是 left，则一定有 height[left]≥height[top]。如果height[i]&gt;height[top]，则得到一个可以接雨水的区域，该区域的宽度是i−left−1，高度是min(height[left],height[i])−height[top]，根据宽度和高度即可计算得到该区域能接的雨水量。</span><br><span class="hljs-string">2. 为了得到 left，需要将 top 出栈。在对 top 计算能接的雨水量之后，left 变成新的 top，重复上述操作，直到栈变为空，或者栈顶下标对应的 height 中的元素大于或等于 height[i]。</span><br><span class="hljs-string">3. 在对下标 i 处计算能接的雨水量之后，将 i 入栈，继续遍历后面的下标，计算能接的雨水量。遍历结束之后即可得到能接的雨水总量。</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">trap</span>(<span class="hljs-params">self, height</span>):<br>        ans = <span class="hljs-number">0</span><br>        stack = <span class="hljs-built_in">list</span>()<br>        n = <span class="hljs-built_in">len</span>(height)<br><br>        <span class="hljs-keyword">for</span> i, h <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span> (height):<br>            <span class="hljs-keyword">while</span> stack <span class="hljs-keyword">and</span> h &gt; height[stack[-<span class="hljs-number">1</span>]]:<br>                top = stack.pop()<br>                <span class="hljs-comment"># 左边界构不成存储雨水的条件，直接退出</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> stack:<br>                    <span class="hljs-keyword">break</span><br>                <span class="hljs-comment"># 构成存储雨水的条件</span><br>                left = stack[-<span class="hljs-number">1</span>]<br>                curWidth = i - left - <span class="hljs-number">1</span><br>                curHeight = <span class="hljs-built_in">min</span>(height[left], height[i]) - height[top]<br>                ans += curWidth * curHeight<br>            stack.append(i)<br>        <span class="hljs-keyword">return</span> ans<br></code></pre></td></tr></table></figure></li><li><p><strong>双指针「按列」</strong>时间复杂度$O(N)$空间复杂度$O(1)$</p><p>解决动态规划的空间复杂度问题</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">trap</span>(<span class="hljs-params">self, height</span>):<br>        ans = <span class="hljs-number">0</span><br>        n = <span class="hljs-built_in">len</span>(height)<br>        l = <span class="hljs-number">0</span><br>        r = n-<span class="hljs-number">1</span><br>        lmax = rmax = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">while</span> l &lt; r:<br>            lmax = <span class="hljs-built_in">max</span>(lmax, height[l])<br>            rmax = <span class="hljs-built_in">max</span>(rmax, height[r])<br>            <span class="hljs-keyword">if</span> height[l]&lt;height[r]:<br>                ans += lmax - height[l]<br>                l += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                ans += rmax - height[r]<br>                r -= <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> ans<br></code></pre></td></tr></table></figure></li></ul><p><strong>总结：</strong></p><ol><li>这里最简单的就是第三种-双指针解法，用左右指针lmax、rmax分别指向当前位置左边和右边最大的值，非常简单高效。</li></ol><h3 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h3><hr><h4 id="8-3-无重复字符的最长字串"><a href="#8-3-无重复字符的最长字串" class="headerlink" title="8. 3: 无重复字符的最长字串"></a>8. 3: 无重复字符的最长字串</h4><p># 滑动窗口# Hash表</p><blockquote><p>给定一个字符串 <code>s</code> ，请你找出其中不含有重复字符的最长子串的长度。</p></blockquote><p><strong>解决：</strong></p><ul><li><p>滑动窗口法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">lengthOfLongestSubstring</span>(<span class="hljs-params">self, s</span>):<br>        s_dict = <span class="hljs-built_in">set</span>()<br>        n = <span class="hljs-built_in">len</span>(s)<br>        maxlength = curlength = <span class="hljs-number">0</span><br>        left = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            curlength += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">while</span> s[i] <span class="hljs-keyword">in</span> s_dict:<br>                s_dict.remove(s[left])<br>                left += <span class="hljs-number">1</span><br>                curlength -= <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> curlength &gt; maxlength:<br>                maxlength = curlength<br>                longest_s = s[left:i+<span class="hljs-number">1</span>]<br>            s_dict.add(s[i])<br>        <span class="hljs-keyword">return</span> maxlength<br></code></pre></td></tr></table></figure></li></ul><p><strong>总结：</strong></p><ol><li><p>最笨的方法是双层遍历，每个字符我都计算以它开头的最长无重复字串的长度，但是这样不聪明，如下所示：</p><p><img src="/2024/06/28/LeetCode/image-20240718201120630.png" alt="双层遍历方法示例"></p></li><li><p>把问题进行延伸，我不仅想要知道长度，还想知道这个字串是什么。我本来以为要用一个队列来实现，后来发现，在我们更改maxLength时候，left指向的就是最长队列的开头，i指向的就是最长队列的末尾，此时进行记录即可。</p></li></ol><h4 id="9-438-找到字符串中所有字母异位词"><a href="#9-438-找到字符串中所有字母异位词" class="headerlink" title="9. 438: 找到字符串中所有字母异位词"></a>9. 438: 找到字符串中所有字母异位词</h4><blockquote><p>给定两个字符串 <code>s</code> 和 <code>p</code>，找到 <code>s</code> 中所有 <code>p</code> 的 <strong>异位词</strong> 的子串，返回这些子串的起始索引。不考虑答案输出的顺序。</p><p><strong>异位词</strong> 指由相同字母重排列形成的字符串（包括相同的字符串）。</p></blockquote><p><strong>解决：</strong></p><ul><li><p>暴力循环时间复杂度$O(n\log n)$空间复杂度$O(1)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">findAnagrams</span>(<span class="hljs-params">self, s, p</span>):<br>        p_sorted = <span class="hljs-string">&#x27;&#x27;</span>.join(<span class="hljs-built_in">sorted</span>(p))<br>        n_s = <span class="hljs-built_in">len</span>(s)<br>        n_p = <span class="hljs-built_in">len</span>(p)<br>        index=[]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_s-n_p+<span class="hljs-number">1</span>):<br>            s_i = s[i:i+n_p]<br>            s_sorted = <span class="hljs-string">&#x27;&#x27;</span>.join(<span class="hljs-built_in">sorted</span>(s_i))<br>            <span class="hljs-keyword">if</span> s_sorted == p_sorted:<br>                index.append(i)<br>        <span class="hljs-keyword">return</span> index<br></code></pre></td></tr></table></figure></li><li><p>滑动窗口</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">findAnagrams</span>(<span class="hljs-params">self, s, p</span>):<br>        n_s = <span class="hljs-built_in">len</span>(s)<br>        n_p = <span class="hljs-built_in">len</span>(p)<br><br>        <span class="hljs-keyword">if</span> n_s &lt; n_p:<br>            <span class="hljs-keyword">return</span> []<br><br>        <span class="hljs-comment"># 使用数组来存储字符串 p 和滑动窗口中每种字母的数量</span><br>        count_s = [<span class="hljs-number">0</span>] * <span class="hljs-number">26</span><br>        count_p = [<span class="hljs-number">0</span>] * <span class="hljs-number">26</span><br>        ans = []<br>    <br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_p):<br>            count_p[<span class="hljs-built_in">ord</span>(p[i])-<span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;a&#x27;</span>)] += <span class="hljs-number">1</span><br>            count_s[<span class="hljs-built_in">ord</span>(s[i])-<span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;a&#x27;</span>)] += <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">if</span> count_s == count_p:<br>            ans.append(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_s - n_p):<br>            count_s[<span class="hljs-built_in">ord</span>(s[i]) - <span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;a&#x27;</span>)] -= <span class="hljs-number">1</span><br>            count_s[<span class="hljs-built_in">ord</span>(s[i + n_p]) - <span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;a&#x27;</span>)] += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> count_s == count_p:<br>                ans.append(i+<span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-keyword">return</span> ans<br></code></pre></td></tr></table></figure></li></ul><p><strong>总结：</strong></p><ol><li>这道题用滑动窗口解法，动态的维持当前窗口内每种字母的值，替代了将字符进行排序的方法，减少了时间复杂度。</li></ol><h3 id="字串"><a href="#字串" class="headerlink" title="字串"></a>字串</h3><h4 id="10-560-和为-K-的子数组"><a href="#10-560-和为-K-的子数组" class="headerlink" title="10. 560: 和为 K 的子数组"></a>10. 560: 和为 K 的子数组</h4><p># 数组# 哈希表# 前缀和</p><blockquote><p>给你一个整数数组 <code>nums</code> 和一个整数 <code>k</code> ，请你统计并返回 该数组中和为 <code>k</code> 的子数组的个数 。</p><p>子数组是数组中元素的连续非空序列。</p></blockquote><p>这个题很有意思的是看评论区很多人直觉用滑动窗口做，结果没有考虑数组中有负数的情况，有负数意味着left向右&#x2F;right向右移动不一定会造成sum的减小&#x2F;增大。</p><p><strong>解决：</strong></p><ul><li><p>枚举法时间复杂度$O(N^2)$空间复杂度$O(1)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">subarraySum</span>(<span class="hljs-params">self, nums, k</span>):<br>        count = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>            <span class="hljs-built_in">sum</span> = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i,-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>):<br>                <span class="hljs-built_in">sum</span> += nums[j]<br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">sum</span> == k:<br>                    count += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> count<br></code></pre></td></tr></table></figure></li><li><p>前缀和</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">subarraySum</span>(<span class="hljs-params">nums:<span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>], k:<span class="hljs-built_in">int</span></span>)-&gt;<span class="hljs-built_in">int</span>:<br>    <span class="hljs-comment"># 当前前缀和=0</span><br>    s = <span class="hljs-number">0</span><br>    <span class="hljs-comment"># 返回值：统计子数组等于k的次数</span><br>    ans = <span class="hljs-number">0</span><br>    <span class="hljs-comment"># mp：[前缀和:出现的次数]</span><br>    mp = defaultdict(<span class="hljs-built_in">int</span>)<br>    <span class="hljs-comment"># mp[0]=1：是为了处理当前前缀和 s=k 的情况</span><br>    mp[<span class="hljs-number">0</span>] = <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> nums:<br>        <span class="hljs-comment"># 计算当前的前缀和</span><br>        s += n<br>        <span class="hljs-keyword">if</span> s-target <span class="hljs-keyword">in</span> mp:<br>            ans += mp[s-target]<br>        mp[s] += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> ans<br></code></pre></td></tr></table></figure></li></ul><h4 id="11-239-滑动窗口最大值"><a href="#11-239-滑动窗口最大值" class="headerlink" title="11. 239: 滑动窗口最大值"></a>11. 239: 滑动窗口最大值</h4><p>#<code>队列</code> #<code>数组</code> #<code>滑动窗口</code> #<code>单调队列</code> #<code>堆（优先队列）</code></p><blockquote><p>给你一个整数数组 <code>nums</code>，有一个大小为 <code>k</code> 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 <code>k</code> 个数字。滑动窗口每次只向右移动一位。</p></blockquote><p><strong>解决：</strong></p><ul><li><p>优先队列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> heapq<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">maxSlidingWindow</span>(<span class="hljs-params">nums: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>], k: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>]:<br>    q = [(-nums[i],i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k)]<br>    heapq.heapify(q)<br>    max_num = -q[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br>    ans = [max_num]<br>    <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k,<span class="hljs-built_in">len</span>(nums)):<br>        heapq.heappush(q, (-nums[i],i))<br>        <span class="hljs-keyword">while</span> q[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] &lt; i-k:<br>            heapq.heappop(q)<br>        ans.append(-q[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])<br>    <span class="hljs-keyword">return</span> ans<br></code></pre></td></tr></table></figure></li><li><p>单调队列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> deque<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">maxSlidingWindow</span>(<span class="hljs-params">nums: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>], k: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>]:<br>    ans = []<br>    q = deque()<br>   <br>    <span class="hljs-comment"># 这里是第一个滑动窗口</span><br>    <span class="hljs-comment"># 依次将下标加入到队列中</span><br>    <span class="hljs-comment"># 当前值大于队列最后一个元素时，将此时队列最后一个元素弹出，因此q[0]记录的一定是最大值对应的下标</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>        <span class="hljs-keyword">while</span> q <span class="hljs-keyword">and</span> nums[i] &gt;= nums[q[-<span class="hljs-number">1</span>]]:<br>            q.pop() <br>        q.append(i)<br>    max_num = [q[<span class="hljs-number">0</span>]]<br>    <span class="hljs-comment"># 这里所做的操作和上述原理一样</span><br>    <span class="hljs-comment"># 不同的是加入左边界的判断</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k,<span class="hljs-built_in">len</span>(nums)):<br>        <span class="hljs-keyword">while</span> q <span class="hljs-keyword">and</span> nums[i] &gt;= nums[q[-<span class="hljs-number">1</span>]]:<br>            q.pop() <br>        q.append(i)<br>        <span class="hljs-keyword">while</span> q[<span class="hljs-number">0</span>] &lt;= i-k:<br>            q.popleft()<br>        max_num.append(nums[q[<span class="hljs-number">0</span>]])<br>    <span class="hljs-keyword">return</span> max_num<br></code></pre></td></tr></table></figure></li></ul><p><strong>总结：</strong></p><ol><li><p>这里新提出来两个数据结构：<strong>优先队列和单调队列</strong></p><p>优先队列这里用了heap来实现，总的来说，这个解法是：</p><blockquote><p>在第一次建堆之后，位于堆顶的元素就是目前的最大值「优先级最高」了，之后随着滑动窗口向右滑动，我们往这个堆中添加当前元素并调整堆。此时我们左边的元素还没有弹出，因此我们需要判断当前最大值是不是没有在滑动窗口内，否则的话就需要把左边的元素弹出堆（保证最大值在当前滑动窗口内）</p></blockquote><blockquote><p><strong>优先队列</strong>是一种数据结构，其中每个元素都有一个优先级，元素按优先级进行排序和访问。通常，优先队列支持以下操作：</p><ol><li><p><strong>插入元素</strong>：将一个元素加入队列中。</p></li><li><p><strong>取出优先级最高的元素</strong>：移除&#x2F;返回优先级最高的元素。</p></li></ol><p>优先队列可以使用多种数据结构来实现，例如堆（Heap），其中二叉堆是最常用的一种实现方式。</p></blockquote><p>单调队列这里用了deque来实现，总的来说，这个解法是：</p><blockquote><p>先来个前提，q是一个单调队列，他会将左边比他小的都弹出去，也因此它是一个单调递减的队列，<code>nums[q[0]]</code>就是当前最大值。</p><p>我们也需要来判断q[0]的位置是否在滑动窗口内。</p><p>这样的好处是我们不需要频繁的调整堆来获取最大值，只需要维护这个队列。</p></blockquote><blockquote><p><strong>单调队列</strong>是一种特殊类型的队列，具有单调性的约束，即队列中的元素按某种单调性排列。单调队列可以是单调递增队列或单调递减队列</p><p>单调队列通常用于滑动窗口问题，其中需要在固定窗口内快速获取最大值或最小值。</p></blockquote></li><li><p>在Python的collections.deque中，pop和popleft方法都用于移除元素，但它们的操作位置不同：</p><ul><li><p>**pop()**：从双端队列的右端（末尾）移除并返回一个元素。</p></li><li><p>**popleft()**：从双端队列的左端（开头）移除并返回一个元素。</p></li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>Algorithm</category>
      
      <category>LeetCode</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LeetCode</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Interview Direction Series</title>
    <link href="/2024/06/28/interview/"/>
    <url>/2024/06/28/interview/</url>
    
    <content type="html"><![CDATA[<h3 id="在当前的AIGC（AI-generated-content）领域，在各个领域中领先的模型："><a href="#在当前的AIGC（AI-generated-content）领域，在各个领域中领先的模型：" class="headerlink" title="在当前的AIGC（AI-generated content）领域，在各个领域中领先的模型："></a>在当前的AIGC（AI-generated content）领域，在各个领域中领先的模型：</h3><h4 id="1-文本生成"><a href="#1-文本生成" class="headerlink" title="1. 文本生成"></a>1. 文本生成</h4><ul><li><p><strong>GPT-4（OpenAI）</strong>：GPT-4 是 OpenAI 发布的最新版本的生成预训练变换模型，以其生成高质量的自然语言文本而闻名。它在各种自然语言处理任务中表现出色，包括对话生成、文本补全和内容创作。</p></li><li><p><strong>BERT（Google）</strong>：虽然BERT主要用于自然语言理解任务，但其衍生版本，如T5（Text-To-Text Transfer Transformer），在生成任务上表现也非常出色。</p></li><li><p><strong>T5（Google）</strong>：T5模型可以将各种NLP任务统一为文本到文本的问题，通过预训练和微调，在文本生成和转换任务中表现出色。</p></li></ul><h4 id="2-图像生成"><a href="#2-图像生成" class="headerlink" title="2. 图像生成"></a>2. 图像生成</h4><ul><li><p><strong>DALL-E 2（OpenAI）</strong>：DALL-E 2 是 OpenAI 开发的文本到图像生成模型，可以根据文本描述生成高度逼真的图像。它展示了AI在多模态生成任务中的强大能力。</p></li><li><p><strong>Stable Diffusion（Stability AI）</strong>：这是一个高效的扩散模型，能够在潜在空间中进行图像生成，从而大幅减少计算资源消耗，同时保持高质量的图像生成。</p></li><li><p><strong>Imagen（Google Research）</strong>：Imagen 是一个由 Google Research 开发的强大图像生成模型，通过结合大型语言模型和扩散模型，实现了高质量的文本到图像生成。</p></li></ul><h4 id="3-音频生成"><a href="#3-音频生成" class="headerlink" title="3. 音频生成"></a>3. 音频生成</h4><ul><li><p><strong>WaveNet（DeepMind）</strong>：WaveNet 是由 DeepMind 开发的生成模型，能够生成高保真的语音和音乐。它在语音合成任务上表现出色，被广泛应用于Google Assistant等产品中。</p></li><li><p><strong>Jukebox（OpenAI）</strong>：Jukebox 是 OpenAI 开发的音乐生成模型，可以生成不同风格和艺术家的音乐。它通过一个VAE-GAN架构实现了长时间的音乐生成。</p></li></ul><h4 id="4-视频生成"><a href="#4-视频生成" class="headerlink" title="4. 视频生成"></a>4. 视频生成</h4><ul><li><p><strong>MoCoGAN（Motion and Content Generative Adversarial Network）</strong>：MoCoGAN 是一种用于视频生成的生成对抗网络（GAN），能够同时生成视频的运动和内容。</p></li><li><p><strong>VideoGPT（OpenAI）</strong>：VideoGPT 是一种将GPT架构应用于视频生成任务的模型，利用自回归方式生成视频帧，展示了在视频生成任务上的潜力。</p></li></ul><h4 id="5-多模态生成"><a href="#5-多模态生成" class="headerlink" title="5. 多模态生成"></a>5. 多模态生成</h4><ul><li><p><strong>CLIP（Contrastive Language-Image Pretraining，OpenAI）</strong>：CLIP 是一个多模态模型，能够同时理解和生成图像和文本。它可以将图像和文本进行对比学习，从而在各种多模态任务中表现出色。</p></li><li><p><strong>ALIGN（Google Research）</strong>：ALIGN 是 Google 开发的多模态对比学习模型，能够在大规模数据上进行图像和文本的对比学习，在图像分类、检索等任务上取得了显著效果。</p></li></ul><p>这些模型在AIGC领域的进展展示了人工智能在生成内容方面的巨大潜力和多样性。随着技术的不断进步，这些模型在各自的应用领域中将继续推动创新和发展。</p><h3 id="视觉基础模型等领域（类似于SAM、SEEM、Grounding-DINO、LISA等工作）"><a href="#视觉基础模型等领域（类似于SAM、SEEM、Grounding-DINO、LISA等工作）" class="headerlink" title="视觉基础模型等领域（类似于SAM、SEEM、Grounding-DINO、LISA等工作）"></a>视觉基础模型等领域（类似于SAM、SEEM、Grounding-DINO、LISA等工作）</h3><ol><li><p><strong>SAM（Segment Anything Model）</strong>：</p><ul><li><strong>简介</strong>：SAM 是由 Meta AI 研究团队开发的一种<font color="red"><strong>图像分割模型</strong></font>，其设计目标是能够对任意图像进行任意物体的分割。</li><li><strong>特点</strong>：SAM 使用了一种新的分割技术，可以通过给定的提示（例如点击、框选或文本描述）来分割图像中的对象。这使得它在处理不同类型的图像和场景时具有很高的灵活性。</li><li><strong>应用</strong>：适用于需要精确分割的任务，如医学图像分析、自动驾驶、图像编辑等。</li></ul></li><li><p><strong>SEEM（Semantic Enhanced Efficient Model）</strong>：</p><ul><li><strong>简介</strong>：SEEM 是一种旨在提高图像识别和分割效率的模型，结合了<font color="red"><strong>语义信息增强和高效计算架构</strong></font>。</li><li><strong>特点</strong>：SEEM 利用语义增强技术，使得模型能够更好地理解图像中的内容，从而提高分割的准确性和效率。此外，模型架构设计也注重计算效率，适合在资源受限的环境中使用。</li><li><strong>应用</strong>：广泛应用于需要高效处理的任务，如移动设备上的图像处理、实时视频分析等。</li></ul></li><li><p><strong>Grounding-DINO</strong>：</p><ul><li><strong>简介</strong>：Grounding-DINO 是一个结合了<font color="red"><strong>语义理解和目标检测</strong></font>的模型，基于 DINO（DETR with Improved Non-Autoregressive Object Detection）的架构。</li><li><strong>特点</strong>：通过结合目标检测和语义理解，Grounding-DINO 可以实现更精确的目标检测，并能够在复杂场景中识别和定位多种对象。其非自回归设计使得检测速度更快。</li><li><strong>应用</strong>：适用于需要高精度和高效目标检测的应用，如自动驾驶、智能监控、无人机导航等。</li></ul></li><li><p><strong>LISA（Language-Image Semantic Alignment）</strong>：</p><ul><li><strong>简介</strong>：LISA 是一种用于图像和语言对齐的模型，旨在通过联合学习图像和文本的语义信息来提高多模态任务的性能。</li><li><strong>特点</strong>：LISA 利用一种对比学习的方法，使得模型能够更好地理解和关联图像和文本的语义信息，从而在图像标注、图像生成和图像搜索等任务中表现出色。</li><li><strong>应用</strong>：适用于多模态任务，如图像描述生成、视觉问答、跨模态检索等。</li></ul></li></ol><p>这些模型各有特点，适用于不同的视觉任务，通过结合不同的技术和架构，推动了计算机视觉领域的进步。</p><h3 id="Bilibili视觉算法实习生：多模态大模型理解、图像和视频分类、图片和视频描述生成、多模态融合、检索等"><a href="#Bilibili视觉算法实习生：多模态大模型理解、图像和视频分类、图片和视频描述生成、多模态融合、检索等" class="headerlink" title="Bilibili视觉算法实习生：多模态大模型理解、图像和视频分类、图片和视频描述生成、多模态融合、检索等"></a>Bilibili视觉算法实习生：多模态大模型理解、图像和视频分类、图片和视频描述生成、多模态融合、检索等</h3><h3 id="快手视觉算法实习生：视频内容理解算法"><a href="#快手视觉算法实习生：视频内容理解算法" class="headerlink" title="快手视觉算法实习生：视频内容理解算法"></a>快手视觉算法实习生：视频内容理解算法</h3>]]></content>
    
    
    <categories>
      
      <category>Interview</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Interview</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BasicKnowledge</title>
    <link href="/2024/06/13/DeepLearningBasicKnowledge/"/>
    <url>/2024/06/13/DeepLearningBasicKnowledge/</url>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><blockquote><p>Batch Normalization (BN) 是一种深度学习技术，用于在训练过程中标准化神经网络的输入。其主要目的是加速训练过程并提高模型的稳定性和性能。</p></blockquote><hr><h4 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h4><p>假设我们有一个图像分类任务，输入图像的像素值在 0 到 255 之间。我们将像素值缩放到 0 到 1 之间，但不同批次的数据分布可能不同。例如，一个批次的图像可能大部分像素值集中在 0.1 到 0.3 之间，而另一个批次的图像像素值集中在 0.7 到 0.9 之间。这种输入数据分布的变化会导致模型训练不稳定。</p><p>通过使用 Batch Normalization，我们可以在每层网络中对输入进行标准化，使得每层的输入在训练过程中保持均值为 0、方差为 1 的分布，从而加快收敛速度并提高模型的稳定性和性能。</p><h4 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h4><ol><li><p><strong>计算批次均值和方差</strong>： 对于一个批次的输入 $x&#x3D;&lt;!–swig￼2–&gt;<br>$$</p></li><li><p>引入两个可训练参数 $\gamma$ 和 $\beta$，分别用于缩放和平移标准化后的输入，以保证模型的表达能力，其中，$\gamma$ 和 $\beta$ 是与输入维度相同的参数：<br>$$<br>y_i &#x3D; \gamma \hat{x_i} + \beta<br>$$</p></li></ol><h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><p>Batch Normalization 之所以对网络训练产生积极作用，主要有以下几个原因：</p><ol><li><p><strong>减轻内部协变量偏移（Internal Covariate Shift）</strong>： 内部协变量偏移是指在训练过程中，由于前一层参数的变化，后续层的输入分布也发生变化，从而影响训练效率。</p><p>BN 通过标准化每一层的输入，使得每一层的输入分布更加稳定，从而减轻了层间输入分布变化带来的影响。这使得网络在训练时更容易收敛。</p></li><li><p><strong>加速训练</strong>： 由于每一层的输入被标准化，梯度下降算法在训练过程中能够使用较大的学习率，从而加快训练速度。</p></li><li><p><strong>提高模型的稳定性</strong>： BN 在一定程度上起到了正则化的作用，减少了过拟合的风险。这是因为每个批次的数据都被标准化，添加了噪声，从而增强了模型的泛化能力。</p></li><li><p><strong>缓解梯度消失和梯度爆炸</strong>： 标准化后的输入使得数据的尺度更加一致，从而缓解了梯度消失和梯度爆炸问题，特别是在深层神经网络中。</p></li></ol><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br><span class="hljs-comment"># 定义一个简单的神经网络</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleNN</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(SimpleNN, self).__init__()<br>        self.fc1 = nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">256</span>)<br>        self.bn1 = nn.BatchNorm1d(<span class="hljs-number">256</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.fc1(x)<br>        x = self.bn1(x)<br>        x = torch.relu(x)<br>        x = self.fc2(x)<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment"># 初始化网络、损失函数和优化器</span><br>model = SimpleNN()<br>criterion = nn.CrossEntropyLoss()<br>optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0.9</span>)<br><br><span class="hljs-comment"># 示例训练步骤</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    <span class="hljs-keyword">for</span> inputs, labels <span class="hljs-keyword">in</span> train_loader:<br>        optimizer.zero_grad()<br>        outputs = model(inputs)<br>        loss = criterion(outputs, labels)<br>        loss.backward()<br>        optimizer.step()<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Epoch <span class="hljs-subst">&#123;epoch+<span class="hljs-number">1</span>&#125;</span>, Loss: <span class="hljs-subst">&#123;loss.item()&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p>在深度学习中，Layer Normalization（层归一化）是一种标准化技术，用于减轻训练过程中不同层之间的内部协变量偏移（internal covariate shift）现象。在Layer Normalization中，归一化操作是在特征维度上进行的，这与Batch Normalization不同。具体来说：</p><h4 id="特征维度上的归一化"><a href="#特征维度上的归一化" class="headerlink" title="特征维度上的归一化"></a>特征维度上的归一化</h4><p>假设我们有一个输入向量 $\mathbf{x} &#x3D; [x_1, x_2, \ldots, x_d] $，其中 ( d ) 是特征的维数。在Layer Normalization中，归一化是在每个样本的特征维度上独立进行的。具体步骤如下：</p><ol><li>计算均值和方差：</li></ol><p>对于给定的输入向量 $\mathbf{x} $，计算其特征维度上的均值和方差：<br>$$<br>\mu &#x3D; \frac{1}{d} \sum_{i&#x3D;1}^{d} x_i<br>\<br>\sigma^2 &#x3D; \frac{1}{d} \sum_{i&#x3D;1}^{d} (x_i - \mu)^2<br>$$</p><ol start="2"><li><p>归一化：</p><p>使用计算得到的均值和方差，对每个特征进行归一化：</p></li></ol><p>$$<br>\hat{x}_i &#x3D; \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}<br>$$</p><p>​其中 ( $\epsilon$ ) 是一个小的常数，用于防止分母为零。</p><ol start="3"><li><p>缩放和平移：</p><p>归一化后的特征通常会进行缩放和平移，以增加模型的表达能力：<br>$$<br>y_i &#x3D; \gamma \hat{x}_i + \beta<br>$$</p></li></ol><p>​其中， ( $\gamma$ ) 和 ( $\beta$ ) 是可学习的参数，分别用于缩放和平移。</p><p>总的来说，Layer Normalization是在每个样本的特征维度上进行归一化，而不是在样本维度上进行归一化，这使得它在处理不同批次大小（尤其是小批次或批次大小为1）的情况下表现更好。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><h4 id="1-L1、L2和Huber损失"><a href="#1-L1、L2和Huber损失" class="headerlink" title="1. L1、L2和Huber损失"></a>1. L1、L2和Huber损失</h4><blockquote><p>L1损失和L2损失是两种常见的损失函数，通常用于机器学习和深度学习中的回归问题。它们的主要区别在于计算损失的方法和对模型的影响。</p></blockquote><h5 id="L1损失（绝对误差损失）："><a href="#L1损失（绝对误差损失）：" class="headerlink" title="L1损失（绝对误差损失）："></a>L1损失（绝对误差损失）：</h5><p>L1损失函数的计算方式是预测值与真实值之差的绝对值的和：</p><p>$$<br> L1 &#x3D; \sum_{i} |y_i - \hat{y}_i| <br>$$<br>其中，$y_i$是真实值，$\hat{y}_i $ 是预测值。</p><p><strong>特点：</strong></p><ul><li>对离群点（outliers）更为鲁棒，因为离群点对总损失的贡献相对较小。</li><li>L1损失倾向于产生稀疏的模型参数，这在某些应用中是一个优势，比如特征选择。</li></ul><h5 id="L2损失（二次误差损失）："><a href="#L2损失（二次误差损失）：" class="headerlink" title="L2损失（二次误差损失）："></a>L2损失（二次误差损失）：</h5><p>L2损失函数的计算方式是预测值与真实值之差的平方和：</p><p>$$<br>L2 &#x3D; \sum_{i} (y_i - \hat{y}_i)^2 <br>$$<br><strong>特点：</strong></p><ul><li>对离群点更为敏感，因为误差的平方会放大离群点对总损失的贡献。</li><li>L2损失倾向于平滑的模型参数，使得模型在整体上更为平滑和稳定。</li></ul><h5 id="Huber损失："><a href="#Huber损失：" class="headerlink" title="Huber损失："></a>Huber损失：</h5><p>结合了L1和L2损失的优点，对离群点较为鲁棒。定义为：<br>$$<br>L_\delta (a) &#x3D;<br>\begin{cases}<br>\frac{1}{2} a^2 &amp; \text{for } |a| \le \delta \<br>\delta (|a| - \frac{1}{2} \delta) &amp; \text{otherwise}<br>\end{cases}<br>$$<br>其中，$ a $是<strong>预测误差</strong>（$a &#x3D; y - \hat{y}$），$\delta $是一个超参数。</p><h4 id="2-分类任务"><a href="#2-分类任务" class="headerlink" title="2. 分类任务"></a>2. 分类任务</h4><h5 id="交叉熵损失（Cross-Entropy-Loss）："><a href="#交叉熵损失（Cross-Entropy-Loss）：" class="headerlink" title="交叉熵损失（Cross-Entropy Loss）："></a>交叉熵损失（Cross-Entropy Loss）：</h5><p>常用于分类问题，尤其是多分类问题。对于二分类问题，交叉熵损失定义为：<br>$$<br> L &#x3D; -\frac{1}{N} \sum_{i&#x3D;1}^{N} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]<br>$$<br>其中，$y_i$是真实标签，$ \hat{y}_i $ 是预测的概率。</p><h5 id="焦点损失（Focal-Loss）「RetinaNet」："><a href="#焦点损失（Focal-Loss）「RetinaNet」：" class="headerlink" title="焦点损失（Focal Loss）「RetinaNet」："></a>焦点损失（Focal Loss）「RetinaNet」：</h5><p>Focal Loss 是一种专门为解决类别不平衡问题而设计的损失函数，最早由 Facebook AI 研究院在 2017 年提出，用于目标检测中的 RetinaNet 模型。它通过减少易分类样本的损失，增加难分类样本的损失，使得模型更加关注难分类的样本。</p><ol><li><p>定义</p><blockquote><p>Focal Loss 是在交叉熵损失（Cross-Entropy Loss）的基础上引入一个调节因子，使得易分类的样本损失减小，难分类的样本损失增大。具体公式如下：</p></blockquote></li><li><p>二分类 Focal Loss</p><p>对于二分类问题，标准的交叉熵损失函数如下，其中，( $p_t$ ) 表示模型对正确类别的预测概率。<br>$$<br>\text{CE}(p_t) &#x3D; -\log(p_t)<br>$$<br>Focal Loss 在此基础上引入了一个调节因子  $(1 - p_t)^\gamma$ ，其中 ( $\gamma$ ) 是一个超参数，用于控制调节因子的大小。Focal Loss 的公式如下：</p></li></ol><p>$$<br>\text{FL}(p_t) &#x3D; - (1 - p_t)^\gamma \log(p_t)<br>$$</p><ol start="3"><li><p>多分类 Focal Loss</p><p>对于多分类问题，标准的交叉熵损失函数如下，其中，( N ) 是类别数量，( y_i ) 是目标类别的one-hot编码，( p_i ) 是模型对第 ( i ) 类的预测概率。<br>$$<br>\text{CE}(p_t) &#x3D; - \sum_{i&#x3D;1}^N y_i \log(p_i)<br>$$</p><p>多分类问题下的 Focal Loss 公式为：<br>$$<br>\text{FL}(p_t) &#x3D; - \sum_{i&#x3D;1}^N y_i (1 - p_i)^\gamma \log(p_i)<br>$$</p></li></ol><p><strong>Focal Loss 的作用：</strong></p><blockquote><p>1.减小易分类样本的损失：当 $p_t$ 较大时，说明样本容易被正确分类，此时  $(1 - p_t)^\gamma$ 会变小，从而减小该样本的损失。<br>2.增加难分类样本的损失：当  $p_t$  较小时，说明样本难以被正确分类，此时  $(1 - p_t)^\gamma $ 会变大，从而增加该样本的损失。</p></blockquote><p><strong>Focal Loss 的优势：</strong></p><pre><code class="hljs">1.缓解类别不平衡问题：在目标检测任务中，背景类通常占据大部分样本，而前景类样本较少。Focal Loss 通过调节因子，使模型更关注前景类的难分类样本，缓解类别不平衡问题。2.提升检测性能：由于 Focal Loss 增加了难分类样本的损失，模型在训练过程中会更倾向于关注这些样本，从而提升检测性能，尤其是在小目标检测和复杂背景下的检测性能。</code></pre><h5 id="对比损失（Contrastive-Loss）："><a href="#对比损失（Contrastive-Loss）：" class="headerlink" title="对比损失（Contrastive Loss）："></a>对比损失（Contrastive Loss）：</h5><p>常用于度量学习中的孪生网络（Siamese Network）。对于一对样本，损失函数定义为：<br>$$<br> L &#x3D; \frac{1}{2N} \sum_{i&#x3D;1}^{N} [y_i d_i^2 + (1 - y_i) \max(0, m - d_i)^2]<br>$$<br>其中，$d_i$是样本对之间的距离，$m$是一个边界阈值。</p><h4 id="3-KL散度（Kullback-Leibler-Divergence）："><a href="#3-KL散度（Kullback-Leibler-Divergence）：" class="headerlink" title="3. KL散度（Kullback-Leibler Divergence）："></a>3. KL散度（Kullback-Leibler Divergence）：</h4><p>常用于变分自编码器（VAE）等生成模型，用于度量两个概率分布之间的差异：</p><p>$$<br>D_{\text{KL}}(P || Q) &#x3D; \sum_{i} P(i) \log \frac{P(i)}{Q(i)}<br>$$</p><h3 id="NMS-非极大值抑制"><a href="#NMS-非极大值抑制" class="headerlink" title="NMS-非极大值抑制"></a>NMS-非极大值抑制</h3><p><strong>NMS的算法流程</strong></p><blockquote><p><strong>排序</strong>：根据候选框的置信度分数进行降序排序。</p><p><strong>选择最高分数的框</strong>：选取当前最高分数的边界框作为一个保留框，然后将其从候选框列表中删除。</p><p><strong>计算重叠度</strong>：对于剩余的候选框，计算它们与当前保留框的交并比（IoU，Intersection over Union）。</p><p><strong>移除高重叠框</strong>：将与当前保留框的IoU大于预设阈值的候选框删除。</p><p><strong>重复步骤2-4</strong>：直到所有的候选框都处理完毕。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">iou</span>(<span class="hljs-params">box1, box2</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;计算两个边界框的交并比（IoU）&quot;&quot;&quot;</span><br>    x1, y1, x2, y2 = box1<br>    x1_prime, y1_prime, x2_prime, y2_prime = box2<br><br>    <span class="hljs-comment"># 计算交集</span><br>    xi1 = <span class="hljs-built_in">max</span>(x1, x1_prime)<br>    yi1 = <span class="hljs-built_in">max</span>(y1, y1_prime)<br>    xi2 = <span class="hljs-built_in">min</span>(x2, x2_prime)<br>    yi2 = <span class="hljs-built_in">min</span>(y2, y2_prime)<br>    inter_area = <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, xi2 - xi1 + <span class="hljs-number">1</span>) * <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, yi2 - yi1 + <span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 计算并集</span><br>    box1_area = (x2 - x1 + <span class="hljs-number">1</span>) * (y2 - y1 + <span class="hljs-number">1</span>)<br>    box2_area = (x2_prime - x1_prime + <span class="hljs-number">1</span>) * (y2_prime - y1_prime + <span class="hljs-number">1</span>)<br>    union_area = box1_area + box2_area - inter_area<br><br>    <span class="hljs-comment"># 计算IoU</span><br>    iou = inter_area / union_area<br>    <span class="hljs-keyword">return</span> iou<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">non_max_suppression</span>(<span class="hljs-params">boxes, scores, iou_threshold</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;执行非极大值抑制（NMS）&quot;&quot;&quot;</span><br>    idxs = np.argsort(scores)[::-<span class="hljs-number">1</span>]  <span class="hljs-comment"># 置信度分数降序排序</span><br>    keep = []<br>    <span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(idxs) &gt; <span class="hljs-number">0</span>:<br>        current = idxs[<span class="hljs-number">0</span>]<br>        keep.append(current)<br>        idxs = idxs[<span class="hljs-number">1</span>:]<br>        filtered_idx = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> idxs:<br>            <span class="hljs-keyword">if</span> iou(boxes[current], boxes[i]) &lt;= iou_threshold:<br>                filtered_idx.append(i)<br>        idxs = np.array(filtered_idx)<br>    <span class="hljs-keyword">return</span> keep<br></code></pre></td></tr></table></figure><h3 id="SVM分类器"><a href="#SVM分类器" class="headerlink" title="SVM分类器"></a>SVM分类器</h3><h3 id="K-Means聚类"><a href="#K-Means聚类" class="headerlink" title="K-Means聚类"></a>K-Means聚类</h3><h3 id="量化、剪枝、蒸馏"><a href="#量化、剪枝、蒸馏" class="headerlink" title="量化、剪枝、蒸馏"></a>量化、剪枝、蒸馏</h3><p>模型的量化、剪枝和蒸馏是深度学习模型优化的三种常见技术，旨在减少模型的计算成本和存储需求，同时尽量保持其性能。下面详细解释这些概念及其实现方法。</p><h4 id="1-模型量化（Quantization）"><a href="#1-模型量化（Quantization）" class="headerlink" title="1. 模型量化（Quantization）"></a>1. 模型量化（Quantization）</h4><p>模型量化是将模型权重和激活值从高精度表示（如32位浮点数）转换为低精度表示（如8位整数）的过程。这可以显著减少模型的存储空间和计算需求。</p><p><strong>类型</strong>：</p><ul><li><strong>权重量化（Weight Quantization）</strong>：仅量化模型的权重。</li><li><strong>激活量化（Activation Quantization）</strong>：量化模型在推理时产生的激活值。</li><li><strong>混合量化（Mixed Precision Quantization）</strong>：同时量化权重和激活值。</li></ul><p><strong>量化方法</strong>：</p><ul><li><strong>静态量化（Static Quantization）</strong>：在推理之前进行量化。</li><li><strong>动态量化（Dynamic Quantization）</strong>：在推理过程中动态量化激活值。</li><li><strong>训练时量化（Quantization-Aware Training, QAT）</strong>：在训练过程中考虑量化误差，以提高量化后的模型性能。</li></ul><p><strong>实现步骤</strong>：</p><ol><li><strong>范围估计</strong>：确定权重和激活值的最小值和最大值。</li><li><strong>缩放和移位</strong>：将浮点数映射到整数范围内。</li><li><strong>量化</strong>：将浮点数转换为整数。</li></ol><p><strong>公式</strong>：<br>假设 ( x ) 是浮点数， ( x_q ) 是量化后的整数，量化过程可以表示为：<br>$$<br>x_q &#x3D; \text{round}\left(\frac{x - x_{\min}}{x_{\max} - x_{\min}} \cdot (2^b - 1)\right)<br>$$<br>其中，$x_{\min}$ 和 $x_{\max} $ 分别是最小值和最大值，( b ) 是量化位数。</p><h4 id="2-模型剪枝（Pruning）"><a href="#2-模型剪枝（Pruning）" class="headerlink" title="2.模型剪枝（Pruning）"></a>2.模型剪枝（Pruning）</h4><p>模型剪枝是通过移除不重要的权重或神经元来减少模型的参数数量和计算需求的过程。</p><p><strong>类型</strong>：</p><ul><li><strong>权重剪枝（Weight Pruning）</strong>：移除权重值接近于零的连接。</li><li><strong>结构化剪枝（Structured Pruning）</strong>：移除整个神经元、卷积核或通道。</li><li><strong>非结构化剪枝（Unstructured Pruning）</strong>：逐个移除权重，但可能导致稀疏矩阵的产生。</li></ul><p><strong>剪枝方法</strong>：</p><ul><li><strong>全局剪枝（Global Pruning）</strong>：基于全局阈值剪枝。</li><li><strong>层级剪枝（Layer-wise Pruning）</strong>：每层独立应用剪枝。</li><li><strong>渐进剪枝（Iterative Pruning）</strong>：逐步应用剪枝，并在每次剪枝后微调模型。</li></ul><p><strong>实现步骤</strong>：</p><ol><li><strong>训练初始模型</strong>。</li><li><strong>评估权重的重要性</strong>。</li><li><strong>根据阈值移除不重要的权重</strong>。</li><li><strong>微调模型</strong>以恢复性能。</li></ol><h4 id="3-模型蒸馏（Distillation）"><a href="#3-模型蒸馏（Distillation）" class="headerlink" title="3. 模型蒸馏（Distillation）"></a>3. 模型蒸馏（Distillation）</h4><p>模型蒸馏是通过训练一个较小的学生模型来模仿较大且更复杂的教师模型的行为，从而达到压缩模型的目的。</p><p><strong>步骤</strong>：</p><ol><li><strong>训练教师模型</strong>：首先训练一个高性能的教师模型。</li><li><strong>生成软标签（Soft Targets）</strong>：教师模型在训练数据上的预测输出，通常是带有温度参数 ( $T$ ) 的 softmax 输出。</li><li><strong>训练学生模型</strong>：使用软标签和原始标签联合训练学生模型。</li></ol><p><strong>损失函数</strong>：<br>学生模型的损失函数由软标签损失和硬标签损失的加权和组成：<br>$$<br>\mathcal{L} &#x3D; \alpha \mathcal{L}<em>{\text{hard}} + (1 - \alpha) \mathcal{L}</em>{\text{soft}}<br>$$</p><p>其中，( $\alpha$ ) 是权重系数，( $\mathcal{L}<em>{\text{hard}}$ ) 是传统的交叉熵损失，( $\mathcal{L}</em>{\text{soft}} $) 是带温度参数的软标签损失。</p><p><strong>软标签损失</strong>：<br>$$<br>\mathcal{L}_{\text{soft}} &#x3D; \text{CrossEntropy}\left(\frac{\exp(z_i &#x2F; T)}{\sum_j \exp(z_j &#x2F; T)}, \frac{\exp(z_i^T &#x2F; T)}{\sum_j \exp(z_j^T &#x2F; T)}\right)<br>$$<br>其中，( $z_i$ ) 是学生模型的logits，( $z_i^T$ ) 是教师模型的logits，( $T$ ) 是温度参数。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结:"></a>总结:</h4><ul><li><strong>量化</strong>：减少模型精度，降低存储和计算需求。</li><li><strong>剪枝</strong>：移除不重要的权重或神经元，减少参数数量和计算需求。</li><li><strong>蒸馏</strong>：使用高性能的教师模型指导训练较小的学生模型，从而压缩模型。</li></ul><p>这三种技术可以独立使用，也可以结合使用，以在不同的应用场景中达到最佳的模型压缩效果和推理效率。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>BasicKnowledge</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch</title>
    <link href="/2024/05/22/Pytorch/"/>
    <url>/2024/05/22/Pytorch/</url>
    
    <content type="html"><![CDATA[<h3 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h3><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs oxygene"><span class="hljs-keyword">Module</span>作为模块封装的父类，可以是一段逻辑，也可以是模型的一个块「<span class="hljs-keyword">block</span>」或一层。<br>Pytorch中自定义模型只需要继承<span class="hljs-keyword">Module</span>，保存好Param并提供<span class="hljs-keyword">forward</span>方法，backward被tensor的自动微分自动完成。<br></code></pre></td></tr></table></figure><hr><p>以一个简单的MLP的代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MLP</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># Call the constructor of the parent class nn.Module to perform</span><br>        <span class="hljs-comment"># the necessary initialization</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.hidden = nn.LazyLinear(<span class="hljs-number">256</span>)<br>        self.out = nn.LazyLinear(<span class="hljs-number">10</span>)<br>    <span class="hljs-comment"># Define the forward propagation of the model, that is, how to return the</span><br>    <span class="hljs-comment"># required model output based on the input X</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> self.out(F.relu(self.hidden(X)))<br></code></pre></td></tr></table></figure><h4 id="1-回调「Hook」函数介绍"><a href="#1-回调「Hook」函数介绍" class="headerlink" title="1. 回调「Hook」函数介绍"></a>1. 回调「Hook」函数介绍</h4><p><strong>hook 函数机制</strong>：不改变主体，实现额外功能，像一个挂件、挂钩 ➡️ hook</p><h5 id="1-1-为什么会有-hook-函数这个机制：参考文章1"><a href="#1-1-为什么会有-hook-函数这个机制：参考文章1" class="headerlink" title="1.1 为什么会有 hook 函数这个机制：参考文章1"></a>1.1 <strong>为什么会有 hook 函数这个机制：</strong><a href="https://blog.csdn.net/weixin_44878336/article/details/133859089">参考文章1</a></h5><blockquote><p>这与 <strong>PyTorch 动态图运行机制</strong>有关:</p><p>在动态图运行机制中，当运算结束后，<font color="red">一些中间变量是会被释放掉的，比如特征图、非叶子节点的梯度。</font>但有时候我们又想要继续关注这些中间变量，那么就可以使用 hook 函数在主体代码中提取中间变量。</p><p>主体代码主要是模型的<strong>前向传播「forward」和反向传播「backward」</strong>，额外的功能就是对模型的中间变量进行操作如：</p><ol><li>提取&#x2F;修改张量梯度</li><li>提取&#x2F;保留非叶子张量的梯度</li><li>查看模型的层与层之间的数据传递情况（数据维度、数据大小等）</li><li>在不修改原始模型代码的基础上可视化各个卷积特征图</li><li>……</li></ol></blockquote><h5 id="1-2-演示Hook的作用：参考文章2"><a href="#1-2-演示Hook的作用：参考文章2" class="headerlink" title="1.2 演示Hook的作用：参考文章2"></a>1.2 <strong>演示Hook的作用</strong>：<a href="https://cloud.tencent.com/developer/article/1745455">参考文章2</a></h5><blockquote><p>一般来说，“hook”是在特定事件之后自动执行的函数。</p><p>PyTorch 为nn.Module 对象 &#x2F; 每个张量<font color="red"><strong>注册</strong></font> hook。hook 由对象的向前或向后传播触发。它们具有以下函数签名:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn, Tensor<br><br><span class="hljs-comment"># For nn.Module objects only.</span><br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">module_hook</span>(<span class="hljs-params">module: nn.Module, <span class="hljs-built_in">input</span>: Tensor, output: Tensor</span>):<br>    <br><span class="hljs-comment"># For Tensor objects only.</span><br>   <span class="hljs-comment"># Only executed during the *backward* pass!</span><br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">tensor_hook</span>(<span class="hljs-params">grad: Tensor</span>):<br></code></pre></td></tr></table></figure></blockquote><ul><li><p><strong>例子1</strong>：假如你想知道每个层输出的形状。我们可以创建一个简单的 wrapper，使用 hook 打印输出形状:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn, Tensor<br><span class="hljs-keyword">from</span> torchvision.models <span class="hljs-keyword">import</span> resnet50<br><span class="hljs-keyword">import</span> warnings<br><br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VerboseExecution</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model: nn.Module</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 传入resnet50模型</span><br>        self.model = model<br><br>        <span class="hljs-keyword">for</span> name, layer <span class="hljs-keyword">in</span> self.model.named_children():<br>            layer.__name__ = name<br>            <span class="hljs-comment"># Register a hook for each layer</span><br>            layer.register_forward_hook(<br>                <span class="hljs-keyword">lambda</span> layer, _, output: <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;layer.__name__&#125;</span>: <span class="hljs-subst">&#123;output.shape&#125;</span>&quot;</span>)<br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x: Tensor</span>) -&gt; Tensor:<br>        <span class="hljs-keyword">return</span> self.model(x)<br><br><br>verbose_resnet = VerboseExecution(resnet50())<br>dummy_input = torch.ones(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)<br><br>_ = verbose_resnet(dummy_input)<br><br><span class="hljs-comment"># --------输出</span><br><span class="hljs-comment"># conv1: torch.Size([10, 64, 112, 112])</span><br><span class="hljs-comment"># bn1: torch.Size([10, 64, 112, 112])</span><br><span class="hljs-comment"># relu: torch.Size([10, 64, 112, 112])</span><br><span class="hljs-comment"># maxpool: torch.Size([10, 64, 56, 56])</span><br><span class="hljs-comment"># layer1: torch.Size([10, 256, 56, 56])</span><br><span class="hljs-comment"># layer2: torch.Size([10, 512, 28, 28])</span><br><span class="hljs-comment"># layer3: torch.Size([10, 1024, 14, 14])</span><br><span class="hljs-comment"># layer4: torch.Size([10, 2048, 7, 7])</span><br><span class="hljs-comment"># avgpool: torch.Size([10, 2048, 1, 1])</span><br><span class="hljs-comment"># fc: torch.Size([10, 1000])</span><br></code></pre></td></tr></table></figure></li><li><p><strong>例子2</strong>：<strong>特征提取</strong>：通常，我们希望从一个预先训练好的网络中生成特性，然后用它们来完成另一个任务(例如分类等)。使用 hook，我们可以提取特征，而不需要重新创建现有模型或以任何方式修改它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn, Tensor<br><span class="hljs-keyword">from</span> torchvision.models <span class="hljs-keyword">import</span> resnet50<br><span class="hljs-keyword">import</span> warnings<br><br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br><br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Dict</span>, Iterable, <span class="hljs-type">Callable</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeatureExtractor</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model: nn.Module, layers: Iterable[<span class="hljs-built_in">str</span>]</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.model = model<br>        self.layers = layers<br>        self._features = &#123;layer: torch.empty(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> layers&#125;<br><br>        <span class="hljs-keyword">for</span> layer_id <span class="hljs-keyword">in</span> layers:<br>            layer = <span class="hljs-built_in">dict</span>([*self.model.named_modules()])[layer_id]<br>            <span class="hljs-comment"># Register a hook</span><br>            layer.register_forward_hook(self.save_outputs_hook(layer_id))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_outputs_hook</span>(<span class="hljs-params">self, layer_id: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-type">Callable</span>:<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">fn</span>(<span class="hljs-params">_, __, output</span>):<br>            self._features[layer_id] = output<br>        <span class="hljs-keyword">return</span> fn<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x: Tensor</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, Tensor]:<br>        _ = self.model(x)<br>        <span class="hljs-keyword">return</span> self._features<br><br>resnet_features = FeatureExtractor(resnet50(), layers=[<span class="hljs-string">&quot;layer4&quot;</span>, <span class="hljs-string">&quot;avgpool&quot;</span>])<br>dummy_input = torch.ones(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)<br>features = resnet_features(dummy_input)<br><br><span class="hljs-built_in">print</span>(&#123;name: output.shape <span class="hljs-keyword">for</span> name, output <span class="hljs-keyword">in</span> features.items()&#125;)<br><br><span class="hljs-comment"># 输出</span><br><span class="hljs-comment"># &#123;&#x27;layer4&#x27;: torch.Size([10, 2048, 7, 7]), &#x27;avgpool&#x27;: torch.Size([10, 2048, 1, 1])&#125;</span><br></code></pre></td></tr></table></figure></li><li><p><strong>例子3：梯度裁剪</strong>：梯度裁剪是处理梯度爆炸的一种著名方法。PyTorch 已经提供了梯度裁剪的工具方法，但是我们也可以很容易地使用 hook 来实现它。其他任何用于梯度裁剪&#x2F;归一化&#x2F;修改的方法都可以用同样的方式实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient_clipper</span>(<span class="hljs-params">model: nn.Module, val: <span class="hljs-built_in">float</span></span>) -&gt; nn.Module:<br>    <span class="hljs-keyword">for</span> parameter <span class="hljs-keyword">in</span> model.parameters():<br>      <span class="hljs-comment"># Register a hook for each parameter</span><br>        <br>        <span class="hljs-comment"># register_hook 方法的主要作用是允许用户在张量的梯度计算中注册一个自定义函数</span><br>        <span class="hljs-comment"># 以便在反向传播期间对梯度进行操作或记录信息。</span><br>        <span class="hljs-comment"># 这对于实现自定义梯度处理、梯度剪裁、可视化梯度信息以及梯度的修改等任务非常有用。</span><br>        parameter.register_hook(<span class="hljs-keyword">lambda</span> grad: grad.clamp_(-val, val))<br>    <br>    <span class="hljs-keyword">return</span> model<br><br>clipped_resnet = gradient_clipper(resnet50(), <span class="hljs-number">0.01</span>)<br>pred = clipped_resnet(dummy_input)<br>loss = pred.log().mean()<br>loss.backward()<br><br><span class="hljs-built_in">print</span>(clipped_resnet.fc.bias.grad[:<span class="hljs-number">25</span>])<br><br><span class="hljs-comment"># 输出</span><br><span class="hljs-comment"># tensor([-0.0010, -0.0047, -0.0010, -0.0009, -0.0015,  0.0027,  0.0017, -0.0023,</span><br><span class="hljs-comment">#          0.0051, -0.0007, -0.0057, -0.0010, -0.0039, -0.0100, -0.0018,  0.0062,</span><br><span class="hljs-comment">#          0.0034, -0.0010,  0.0052,  0.0021,  0.0010,  0.0017, -0.0100,  0.0021,</span><br><span class="hljs-comment">#          0.0020])</span><br></code></pre></td></tr></table></figure></li></ul><h5 id="1-3一些常见的Hook函数："><a href="#1-3一些常见的Hook函数：" class="headerlink" title="1.3一些常见的Hook函数："></a>1.3<strong>一些常见的Hook函数</strong>：</h5><ul><li><p><code>register_forward_hook</code> 是 PyTorch 中用于在神经网络的<strong>前向传播过程中</strong>注册钩子的一个函数。这个钩子函数会在模块执行其 <code>forward</code> 方法时被调用，可以用来检查或修改中间输出。</p><p><code>module.register_forward_hook(hook)</code>:</p><p><img src="/2024/05/22/Pytorch/image-20240522161226987.png" alt="register_forward_hook"></p></li><li><p><code>register_hook</code> 是 PyTorch 中用于在神经网络的<strong>反向传播过程中</strong>注册钩子的函数。这个钩子函数会在张量的梯度计算过程中被调用，主要用于调试和修改梯度。</p><p><code>tensor.register_hook(hook)</code>:</p><p><img src="/2024/05/22/Pytorch/image-20240522161159145.png" alt="register_hook"></p></li><li><p>回调「Hook」函数注册</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 三个全局变量，dict类型，存储回调函数（即hook），用于net中的所有module</span><br><span class="hljs-comment"># 用于输入输出tensor</span><br>_global_buffer_registration_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] = OrderedDict()<br><span class="hljs-comment"># 用于module定义</span><br>_global_module_registration_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] = OrderedDict()<br><span class="hljs-comment"># 用于模型参数</span><br>_global_parameter_registration_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] = OrderedDict()<br><br><br><span class="hljs-string">r&quot;&quot;&quot;This tracks hooks common to all modules that are executed before/after</span><br><span class="hljs-string">calling forward and backward. This is global state used for debugging/profiling</span><br><span class="hljs-string">purposes&quot;&quot;&quot;</span> <br><span class="hljs-comment"># 用于在module的forward和backward接口前后注册回调函数，例如dump出每个op的输入输出结果</span><br>_global_backward_pre_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] = OrderedDict()<br>_global_backward_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] = OrderedDict()<br>_global_is_full_backward_hook: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span><br>_global_forward_pre_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] = OrderedDict()<br>_global_forward_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] = OrderedDict()<br><br><span class="hljs-comment"># 提供reg接口在完成回调函数注册</span><br>register_module_buffer_registration_hook()<br>register_module_module_registration_hook()<br>register_module_parameter_registration_hook()<br><br>register_module_forward_pre_hook()<br>register_module_forward_hook()<br>register_module_backward_hook()<br>register_module_full_backward_pre_hook()<br>register_module_full_backward_hook()<br></code></pre></td></tr></table></figure></li></ul><h5 id="1-4-Module成员变量分析"><a href="#1-4-Module成员变量分析" class="headerlink" title="1.4 Module成员变量分析"></a>1.4 Module成员变量分析</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 版本号，一个内部使用的属性，用于跟踪模块的版本。这一机制主要用于在序列化（serialization）和反序列化（deserialization）过程中管理模型的兼容性。</span><br>_version: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span> <br><span class="hljs-comment"># 一个布尔值，表示模块是否处于训练模式。可以通过 model.train() 和 model.eval() 方法切换。</span><br>training: <span class="hljs-built_in">bool</span> <br><span class="hljs-comment"># 存储模块的所有参数（Parameter 对象），类型为 OrderedDict，如conv的weight、bias等</span><br>_parameters: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Optional</span>[Parameter]] <br><span class="hljs-comment"># 存储模块中的所有缓冲区（Tensor 对象），类型为 OrderedDict。缓冲区是模型状态的一部分，但不是参数，比如 BatchNorm 的running mean 和 running variance。</span><br>_buffers: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Optional</span>[Tensor]] <br><span class="hljs-comment"># 存储模块的子模块，类型为 OrderedDict。每个子模块在模型中都有一个唯一的名称。</span><br>_modules: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Optional</span>[<span class="hljs-string">&#x27;Module&#x27;</span>]]<br><br><br><span class="hljs-comment"># 存储反向传播前的钩子，类型为 OrderedDict。这些钩子在反向传播前的过程中被调用。</span><br>_backward_pre_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] <br><span class="hljs-comment"># 存储反向传播钩子，类型为 OrderedDict。这些钩子在反向传播过程中被调用。</span><br>_backward_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>]<br><span class="hljs-comment"># 存储前向传播钩子，类型为 OrderedDict。这些钩子在前向传播过程中被调用。</span><br>_forward_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>]<br><br><br><span class="hljs-comment"># 存储 state_dict 钩子，类型为 OrderedDict。这些钩子在调用 state_dict 时被调用。</span><br>_state_dict_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>]  // 模型加载时，op的参数加载相关的回调函数<br>_load_state_dict_pre_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>]<br>_state_dict_pre_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>]<br>_load_state_dict_post_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>]<br><br></code></pre></td></tr></table></figure><h5 id="1-5-Module方法分析"><a href="#1-5-Module方法分析" class="headerlink" title="1.5 Module方法分析"></a>1.5 Module方法分析</h5>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fatser R-CNN Source Code</title>
    <link href="/2024/05/21/Faster-RCNN_SourceCode/"/>
    <url>/2024/05/21/Faster-RCNN_SourceCode/</url>
    
    <content type="html"><![CDATA[<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="/2024/05/21/Faster-RCNN_SourceCode/v2-4c23436f431a535e2cc2e9919b3ca10f_r.jpg" alt="Faster RCNN model"></p><h3 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h3><p>1. </p><h3 id="PyTorch代码学习"><a href="#PyTorch代码学习" class="headerlink" title="PyTorch代码学习"></a>PyTorch代码学习</h3><ol><li><p><code>torch._assert(False, &quot;XXX&quot;)</code></p><blockquote><p>assert函数叫做断言函数，它可以用于判断某个表达式的值，若是该值为真，那么程序就能够继续往下执行; 反之，会报出AssertionError错误。</p></blockquote></li><li><p><code>if isinstance(boxes, torch.Tensor)</code></p><blockquote><p>判断两个类型是否相同</p></blockquote></li></ol>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Object Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RCNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PaddlePaddle-CPPD</title>
    <link href="/2024/05/19/PaddlePaddle_CPPD/"/>
    <url>/2024/05/19/PaddlePaddle_CPPD/</url>
    
    <content type="html"><![CDATA[<ul><li><h4 id="准备好训练数据集"><a href="#准备好训练数据集" class="headerlink" title="准备好训练数据集"></a>准备好训练数据集</h4><p>以PARSeq（lmdb数据格式）为例，在项目文件中如下：</p><p><img src="/2024/05/19/PaddlePaddle_CPPD/image-20240519204701863.png" alt="image-20240519204701863"></p></li><li><h4 id="报错解决"><a href="#报错解决" class="headerlink" title="报错解决"></a>报错解决</h4><ol><li><p>CUDA和CUDNN的配置问题</p><p><img src="/2024/05/19/PaddlePaddle_CPPD/image-20240519174249876.png" alt="image-20240519174249876"></p><p><img src="/2024/05/19/PaddlePaddle_CPPD/image-20240519202939423.png" alt="image-20240519202939423">解决办法：</p><blockquote><ol><li>下载正确CUDA版本的CUDA Toolkit</li><li>下载正确CUDA版本的cuDNN，将dll动态链接库文件复制到CUDA Toolkit中</li></ol></blockquote></li><li><p>其他报错解决</p></li></ol></li><li><h4 id="成功训练"><a href="#成功训练" class="headerlink" title="成功训练"></a>成功训练</h4><p><img src="/2024/05/19/PaddlePaddle_CPPD/image-20240519204959939.png" alt="image-20240519204959939"></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Optical Character Recognition</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CPPD</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>R-CNN Series</title>
    <link href="/2024/05/15/RCNN/"/>
    <url>/2024/05/15/RCNN/</url>
    
    <content type="html"><![CDATA[<h3 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h3><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs arcade">R-CNN源于<span class="hljs-number">2014</span>年伯克利大学的这篇论文<br>《Rich <span class="hljs-built_in">feature</span> hierarchies <span class="hljs-keyword">for</span> accurate object detection and semantic segmentation》<br>其架构和模型训练参数等借鉴了AlexNet，也和同时期的Overfeat也有很多共同之处。成为目标检测领域尤其是two stage模式的开山鼻祖。<br></code></pre></td></tr></table></figure><hr><p><img src="/2024/05/15/RCNN/R-CNN.png" alt="R-CNN网络结构"></p><ul><li><h4 id="R-CNN-算法流程分四个步骤："><a href="#R-CNN-算法流程分四个步骤：" class="headerlink" title="R-CNN 算法流程分四个步骤："></a>R-CNN 算法流程分四个步骤：</h4><ol><li><p>一张图像生成1k～2k个候选区域（使用 <strong><u>Selective Search</u></strong> 方法)[<a href="https://zhuanlan.zhihu.com/p/485727819]">https://zhuanlan.zhihu.com/p/485727819]</a></p><blockquote><p>候选区域生成 (Selective Search) 的主要思想：图像中物体可能存在的区域应该是有某些相似性或者连续性区域的。</p><ol><li>首先，对输入图像进行<strong>分割算法</strong>产生许多小的子区域。</li><li>其次，根据这些子区域之间相似性(相似性标准主要有颜色、纹理、大小等等)进行<strong>区域合并</strong>，不断的进行区域迭代合并。每次迭代过程中对这些合并的子区域做bounding boxes(外切矩形)，这些子区域外切矩形就是通常所说的候选框。</li></ol><p><img src="/2024/05/15/RCNN/SS%E7%AE%97%E6%B3%95.png" alt="SS算法"></p></blockquote></li><li><p>对每个候选区域，使用深度网络（图片分类网络）提取特征</p><blockquote><ol><li>通过SS算法可以在一张图片中生成大概2000个候选区域，将候选区域送到CNN网络之前先进行<strong>resize处理</strong>，将2000候选区域缩放到 227 × 227 (原文是不管候选框多大都resize到 227 × 227)</li><li>接着将候选区域输入事先训练好的AlexNet &#x2F; VGG CNN网络<strong>获取4096维的特征</strong>，得到2000×4096维矩阵。</li></ol></blockquote></li><li><p>特征送入每一类SVM分类器，判断是否属于该类</p><blockquote><ol><li><p>将<code>2000×4096</code>维特征与<code>20</code>个<strong>SVM</strong>组成的权值矩阵<code>4096×20</code>相乘，获得<code>2000×20</code>维矩阵表示每个建议框是某个目标类别的得分</p><p><img src="/2024/05/15/RCNN/SVM.png" alt="SVM分类器计算矩阵"></p></li><li><p>分别对上述<code>2000×20</code>维矩阵中每一列即每一类进行**NSM(非极大值抑制)**剔除重叠建议框，得到该列即该类中得分最高的一些建议框。</p></li></ol></blockquote></li><li><p>使用回归器精细修正候选框位置。（使用 Selective Search 算法得到的候选框并不是框得那么准）</p><blockquote><ol><li>对NMS处理后剩余的建议框进一步筛选</li><li>分别用20个<strong>回归器</strong>对上述20个类别中剩余的建议框进行回归操作，最终得到每个类别的修正后的得分最高的bounding box</li></ol><p>(原文)：</p><ol><li>保留与真实目标比IoU大于某一阈值的预测框，不满足的直接删除</li><li>接着再分别使用20个回归器对剩余的预测框进行回归操作，最终得到每个类别的修正后得分最高的预测框。这里的实现方法跟上面的SVM分类差不多，依旧是对卷积神经网络输出的特征向量进行预测，利用每个边界框得到4096维特征向量来预测的。通过回归分类器之后会得到四个参数分别对应着目标建议框的中心点的x,y偏移量和目标边界框的宽高缩放因子。通过预测的四个值对得到的建议框进行调整得到最终的预测边界框。</li></ol></blockquote></li></ol></li><li><h4 id="RCNN存在的问题"><a href="#RCNN存在的问题" class="headerlink" title="RCNN存在的问题"></a>RCNN存在的问题</h4><ol><li>检测速度慢，测试一张图片约53s (CPU)。用Selective Search算法提取候选框用时约2秒，一张图像内候选框之间存在大量重叠，提取特征操作冗余。</li><li>训练速度慢，并且训练过程极其复杂。</li><li>训练所需空间大，对于SVM和bbox回归训练，需要从每个图像中的每个目标候选框提取特征，并写入磁盘。对于非常深的网络，如VGG16，从VOC07训练集上的5k图像上提取的特征需要数百GB的存储空间。</li></ol></li></ul><h3 id="SPPNet"><a href="#SPPNet" class="headerlink" title="SPPNet"></a>SPPNet</h3><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mathematica">由提出<span class="hljs-variable">ResNet</span>的何凯明在论文《<span class="hljs-variable">Spatial</span> <span class="hljs-built_in">Pyramid</span> <span class="hljs-variable">Pooling</span> <span class="hljs-variable">in</span> <span class="hljs-variable">Deep</span> <span class="hljs-variable">Convolutional</span> <span class="hljs-variable">Networks</span> <span class="hljs-variable">for</span> <span class="hljs-variable">Visual</span> <span class="hljs-variable">Recognition</span>》中提出<br>主要就是可以解决<span class="hljs-variable">CNN</span>输入需要固定尺寸的问题，而且在分类和目标检测中都可以得到比较好的效果<br></code></pre></td></tr></table></figure><hr><p><img src="/2024/05/15/RCNN/SPPNet.png" alt="SPPNet网络结构"></p><ul><li><h4 id="针对R-CNN的缺点："><a href="#针对R-CNN的缺点：" class="headerlink" title="针对R-CNN的缺点："></a>针对R-CNN的缺点：</h4><p>1).对每个候选区域提取特征。2).输入CNN的候选区域大小固定，要经过Resize。SPPNet做出了以下改进，<strong>具体内容看表格</strong>：</p><ol><li><p>SPPNet让SS算法得到<strong>候选区域与feature map直接映射</strong>，得到候选区域的映射特征向量（这是映射来的，不需要每个候选区域都再经过CNN的计算，极大的减少了计算量）。</p></li><li><p>SPPNet引入一种<u><strong>空间金字塔池化( spatial pyramid pooling，SPP)层</strong></u>以移除网络对固定尺寸的限制（不需要候选区域经过crop&#x2F;wrap等操作变换成固定大小的图像）。</p><p><img src="/2024/05/15/RCNN/feature_map_regions.png" alt="image regions vs feature map regions"></p><table><thead><tr><th>R-CNN模型</th><th>SPPNet模型</th></tr></thead><tbody><tr><td>1.R-CNN让每个候选区域经过crop&#x2F;wrap等操作变换成固定大小的图像.  2. 固定大小的每个候选区域塞给CNN 传给后面的层做训练回归、分类操作</td><td>1.SPPNet把全图塞给CNN得到全图的feature map.  2.让SS算法得到候选区域与feature map直接映射，得到候选区域的映射特征向量(这是映射来的，不需要过CNN). 3.映射过来的特征向量大小不固定，所以这些特征向量塞给SPP层(空间金字塔变换层)，SPP层接收任何大小的输入，输出固定大小的特征向量，再塞给FC层. 4.经过映射+SPP转换，简化了计算，速度&#x2F;精确度也上去了</td></tr></tbody></table></li></ol></li><li><h4 id="两个关键问题："><a href="#两个关键问题：" class="headerlink" title="两个关键问题："></a>两个关键问题：</h4><ul><li><p><input checked disabled type="checkbox"> <strong>SPP层怎么可以接收任意大小的输入，输出固定的向量？</strong></p><p>spp layer会将<strong>每个候选区域分成1x1，2x2，4x4三张子图</strong>，对每个子图的每个区域作<strong>max pooling</strong>，得出的特征再<strong>连接</strong>到一起就是(16+4+1)x256&#x3D;21x256&#x3D;5376维向量，接着给全连接层做进一步处理。</p></li></ul><ul><li><p><input checked disabled type="checkbox"> <strong>SPPNet怎么就能把候选区域从全图的feature map 直接得到特征向量？</strong></p><p><img src="/2024/05/15/RCNN/Feature_map.png" alt="feature map">整个映射过程有具体的公式：$(x,y)&#x3D;(S<em>x’,S</em>y’)$ $(x’,y’)&#x3D;([x&#x2F;S]+1,[y&#x2F;S]+1)$（左上角+，右下角-)</p><p>其中 S 就是CNN中<strong>所有的strides的乘积</strong>，包含了池化、卷积的stride。论文中使用S的计算出来为2x2x2x2&#x3D;16,在ZF-5结构中。</p></li></ul></li></ul><h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h3><figure class="highlight objectivec"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs objectivec">Fast R-<span class="hljs-built_in">CNN</span> 是作者Ross Girshick继 R-<span class="hljs-built_in">CNN</span> 后的又一力作，论文名就叫做 Fast R-<span class="hljs-built_in">CNN</span>，<span class="hljs-number">2015</span>年发表。<br>同样使用VGG16作为网络的backbone，与 R-<span class="hljs-built_in">CNN</span> 相比训练时间快了 <span class="hljs-number">9</span> 倍，测试推理时间快了 <span class="hljs-number">213</span> 倍，准确率从 <span class="hljs-number">62</span>% 提升至了 <span class="hljs-number">66</span>%（在 Pascal VOC 数据集上）<br></code></pre></td></tr></table></figure><p>参考文章：</p><p><a href="https://zhuanlan.zhihu.com/p/165324194">Fast R-CNN</a></p><p><a href="https://www.cnblogs.com/yymn/articles/13629478.html">ROI Pooling</a></p><p><a href="https://www.jianshu.com/p/670a3e42107d">RoI Pooling及其改进</a></p><hr><p><img src="/2024/05/15/RCNN/Fast_R-CNN.png" alt="Fast R-CNN网络结构"></p><ul><li><h4 id="Fast-R-CNN-算法流程分四个步骤："><a href="#Fast-R-CNN-算法流程分四个步骤：" class="headerlink" title="Fast R-CNN 算法流程分四个步骤："></a>Fast R-CNN 算法流程分四个步骤：</h4><ol><li><p>一张图像生成1k～2k个候选区域（使用 Selective Search 方法）</p></li><li><p>将图像输入网络得到相应的特征图，将 Selective Search 算法生成的候选框投影到特征图上获得相应的特征矩阵</p></li><li><p>将每个特征矩阵通过 ROI pooling层缩放为$ 7 \times 7$大小的特征图  </p><blockquote><p>ROI「region of interests」：指的是矩形框框出的区域，可能是有目标的也可能没目标，概念上等价于proposal region。</p><p>ROI Pooling层的具体做法是：</p><p>对候选框所对应的特征矩阵，将其划分为7*7，也就是49等份。划分之后，对每一个区域做一个最大池化下采样操作，也就是MaxPooling操作。如此对49等分的候选区域操作，便得到了一个7*7的特征矩阵。</p><p>也就是说，无论候选框的特征矩阵是怎么样的尺寸，都被缩放到一个7*7的大小，这样就可以不去限制输入图像的尺寸了。</p></blockquote></li><li><p>接着将特征图展平通过一系列全连接层获得预测结果</p><blockquote><p>最后并联了两个全连接层分别对分类和bbox进行预测。</p><p>分类结点数为 N+1，因为需要增加上背景。bbox预测的全连接层则是$4*(N+1)$个结点，对每一类都要预测出来边界框回归参数。</p><p><img src="/2024/05/15/RCNN/%E8%BE%B9%E7%95%8C%E6%A1%86%E5%9B%9E%E5%BD%92%E5%99%A8.png" alt="边界框回归器"></p><p><img src="/2024/05/15/RCNN/image-20240519120214206.png" alt="边界框回归器"></p></blockquote></li></ol></li><li><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>Fast RCNN将分类与回归做到了一个网络里面，因此损失函数必定是多任务的：<br>$$<br>L(p,u,t^u,v)&#x3D;L_{cls}(p,u)+\lambda[u\geq1]L_{loc}(t^u,v)<br>$$</p><ol><li><p><strong>分类损失</strong>：$L_{cls}(p,u) $ </p><blockquote><p>使用交叉熵损失，<code>p</code>为预测结果的向量表示，<code>u</code>为真实类别的标签数据。</p></blockquote></li><li><p><strong>bbox回归器损失</strong>：$\lambda[u&gt;&#x3D;1]L_{loc}(t^u,v)$ ：</p><blockquote><ul><li><p>$\lambda$: 用于调节两部分损失函数的比例，一般取1</p></li><li><p>$[u\geq1]$: 是因为将<code>u</code>定义成了真实类别的索引，而且将background这一类定义成了0。所以如果标签是0的时候，这部分是不需要计算bbox的损失函数的，因为background不需要bbox。</p></li><li><p>$L_{loc}(t^u,v)$<br>$$<br>L_{loc}(t^u,v)&#x3D;\sum_{i\in{x,y,w,h}}smooth_{L1}(t_i^u-v_i)<br>$$</p><p>$$<br>smooth_{L1}(x)&#x3D;<br>\begin{cases}<br>0.5x^2 &amp;if\space|x|&lt;1\<br>|x|-0.5 &amp;otherwise<br>\end{cases}<br>$$</p><p>这一部分就是把预测框与groud truth中的x, y, w, h都单独拿出来进行相减（实际上就是L1的损失函数），然后计算smooth函数($Smooth_{L1}$)，最后把这四个的smooth进行相加就是bbox回归器的损失函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">smooth_l1</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    <span class="hljs-comment"># y_true和y_pred是特征位置的真实值和预测值</span><br>    abs_diff = K.<span class="hljs-built_in">abs</span>(y_true - y_pred)<br>    less_than_one = K.cast(K.less(abs_diff, <span class="hljs-number">1.0</span>), <span class="hljs-string">&quot;float32&quot;</span>)<br>    loss = (<span class="hljs-number">0.5</span> * abs_diff**<span class="hljs-number">2</span>) * less_than_one + (abs_diff - <span class="hljs-number">0.5</span>) * (<span class="hljs-number">1</span> - less_than_one)<br>    <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure></li></ul></blockquote></li></ol></li></ul><h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><figure class="highlight objectivec"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs objectivec">Faster R-<span class="hljs-built_in">CNN</span> 是作者 Ross Girshick 继 Fast R-<span class="hljs-built_in">CNN</span> 后的又一力作，<br>同样使用 VGG16 作为 backbone，推理速度在 GPU 上达到 <span class="hljs-number">5</span>fps（每秒检测五张图，包括候选区域生成），准确度也有一定的进步。核心在于 RPN 区域生成网络（Region Proposal Network）。<br></code></pre></td></tr></table></figure><p><a href="https://www.zhihu.com/tardis/bd/art/31426458">这篇文章</a>介绍Faster R-CNN写的非常非常详细！</p><hr><p><img src="/2024/05/15/RCNN/ff2cd606a307b927701694ec05d0f599.png" alt="Faster R-CNN网络结构"></p><ul><li><h4 id="Faster-R-CNN-算法流程分三个步骤："><a href="#Faster-R-CNN-算法流程分三个步骤：" class="headerlink" title="Faster R-CNN 算法流程分三个步骤："></a>Faster R-CNN 算法流程分三个步骤：</h4><ol><li>将图像输入网络得到相应的特征图-feature map</li><li>使用RPN网络（Region Proposal Network）生成候选框，将 RPN 生成的候选框投影到特征图上获得相应的特征矩阵</li><li>将每个特征矩阵通过 ROI pooling 层缩放为$7 \times 7$大小的特征图，接着将特征图展平通过一系列全连接层获得预测结果</li></ol></li><li><h4 id="RPN-网络结构"><a href="#RPN-网络结构" class="headerlink" title="RPN 网络结构"></a><font color="red">RPN 网络结构</font></h4><blockquote><p><strong>滑动窗口生成anchors -&gt; softmax分类器提取positvie anchors -&gt; bbox reg回归positive anchors -&gt; Proposal Layer生成proposals</strong></p></blockquote><p><img src="/2024/05/15/RCNN/RPN.png" alt="Faster R-CNN细节"></p><p>流程细节：</p><ol><li><p>在经过backbone之后生成Feature Map大小为：<strong>H*W*N</strong></p><blockquote><p>N是根据使用backbone的通道数来定的，比如VGG16为512个通道，而使用ZF网络则是256个通道。</p></blockquote></li><li><p>对特征图做了3x3卷积，但输出层数保持不变（<strong>N</strong>），相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？反正我没测试）</p></li><li><p>在上一步的结果上使用滑动窗口，每滑动到一个位置生成一个一维的向量， 在向量的基础上通过两个全连接层去输出目标概率（背景&#x2F;非背景）和边界框回归参数。</p><blockquote><p>256-d是指使用ZF model作为backbone，最终每个点都用一个256维的向量表示，即<code>N=256</code>。</p></blockquote><p>1*1卷积+Cls layer + Reg layer：</p><blockquote><p>在<strong>分类任务「Cls layer」</strong>中，经过1*1卷积后，输出图像大小为：<strong>H*W*18</strong>，正好对应9个anchor boxes的2分类结果。为何要在softmax前后都接一个reshape layer？其实只是为了便于softmax分类，具体原因这就要从caffe的实现形式说起。</p><p>在<strong>回归任务「Reg layer」</strong>，经过1*1卷积后，输出图像大小为：<strong>H*W*36</strong>，正好对应9个anchor boxes的边界框。</p><p>2k 中的 k 指的是 k 个 anchor boxes，<strong>2是指为背景的概率和为前景的概率</strong>。</p><p>4k 中的 k 指的是 k 个 anchor boxes，<strong>4是指每个 anchor 有 4 个<font color="red">边界框回归参数</font>。</strong></p></blockquote><p><img src="/2024/05/15/RCNN/u=1361402012,3659437005&fm=253&fmt=auto&app=138&f=JPEG-20240520150557582.jpeg" alt="RPN"></p></li><li><p><font color="red">Proposal层</font></p><p>Proposal层共有三个输入：分类器结果【1，2*9，H，W】、边界框回归参数【1，4*9，H，W】、img_info</p><p>Proposal Layer forward（caffe layer的前向函数）按照以下顺序依次处理：</p><ol><li><p>生成anchors，利用$d_x(A),d_y(A),d_w(A),d_h(a)$对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致)</p></li><li><p>按照输入的positive softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的positive anchors</p></li><li><p>判定超出图像边界的positive anchors为图像边界，防止后续roi pooling时proposal超出图像边界</p></li><li><p>剔除尺寸非常小的positive anchors</p></li><li><p>对剩余的positive anchors进行NMS（nonmaximum suppression）</p></li><li><p>对应的bbox reg的结果作为proposal输出</p><blockquote><p>由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的，这点在后续网络中有用。另外我认为，严格意义上的检测应该到此就结束了，后续部分应该属于识别了。</p></blockquote></li></ol></li></ol></li><li><h4 id="ROI-Pooling层"><a href="#ROI-Pooling层" class="headerlink" title="ROI Pooling层"></a><font color="red">ROI Pooling层</font></h4><p>同Fast R-CNN中的ROI层作用和用法</p></li><li><h4 id="FC层"><a href="#FC层" class="headerlink" title="FC层"></a>FC层</h4><p>同Fast R-CNN中的FC层作用和用法</p></li><li><h4 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h4><p><strong>RPN的损失</strong>也分为两个部分：分类损失和边界框回归损失<br>$$<br>L({p_i},{t_i})&#x3D;\frac{1}{N_{cls}}\sum_{i}L_{cls}(p_i,p_i^*)+\lambda\frac{1}{N_{reg}}\sum_{i}{p_i^*}L_{reg}(t_i,t_i^*)<br>$$</p></li></ul><h3 id="补充材料"><a href="#补充材料" class="headerlink" title="补充材料"></a>补充材料</h3><ul><li><p>Faster R-CNN</p><ol><li><p>实际上生成的那么多 anchors 并不是每个都用来训练 RPN 网络。对于每张图片我们从上万个 anchor 当中采样 256 个 anchor，这些 anchor 由正样本和负样本 1:1 组成的。如果正样本不足 128，就用负样本进行填充。两种定义正样本的方式：（1）anchor 与 ground-truth 的 iou 超过 0.7，（2）某个 anchor 与 ground-truth 拥有最大的 iou。负样本是与所有的 ground-truth 的 iou 都小于 0.3 的。</p></li><li><p>感受野</p><p><img src="/2024/05/15/RCNN/v2-6193e85eb99c691c051ef55840154f9e_r.jpg" alt="感受野"></p></li></ol></li></ul><p>​</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Object Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RCNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker</title>
    <link href="/2024/05/15/Docker/"/>
    <url>/2024/05/15/Docker/</url>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><hr><p>Docker是一个用于<strong>构建(build)</strong>,<strong>运行(run)</strong>,<strong>传送(share)</strong> 应用程序的平台</p><ol><li><strong>将应用程序打包为一个个的集装箱</strong>，小鲸鱼(Docker)就会帮我们将它们运送到任何需要的地方</li></ol><p><img src="/2024/05/15/Docker/docker.png" alt="集装箱"></p><ol start="2"><li><strong>每个集装箱将下列文件打包在一起</strong>（应用程序和它运行时所需要的各种依赖包、第三方软件库、配置文件等），以便在任何环境中都可以正确的运行</li></ol><p><img src="/2024/05/15/Docker/image-20240515112551349.png" alt="集装箱内文件"></p><h3 id="为什么要使用Docker"><a href="#为什么要使用Docker" class="headerlink" title="为什么要使用Docker"></a>为什么要使用Docker</h3><hr><p>比如我们写了一个网站：</p><ol><li><p>前端使用Vue搭建界面</p></li><li><p>后端使用SpringBoot微服务框架来提供各种服务和接口</p></li><li><p>使用MySQL数据库来存储数据</p></li></ol><ul><li><strong>如果没有使用Docker</strong>，我们在生产环境和测试环境中都需要进行各种配置：</li></ul><p><img src="/2024/05/15/Docker/image-20240515113511454.png" alt="开发环境和测试环境"></p><ul><li><strong>如果有了Docker</strong>，我们可以将这些（前端、后端、数据库等）打包成一个个的集装箱，只要在开发环境中运行成功了，在其他环境中也一定可以成功。</li></ul><p><img src="/2024/05/15/Docker/image-20240515113744151.png" alt="开发环境和测试环境"></p><h3 id="Docker和虚拟机的区别"><a href="#Docker和虚拟机的区别" class="headerlink" title="Docker和虚拟机的区别"></a>Docker和虚拟机的区别</h3><hr><ol><li>架构差异：虚拟机是基于<strong>虚拟化技术</strong>（<strong>hypervisor</strong>）实现的，创建一个完整的虚拟硬件环境，模拟一台计算机，包括处理器、内存、硬盘等设备。而Docker则是基于<strong>容器化技术</strong>（<strong>Containerization</strong>），使用Docker引擎访问宿主机操作系统，将应用程序打包到容器中。因此，Docker的架构更加轻量级，启动速度也更快。</li><li>隔离原理：虚拟机通常隔离整个操作系统，进程无法直接访问宿主机资源和数据，需要通过网络或共享文件夹等方式交互。Docker使用Linux内核的namespace和cgroups功能，实现对不同容器中进程的隔离，允许它们共享宿主机资源同时独立运行。</li></ol><h3 id="基本原理和概念"><a href="#基本原理和概念" class="headerlink" title="基本原理和概念"></a>基本原理和概念</h3><hr><p><strong>Docker中的镜像、容器和仓库</strong></p><ul><li><p>镜像和容器的关系就像Java中的类和实例的关系一样：</p><ol><li><strong>镜像是一个只读的模版</strong>，它可以用来创建容器</li><li><strong>容器是Docker的运行实例</strong>，它提供了一个独立的可移植的环境，可以在这个环境中运行应用程序</li></ol></li><li><p>Docker仓库是用来存储Docker镜像的地方，最流行和最常用的仓库就是Dockerhub，实现镜像的共享和复用</p></li></ul><h3 id="Docker的安装"><a href="#Docker的安装" class="headerlink" title="Docker的安装"></a>Docker的安装</h3><hr><p><img src="/2024/05/15/Docker/image-20240515122544372.png" alt="Docker Daemon"></p><ol><li><p>Docker采用Client-Server架构模式</p><ul><li>Docker Client和Docker Daemon之间通过Socket或者RESTful API进行通信</li><li>Docker Daemon就是服务端的守护进程，他负责管理Docker的各种资源</li><li>Docker Client负责向Docker Daemon发送请求，Docker Daemon接收到请求后进行处理，然后将结果返回给Client。因此我们在终端中输入的各种命令，都是Client发送给Docker Daemon的</li></ul></li><li><p>容器化和Dockerfile</p><p><strong>容器化</strong>：顾名思义就是将应用程序打包为容器，在容器中运行应用程序的过程</p><p><strong>Dockerfile</strong>：是一个文本文件，里面包含了一条条的指令，用来告诉Docker如何来构建镜像，这个镜像中包含了我们应用程序执行的所有命令，也就是上边提到的依赖、第三方软件包、配置文件等</p><p>这个过程简单来说可以分成三个步骤:</p><ul><li>首先需要创建一个Dockerfile，来告诉Docker构建应用程序镜像所需要的步骤和配置</li><li>使用Dockerfile来构建镜像</li><li>使用镜像创建和运行容器</li></ul></li></ol><h3 id="实践环节"><a href="#实践环节" class="headerlink" title="实践环节"></a>实践环节</h3><hr><p><img src="/2024/05/15/Docker/image-20240515164517046.png" alt="测试程序_python: 在这里使用默认的ENTRYPOINT[&quot;top&quot;, &quot;-b&quot;]在build时会报错，先删除做测试"></p><hr><p><img src="/2024/05/15/Docker/image-20240515164536314.png" alt="打包为hello-docker镜像"></p><hr><p><img src="/2024/05/15/Docker/image-20240515164551800.png" alt="docker 镜像创建成功"></p>]]></content>
    
    
    <categories>
      
      <category>Develop</category>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
