<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Interview Direction Series</title>
    <link href="/2024/06/28/interview/"/>
    <url>/2024/06/28/interview/</url>
    
    <content type="html"><![CDATA[<h3 id="在当前的AIGC（AI-generated-content）领域，在各个领域中领先的模型："><a href="#在当前的AIGC（AI-generated-content）领域，在各个领域中领先的模型：" class="headerlink" title="在当前的AIGC（AI-generated content）领域，在各个领域中领先的模型："></a>在当前的AIGC（AI-generated content）领域，在各个领域中领先的模型：</h3><h4 id="1-文本生成"><a href="#1-文本生成" class="headerlink" title="1. 文本生成"></a>1. 文本生成</h4><ul><li><p><strong>GPT-4（OpenAI）</strong>：GPT-4 是 OpenAI 发布的最新版本的生成预训练变换模型，以其生成高质量的自然语言文本而闻名。它在各种自然语言处理任务中表现出色，包括对话生成、文本补全和内容创作。</p></li><li><p><strong>BERT（Google）</strong>：虽然BERT主要用于自然语言理解任务，但其衍生版本，如T5（Text-To-Text Transfer Transformer），在生成任务上表现也非常出色。</p></li><li><p><strong>T5（Google）</strong>：T5模型可以将各种NLP任务统一为文本到文本的问题，通过预训练和微调，在文本生成和转换任务中表现出色。</p></li></ul><h4 id="2-图像生成"><a href="#2-图像生成" class="headerlink" title="2. 图像生成"></a>2. 图像生成</h4><ul><li><p><strong>DALL-E 2（OpenAI）</strong>：DALL-E 2 是 OpenAI 开发的文本到图像生成模型，可以根据文本描述生成高度逼真的图像。它展示了AI在多模态生成任务中的强大能力。</p></li><li><p><strong>Stable Diffusion（Stability AI）</strong>：这是一个高效的扩散模型，能够在潜在空间中进行图像生成，从而大幅减少计算资源消耗，同时保持高质量的图像生成。</p></li><li><p><strong>Imagen（Google Research）</strong>：Imagen 是一个由 Google Research 开发的强大图像生成模型，通过结合大型语言模型和扩散模型，实现了高质量的文本到图像生成。</p></li></ul><h4 id="3-音频生成"><a href="#3-音频生成" class="headerlink" title="3. 音频生成"></a>3. 音频生成</h4><ul><li><p><strong>WaveNet（DeepMind）</strong>：WaveNet 是由 DeepMind 开发的生成模型，能够生成高保真的语音和音乐。它在语音合成任务上表现出色，被广泛应用于Google Assistant等产品中。</p></li><li><p><strong>Jukebox（OpenAI）</strong>：Jukebox 是 OpenAI 开发的音乐生成模型，可以生成不同风格和艺术家的音乐。它通过一个VAE-GAN架构实现了长时间的音乐生成。</p></li></ul><h4 id="4-视频生成"><a href="#4-视频生成" class="headerlink" title="4. 视频生成"></a>4. 视频生成</h4><ul><li><p><strong>MoCoGAN（Motion and Content Generative Adversarial Network）</strong>：MoCoGAN 是一种用于视频生成的生成对抗网络（GAN），能够同时生成视频的运动和内容。</p></li><li><p><strong>VideoGPT（OpenAI）</strong>：VideoGPT 是一种将GPT架构应用于视频生成任务的模型，利用自回归方式生成视频帧，展示了在视频生成任务上的潜力。</p></li></ul><h4 id="5-多模态生成"><a href="#5-多模态生成" class="headerlink" title="5. 多模态生成"></a>5. 多模态生成</h4><ul><li><p><strong>CLIP（Contrastive Language-Image Pretraining，OpenAI）</strong>：CLIP 是一个多模态模型，能够同时理解和生成图像和文本。它可以将图像和文本进行对比学习，从而在各种多模态任务中表现出色。</p></li><li><p><strong>ALIGN（Google Research）</strong>：ALIGN 是 Google 开发的多模态对比学习模型，能够在大规模数据上进行图像和文本的对比学习，在图像分类、检索等任务上取得了显著效果。</p></li></ul><p>这些模型在AIGC领域的进展展示了人工智能在生成内容方面的巨大潜力和多样性。随着技术的不断进步，这些模型在各自的应用领域中将继续推动创新和发展。</p><h3 id="视觉基础模型等领域（类似于SAM、SEEM、Grounding-DINO、LISA等工作）"><a href="#视觉基础模型等领域（类似于SAM、SEEM、Grounding-DINO、LISA等工作）" class="headerlink" title="视觉基础模型等领域（类似于SAM、SEEM、Grounding-DINO、LISA等工作）"></a>视觉基础模型等领域（类似于SAM、SEEM、Grounding-DINO、LISA等工作）</h3><ol><li><p><strong>SAM（Segment Anything Model）</strong>：</p><ul><li><strong>简介</strong>：SAM 是由 Meta AI 研究团队开发的一种<font color="red"><strong>图像分割模型</strong></font>，其设计目标是能够对任意图像进行任意物体的分割。</li><li><strong>特点</strong>：SAM 使用了一种新的分割技术，可以通过给定的提示（例如点击、框选或文本描述）来分割图像中的对象。这使得它在处理不同类型的图像和场景时具有很高的灵活性。</li><li><strong>应用</strong>：适用于需要精确分割的任务，如医学图像分析、自动驾驶、图像编辑等。</li></ul></li><li><p><strong>SEEM（Semantic Enhanced Efficient Model）</strong>：</p><ul><li><strong>简介</strong>：SEEM 是一种旨在提高图像识别和分割效率的模型，结合了<font color="red"><strong>语义信息增强和高效计算架构</strong></font>。</li><li><strong>特点</strong>：SEEM 利用语义增强技术，使得模型能够更好地理解图像中的内容，从而提高分割的准确性和效率。此外，模型架构设计也注重计算效率，适合在资源受限的环境中使用。</li><li><strong>应用</strong>：广泛应用于需要高效处理的任务，如移动设备上的图像处理、实时视频分析等。</li></ul></li><li><p><strong>Grounding-DINO</strong>：</p><ul><li><strong>简介</strong>：Grounding-DINO 是一个结合了<font color="red"><strong>语义理解和目标检测</strong></font>的模型，基于 DINO（DETR with Improved Non-Autoregressive Object Detection）的架构。</li><li><strong>特点</strong>：通过结合目标检测和语义理解，Grounding-DINO 可以实现更精确的目标检测，并能够在复杂场景中识别和定位多种对象。其非自回归设计使得检测速度更快。</li><li><strong>应用</strong>：适用于需要高精度和高效目标检测的应用，如自动驾驶、智能监控、无人机导航等。</li></ul></li><li><p><strong>LISA（Language-Image Semantic Alignment）</strong>：</p><ul><li><strong>简介</strong>：LISA 是一种用于图像和语言对齐的模型，旨在通过联合学习图像和文本的语义信息来提高多模态任务的性能。</li><li><strong>特点</strong>：LISA 利用一种对比学习的方法，使得模型能够更好地理解和关联图像和文本的语义信息，从而在图像标注、图像生成和图像搜索等任务中表现出色。</li><li><strong>应用</strong>：适用于多模态任务，如图像描述生成、视觉问答、跨模态检索等。</li></ul></li></ol><p>这些模型各有特点，适用于不同的视觉任务，通过结合不同的技术和架构，推动了计算机视觉领域的进步。</p><h3 id="Bilibili视觉算法实习生：多模态大模型理解、图像和视频分类、图片和视频描述生成、多模态融合、检索等"><a href="#Bilibili视觉算法实习生：多模态大模型理解、图像和视频分类、图片和视频描述生成、多模态融合、检索等" class="headerlink" title="Bilibili视觉算法实习生：多模态大模型理解、图像和视频分类、图片和视频描述生成、多模态融合、检索等"></a>Bilibili视觉算法实习生：多模态大模型理解、图像和视频分类、图片和视频描述生成、多模态融合、检索等</h3><h3 id="快手视觉算法实习生：视频内容理解算法"><a href="#快手视觉算法实习生：视频内容理解算法" class="headerlink" title="快手视觉算法实习生：视频内容理解算法"></a>快手视觉算法实习生：视频内容理解算法</h3>]]></content>
    
    
    <categories>
      
      <category>Interview</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Interview</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AIGC-StableDiffusion</title>
    <link href="/2024/06/28/AIGC/"/>
    <url>/2024/06/28/AIGC/</url>
    
    <content type="html"><![CDATA[<h2 id="AIGC"><a href="#AIGC" class="headerlink" title="AIGC"></a>AIGC</h2><h3 id="Stable-Diffusion"><a href="#Stable-Diffusion" class="headerlink" title="Stable Diffusion"></a>Stable Diffusion</h3><p>参考文章：</p><p><a href="http://shiyanjun.cn/archives/2212.html">http://shiyanjun.cn/archives/2212.html</a></p><p><a href="https://jalammar.github.io/illustrated-stable-diffusion/">https://jalammar.github.io/illustrated-stable-diffusion/</a></p><p><img src="/2024/06/28/AIGC/image-20240630102333319.png" alt="Latent Diffusion Model Abstract"></p><blockquote><p><strong>Stable Diffusion</strong>是一个基于Latent Diffusion Model（LDM）的文转图AI模型，其使用了<strong>CLIP ViT-L&#x2F;14的文本编码器</strong>，能够通过<strong>文本提示</strong>调整<strong>图像模型</strong>。它在运行时将成像过程分离成“<strong>扩散 （diffusion）</strong>”的过程，从有噪声的情况开始，逐渐改善图像，直到完全没有噪声，逐步接近所提供的文本描述。</p></blockquote><h3 id="1-模型结构"><a href="#1-模型结构" class="headerlink" title="1. 模型结构"></a>1. 模型结构</h3><p><strong>结构图：</strong></p><p><img src="/2024/06/28/AIGC/stable-diffusion-unet-steps.png" alt="Stable Diffusion文生图模型结构"></p><p><strong>模块：</strong></p><blockquote><p><strong>CLIP Text Encoder</strong> : 提取输入的text的embedding , <font color="red">通过 cross attention 方式</font>送入扩散模型的 UNet 中作为 condition，注入语义信息;</p><p><strong>Image Information Creator</strong></p><ul><li><p><strong>Image Encoder</strong> : 基于 VAE Eecoder 将图像压缩到 latent 空间，用向量表示；</p></li><li><p><strong>UNet+Scheduler</strong> : 扩散模型的主体，用来实现文本引导下的 latent 去噪生成；</p></li></ul><p><strong>Image Decode</strong> : 根据得到的 Latent Image，基于 VAE Decoder 生成最终的图像。。</p></blockquote><p><strong>过程：</strong></p><blockquote><p><strong>首先</strong>，输入 Prompt 提示词 “paradise, cosmic, beach”，经过 <strong>CLIP Text Encoder</strong> 组件的处理，将输入的 Prompt 提示词转换成 77×768 的 Token Embeddings，<code>该 Embeddings 输入到 Image Information Creator 组件</code>；</p><p><font color="green">这里如果有别的输入信息，比如Semantic Map（语义图）、Text（文本）、Representations（表示）、Images（图像）等，经过不同组件的处理，然后通过 cross attention 方式送入扩散模型的 UNet 中作为 condition。</font></p><p><strong>然后</strong>，Random image information tensor 是由一个 Latent Seed（Gaussian noise ~ N(0, 1)） 随机生成的 64×64 大小的图片表示，它表示一个完全的噪声图片，<code>作为 Image Information Creator 组件的另一个初始输入</code>；</p><p><strong>接着</strong>，通过 Image Information Creator 组件的处理（该过程称为 Diffusion），<code>生成一个包含图片信息的 64×64 的 Latent Image</code>，该输出包含了前面输入 Prompt 提示词所具有的语义信息的图片的信息；</p><p><strong>最后</strong>，上一步生成的 Latent Image 经过 Image Decoder 组件处理后<code>生成最终的和输入 Prompt 提示词相关的 512×512 大小的图片输出</code>。</p></blockquote><h3 id="2-具体实现"><a href="#2-具体实现" class="headerlink" title="2. 具体实现"></a>2. 具体实现</h3><h4 id="2-0-到底什么是扩散-Diffusion-？"><a href="#2-0-到底什么是扩散-Diffusion-？" class="headerlink" title="2.0 到底什么是扩散(Diffusion)？"></a>2.0 <strong>到底什么是扩散(Diffusion)？</strong></h4><p>扩散是发生在“<strong>Image Information Creator</strong>”组件内部的过程。有了表示输入文本的标记嵌入「<strong>Token embeddings</strong>」和随机起始图像信息数组「<strong>Random image information tensor</strong>」，该过程会生成图像解码器用来绘制最终图像的信息数组。</p><h4 id="2-1-CLIP-Text-Encoder"><a href="#2-1-CLIP-Text-Encoder" class="headerlink" title="2.1 CLIP Text Encoder"></a>2.1 CLIP Text Encoder</h4><p>Stable Diffusion所使用的是OpenAI的CLIP的预训练模型，CLIP需要的数据为图像及其描述，数据集中大约包含4亿张图像及其描述。CLIP「Contrastive Language-Image Pretraining」整体模型如下所示：</p><p><img src="/2024/06/28/AIGC/image-20240628165159974.png" alt="CLIP模型"></p><ul><li><p>(1). <strong>对比预训练</strong> 「Contrastive pre-training」</p><blockquote><p>CLIP 是图像编码器和文本编码器的组合，使用两个编码器对数据分别进行编码。然后使用余弦距离比较结果嵌入，刚开始训练时，即使文本描述与图像是相匹配的，它们之间的相似性肯定也是很低的。随着模型的不断更新，在整个数据集中重复该过程，编码器对图像和文本编码得到的嵌入会逐渐相似。</p></blockquote></li><li><p>(2). <strong>从标签文本创建数据集分类器</strong> 「Create dataset classifier from label text」</p><blockquote><p><strong>作用</strong>：</p><ol><li><p><strong>构建分类器</strong>：在传统的图像分类任务中，通常需要为每个类别训练一个专门的分类器。<font color="red">CLIP 通过文本编码器将标签转换为描述性文本，并进一步转换为特征向量，从而构建一个零样本分类器。这些特征向量可以看作是每个类别的“代表”，可以直接用于分类任务。</font></p></li><li><p><strong>零样本学习</strong>：通过这种方式，CLIP 可以在没有看到任何特定类别的训练样本的情况下，依靠标签文本的描述来进行分类。这种能力被称为“零样本学习”，即模型可以识别从未见过的类别。</p></li></ol><p><strong>为什么需要这一步</strong>：</p><ol><li><p><strong>减少数据依赖</strong>：在实际应用中，获取每个类别的大量标注数据是非常困难的。通过从标签文本创建分类器，CLIP 能够在没有特定训练数据的情况下，利用现有的文本描述来实现分类。</p></li><li><p><strong>提高模型泛化能力</strong>：传统分类器只能处理训练时见过的类别，而 CLIP 的这种方法使其能够处理新类别，提高模型的泛化能力和适用范围。</p></li></ol></blockquote></li><li><p>(3). <strong>用于零样本预测</strong> 「Use for zero-shot prediction」</p><blockquote><p><strong>作用：</strong></p><ol><li><p><strong>实际应用</strong>：在实际应用中，将待分类的图像输入图像编码器，得到图像的特征向量，然后将这个特征向量与之前生成的类别特征向量进行匹配，找到最接近的类别。这一过程就是零样本预测的核心。</p></li><li><p><strong>扩展性</strong>：这一过程使得 CLIP 可以处理任意数量和类型的类别，只需提供相应的文本描述即可，无需重新训练模型。</p></li></ol><p><strong>为什么需要这一步</strong>：</p><ol><li><p><strong>实现分类任务</strong>：这一步是将 CLIP 应用于实际分类任务的关键，通过匹配图像和文本特征向量，实现图像分类。</p></li><li><p><strong>灵活性和扩展性</strong>：这种方法使得模型可以处理新出现的类别和变化多端的任务，只需要相应的文本描述即可，无需重新训练。这大大提高了模型的灵活性和扩展性。</p></li></ol></blockquote></li></ul><h4 id="2-2-Image-Information-Creator"><a href="#2-2-Image-Information-Creator" class="headerlink" title="2.2 Image Information Creator"></a>2.2 <strong>Image Information Creator</strong></h4><hr><h5 id="2-2-1-Image-Encoder「VQ-VAE中的编码器部分」"><a href="#2-2-1-Image-Encoder「VQ-VAE中的编码器部分」" class="headerlink" title="2.2.1 Image Encoder「VQ-VAE中的编码器部分」"></a>2.2.1 Image Encoder「<strong>VQ-VAE</strong>中的编码器部分」</h5><p>在Stable Diffusion模型中，Image Encoder（图像编码器）是一个关键组件，用于将图像转换为潜在空间（latent space）中的表示，实现信息的有效压缩和重构。这些表示之后用于生成高质量的图像。</p><ul><li><p>架构</p><blockquote><ol><li><strong>卷积层</strong>：一系列的卷积层，用于逐步提取图像的局部特征。</li><li><strong>池化层</strong>：减少特征图的空间维度，从而降低计算复杂度。</li><li><strong>编码层</strong>：将提取到的特征转换为潜在向量表示。</li></ol></blockquote></li><li><p>工作流程</p><blockquote><ol><li><strong>输入图像</strong>：输入图像首先经过预处理步骤，例如归一化和大小调整。</li><li><strong>卷积层处理</strong>：经过多个卷积层和池化层，提取图像的特征并逐步降低空间维度。</li><li><strong>潜在表示生成</strong>：通过编码层，将提取的特征转换为潜在空间中的向量表示。</li></ol></blockquote></li></ul><p>这些潜在向量表示是扩散模型操作的核心，它们在扩散过程中逐步反向扩散（denoising），最终生成高质量的图像。Image Encoder在这个过程中扮演了至关重要的角色，确保输入图像能够被有效地压缩和表示，从而使得扩散过程能够生成准确且高质量的图像。</p><h5 id="2-2-2-UNet-Scheduler"><a href="#2-2-2-UNet-Scheduler" class="headerlink" title="2.2.2 UNet+Scheduler"></a>2.2.2 UNet+Scheduler</h5><p>当输入的图像、 文本提示词等信息经过不同组件分别被转换成 Image Embeddings、Token Embeddings… 之后，后续的操作就开始进入 Latent Space 中，通过向量来表示并进行各种处理操作，得到了包含 “原始图像 + 提示词” 信息的图像向量数据信息（Latent Image）。</p><p>下图中前向过程（使用自动编码器的编码器）<strong>是我们生成数据以训练噪声预测器的方式</strong>。训练完成后，我们可以通过运行反向过程（使用自动编码器的解码器）来生成图像。</p><p><img src="/2024/06/28/AIGC/image-20240630111107956.png" alt="图1"></p><p><img src="/2024/06/28/AIGC/image-20240629232700236.png" alt="图2"></p><p> Image Embeddings、Token Embeddings 等是如何在 UNet 网络中整合在一起的：</p><p><img src="/2024/06/28/AIGC/image-20240630110811534.png" alt="Image Embeddings、Token Embeddings 如何在UNet网络中整合在一起"></p><h4 id="2-3-Image-Decoder「VQ-VAE中的解码器部分」"><a href="#2-3-Image-Decoder「VQ-VAE中的解码器部分」" class="headerlink" title="2.3 Image Decoder「VQ-VAE中的解码器部分」"></a>2.3 Image Decoder「<strong>VQ-VAE</strong>中的解码器部分」</h4><p>最后要把这个Latent Image，基于 VAE Decoder 从 Latent Space 再映射到 Pixel Space，得到我们最终需要生成的视觉图像。</p><hr><h3 id="3-补充知识"><a href="#3-补充知识" class="headerlink" title="3. 补充知识"></a>3. 补充知识</h3><h4 id="3-1-VQ-VAE（Vector-Quantized-Variational-Autoencoder）⚠️"><a href="#3-1-VQ-VAE（Vector-Quantized-Variational-Autoencoder）⚠️" class="headerlink" title="3.1 VQ-VAE（Vector Quantized Variational Autoencoder）⚠️"></a>3.1 VQ-VAE（Vector Quantized Variational Autoencoder）⚠️</h4><p>关于 VAE（Variational Autoencoder），它是一个神经网络，由 Encoder 和 Decoder 两部分组成，如下图所示：</p><p><img src="/2024/06/28/AIGC/image-20240630105651318.png" alt=" VAE「Variational Autoencoder」"></p><p>其中，Encoder 能够将一个图像压缩到低维空间表示，在 Stable Diffusion 模型中，将原始输入图像通过 Encode 转换成 Latent Space 中的向量表示 Latent Image；</p><p>Decoder 能够将一个压缩表示的图像向量数据转换成高维空间表示，在 Stable Diffusion 模型中将 Latent Space 中图像的向量表示 Latent Image 通过 Decode 转换成 Pixel Space 中的视觉图像。</p><h4 id="3-2-什么叫做转换为潜在空间中的向量表示，它和提取的图像特征不是一回事吗？"><a href="#3-2-什么叫做转换为潜在空间中的向量表示，它和提取的图像特征不是一回事吗？" class="headerlink" title="3.2 什么叫做转换为潜在空间中的向量表示，它和提取的图像特征不是一回事吗？"></a>3.2 什么叫做转换为潜在空间中的向量表示，它和提取的图像特征不是一回事吗？</h4><p>是的，将图像转换为潜在空间中的向量表示和提取图像特征是密切相关的过程，但在细节和目的上有所不同。让我详细解释一下这两个概念及其关系。</p><ul><li><p>图像特征提取</p><blockquote><p>图像特征提取是指从输入图像中提取出能够代表图像内容的重要信息。这通常是通过一系列的卷积操作和其他神经网络层来实现的。特征提取的主要目标是捕捉图像的局部和全局信息，包括颜色、纹理、形状等。</p></blockquote></li><li><p><strong>潜在空间中的向量表示</strong></p><blockquote><p>潜在空间中的向量表示是<font color="red"><strong>通过对图像特征进一步处理</strong></font>得到的。这些表示通常是低维的紧凑向量，包含了图像的高层次抽象信息。潜在空间中的向量表示的主要目的是将图像数据压缩到一个更易处理和操作的形式，同时保留图像的关键信息。</p></blockquote></li><li><p>关系与区别</p><blockquote><ol><li><p><strong>特征提取 vs. 潜在表示</strong>：</p><ul><li><strong>特征提取</strong>：这是一个较广义的概念，指的是从图像中提取重要信息的过程。提取的特征可以是局部的（如边缘、角点）或全局的（如整体形状）。</li><li><strong>潜在表示</strong>：这是特征提取的进一步抽象。它将提取到的特征压缩和编码到一个低维空间，使其成为更抽象的、高层次的表示。</li></ul></li><li><p><strong>维度与表示形式</strong>：</p><ul><li><strong>特征提取</strong>：通常得到的是高维特征图（feature map），其空间维度较大，包含了丰富的局部信息。</li><li><strong>潜在表示</strong>：通过进一步的编码，将高维特征图转换为低维向量。这些向量保留了图像的核心信息，但空间维度较低，便于后续处理和生成任务。</li></ul></li></ol></blockquote></li><li><p>例子</p><blockquote><p>假设我们有一张猫的图像：</p><ol><li><strong>特征提取</strong>：卷积神经网络提取出猫的边缘、纹理、耳朵形状等特征，形成一个高维的特征图。</li><li><strong>潜在表示</strong>：特征图通过编码器压缩为一个低维向量。这个向量可能表示为[0.8, -1.2, 0.3, …]，其中的每个值都对应着图像的一些高层次特征，例如猫的种类、颜色、姿态等。</li></ol></blockquote></li><li><p>总结</p><blockquote><p>将图像转换为潜在空间中的向量表示是特征提取的进一步抽象和压缩过程。这一过程的主要目的是将高维的、冗长的特征图压缩为低维的紧凑表示，同时保留图像的核心信息，便于后续的生成和操作。</p></blockquote></li></ul>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>AIGC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Stable Diffusion</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LeetCode</title>
    <link href="/2024/06/28/LeetCode/"/>
    <url>/2024/06/28/LeetCode/</url>
    
    <content type="html"><![CDATA[<h3 id="LeetCode-热题100"><a href="#LeetCode-热题100" class="headerlink" title="LeetCode 热题100"></a>LeetCode 热题100</h3><hr><h4 id="1-题目一：两数之和"><a href="#1-题目一：两数之和" class="headerlink" title="1. 题目一：两数之和"></a>1. 题目一：两数之和</h4><blockquote><p>给定一个整数数组 <code>nums</code> 和一个整数目标值 <code>target</code>，请你在该数组中找出 <strong>和为目标值</strong> <code>target</code> 的那 <strong>两个</strong> 整数，并返回它们的数组下标。</p></blockquote><p><strong>解决：</strong></p><ul><li><p>暴力枚举法空间复杂度$O(1)$时间复杂度$O(N^2)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">twoSum</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        n = <span class="hljs-built_in">len</span>(nums)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i + <span class="hljs-number">1</span>, n):<br>                <span class="hljs-keyword">if</span> nums[i] + nums[j] == target:<br>                    <span class="hljs-keyword">return</span> [i, j]        <br>        <span class="hljs-keyword">return</span> []<br><br></code></pre></td></tr></table></figure></li><li><p>Hash法空间复杂度$O(N)$时间复杂度$O(N)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">twoSum</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span></span>):<br>        hashtable = <span class="hljs-built_in">dict</span>()<br>        <span class="hljs-keyword">for</span> i, num <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(nums):<br>            <span class="hljs-keyword">if</span> target - num <span class="hljs-keyword">in</span> hashtable:<br>                <span class="hljs-keyword">return</span> [hashtable[target-num], i]<br>            hashtable[nums[i]] = i<br>        <span class="hljs-keyword">return</span> []<br></code></pre></td></tr></table></figure></li></ul><p><strong>总结：</strong></p><ol><li><p>enumerate()是python的内置函数，用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">enumerate</span>(sequence, [start=<span class="hljs-number">0</span>])<br>* sequence: 一个序列、迭代器或其他支持迭代对象。<br>* start: 下标起始位置。<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">seasons = [<span class="hljs-string">&#x27;Spring&#x27;</span>, <span class="hljs-string">&#x27;Summer&#x27;</span>, <span class="hljs-string">&#x27;Fall&#x27;</span>, <span class="hljs-string">&#x27;Winter&#x27;</span>]<br><br><span class="hljs-built_in">list</span>(<span class="hljs-built_in">enumerate</span>(seasons))<br>[(<span class="hljs-number">0</span>, <span class="hljs-string">&#x27;Spring&#x27;</span>), (<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;Summer&#x27;</span>), (<span class="hljs-number">2</span>, <span class="hljs-string">&#x27;Fall&#x27;</span>), (<span class="hljs-number">3</span>, <span class="hljs-string">&#x27;Winter&#x27;</span>)]<br><span class="hljs-built_in">list</span>(<span class="hljs-built_in">enumerate</span>(seasons, start=<span class="hljs-number">1</span>))       <span class="hljs-comment"># 下标从 1 开始</span><br>[(<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;Spring&#x27;</span>), (<span class="hljs-number">2</span>, <span class="hljs-string">&#x27;Summer&#x27;</span>), (<span class="hljs-number">3</span>, <span class="hljs-string">&#x27;Fall&#x27;</span>), (<span class="hljs-number">4</span>, <span class="hljs-string">&#x27;Winter&#x27;</span>)]<br></code></pre></td></tr></table></figure></li></ol><h4 id="2-题目二：字母异位词分组"><a href="#2-题目二：字母异位词分组" class="headerlink" title="2. 题目二：字母异位词分组"></a>2. 题目二：字母异位词分组</h4><blockquote><p>给你一个字符串数组，请你将 <strong>字母异位词</strong> 组合在一起。可以按任意顺序返回结果列表。</p><p><strong>字母异位词</strong> 是由重新排列源单词的所有字母得到的一个新单词。</p></blockquote><p><strong>解决：</strong></p><ul><li><p>Hash表时间复杂度$O(nk\log{k})$空间复杂度$O(nk)$</p><p>其中 <em>n</em> 是 <em>strs</em> 中的字符串的数量，<em>k</em> 是 <em>strs</em> 中的字符串的的最大长度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">groupAnagrams</span>(<span class="hljs-params">self, strs: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]</span>):<br>        hashtable = collections.defaultdict(<span class="hljs-built_in">list</span>)<br>        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> strs:<br>            key = <span class="hljs-string">&quot;&quot;</span>.join(<span class="hljs-built_in">sorted</span>(s))<br>            hashtable[key].append(s)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">list</span>(hashtable.values())<br><br></code></pre></td></tr></table></figure></li><li><p>计数时间复杂度$O(n(k+26)$空间复杂度$O(n(k+26))$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">groupAnagrams</span>(<span class="hljs-params">self, strs</span>):<br>        hashtable = collections.defaultdict(<span class="hljs-built_in">list</span>)<br>        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> strs:<br>            count = [<span class="hljs-number">0</span>] * <span class="hljs-number">26</span><br>            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> s:<br>                count[<span class="hljs-built_in">ord</span>(c)-<span class="hljs-built_in">ord</span>(<span class="hljs-string">&quot;a&quot;</span>)] += <span class="hljs-number">1</span><br>            hashtable[<span class="hljs-built_in">tuple</span>(count)].append(s)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">list</span>(hashtable.values())<br></code></pre></td></tr></table></figure></li></ul><p><strong>总结：</strong></p><ol><li><p>在这里使用<code>hashtable=&#123;&#125;</code>可能会有一些keyError等问题，需要我们多写一些代码，而使用<code>defaultdict</code>可以避免 ：</p><p><code>defaultdict</code> 的构造函数接受一个工厂函数作为参数，这个工厂函数<strong>在访问不存在的键时</strong>会被调用生成默认值。例如，<code>defaultdict(list) </code>表示当访问的键不存在时，会自动生成一个空列表作为默认值。</p></li><li><p>Map、HashTable、Dict等的区别和联系</p></li></ol><h4 id="3-题目三：最长连续序列"><a href="#3-题目三：最长连续序列" class="headerlink" title="3. 题目三：最长连续序列"></a>3. 题目三：最长连续序列</h4><blockquote><p>给定一个未排序的整数数组 <code>nums</code> ，找出数字连续的最长序列（不要求序列元素在原数组中连续）的长度。</p><p>请你设计并实现时间复杂度为 <code>O(n)</code> 的算法解决此问题。</p></blockquote><p><strong>解决：</strong></p><ul><li><p>数组排序时间复杂度$O(n\log{n})$空间复杂度$O(1)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">longestConsecutive</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>)-&gt;<span class="hljs-built_in">int</span>:<br>        <span class="hljs-keyword">if</span> nums == []:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        nums_sorted = <span class="hljs-built_in">sorted</span>(nums)<br>        max_length = <span class="hljs-number">1</span><br>        tmp = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums_sorted) - <span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">if</span> nums_sorted[i+<span class="hljs-number">1</span>] == nums_sorted[i]+<span class="hljs-number">1</span>:<br>                tmp += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">elif</span> nums_sorted[i+<span class="hljs-number">1</span>] == nums_sorted[i]:<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">else</span>:<br>                tmp=<span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> tmp&gt;max_length:<br>                max_length = tmp<br>        <span class="hljs-keyword">return</span> max_length<br></code></pre></td></tr></table></figure></li><li><p>Hash表</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>LeetCode</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LeetCode</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BasicKnowledge</title>
    <link href="/2024/06/13/DeepLearningBasicKnowledge/"/>
    <url>/2024/06/13/DeepLearningBasicKnowledge/</url>
    
    <content type="html"><![CDATA[<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><blockquote><p>Batch Normalization (BN) 是一种深度学习技术，用于在训练过程中标准化神经网络的输入。其主要目的是加速训练过程并提高模型的稳定性和性能。</p></blockquote><hr><h4 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h4><p>假设我们有一个图像分类任务，输入图像的像素值在 0 到 255 之间。我们将像素值缩放到 0 到 1 之间，但不同批次的数据分布可能不同。例如，一个批次的图像可能大部分像素值集中在 0.1 到 0.3 之间，而另一个批次的图像像素值集中在 0.7 到 0.9 之间。这种输入数据分布的变化会导致模型训练不稳定。</p><p>通过使用 Batch Normalization，我们可以在每层网络中对输入进行标准化，使得每层的输入在训练过程中保持均值为 0、方差为 1 的分布，从而加快收敛速度并提高模型的稳定性和性能。</p><h4 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h4><ol><li><p><strong>计算批次均值和方差</strong>： 对于一个批次的输入 $x&#x3D;&lt;!–swig￼1–&gt;<br>$$</p></li><li><p>引入两个可训练参数 $\gamma$ 和 $\beta$，分别用于缩放和平移标准化后的输入，以保证模型的表达能力，其中，$\gamma$ 和 $\beta$ 是与输入维度相同的参数：<br>$$<br>y_i &#x3D; \gamma \hat{x_i} + \beta<br>$$</p></li></ol><h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><p>Batch Normalization 之所以对网络训练产生积极作用，主要有以下几个原因：</p><ol><li><p><strong>减轻内部协变量偏移（Internal Covariate Shift）</strong>： 内部协变量偏移是指在训练过程中，由于前一层参数的变化，后续层的输入分布也发生变化，从而影响训练效率。</p><p>BN 通过标准化每一层的输入，使得每一层的输入分布更加稳定，从而减轻了层间输入分布变化带来的影响。这使得网络在训练时更容易收敛。</p></li><li><p><strong>加速训练</strong>： 由于每一层的输入被标准化，梯度下降算法在训练过程中能够使用较大的学习率，从而加快训练速度。</p></li><li><p><strong>提高模型的稳定性</strong>： BN 在一定程度上起到了正则化的作用，减少了过拟合的风险。这是因为每个批次的数据都被标准化，添加了噪声，从而增强了模型的泛化能力。</p></li><li><p><strong>缓解梯度消失和梯度爆炸</strong>： 标准化后的输入使得数据的尺度更加一致，从而缓解了梯度消失和梯度爆炸问题，特别是在深层神经网络中。</p></li></ol><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br><span class="hljs-comment"># 定义一个简单的神经网络</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleNN</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(SimpleNN, self).__init__()<br>        self.fc1 = nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">256</span>)<br>        self.bn1 = nn.BatchNorm1d(<span class="hljs-number">256</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.fc1(x)<br>        x = self.bn1(x)<br>        x = torch.relu(x)<br>        x = self.fc2(x)<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment"># 初始化网络、损失函数和优化器</span><br>model = SimpleNN()<br>criterion = nn.CrossEntropyLoss()<br>optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0.9</span>)<br><br><span class="hljs-comment"># 示例训练步骤</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    <span class="hljs-keyword">for</span> inputs, labels <span class="hljs-keyword">in</span> train_loader:<br>        optimizer.zero_grad()<br>        outputs = model(inputs)<br>        loss = criterion(outputs, labels)<br>        loss.backward()<br>        optimizer.step()<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Epoch <span class="hljs-subst">&#123;epoch+<span class="hljs-number">1</span>&#125;</span>, Loss: <span class="hljs-subst">&#123;loss.item()&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>BasicKnowledge</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch</title>
    <link href="/2024/05/22/Pytorch/"/>
    <url>/2024/05/22/Pytorch/</url>
    
    <content type="html"><![CDATA[<h3 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h3><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs oxygene"><span class="hljs-keyword">Module</span>作为模块封装的父类，可以是一段逻辑，也可以是模型的一个块「<span class="hljs-keyword">block</span>」或一层。<br>Pytorch中自定义模型只需要继承<span class="hljs-keyword">Module</span>，保存好Param并提供<span class="hljs-keyword">forward</span>方法，backward被tensor的自动微分自动完成。<br></code></pre></td></tr></table></figure><hr><p>以一个简单的MLP的代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MLP</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># Call the constructor of the parent class nn.Module to perform</span><br>        <span class="hljs-comment"># the necessary initialization</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.hidden = nn.LazyLinear(<span class="hljs-number">256</span>)<br>        self.out = nn.LazyLinear(<span class="hljs-number">10</span>)<br>    <span class="hljs-comment"># Define the forward propagation of the model, that is, how to return the</span><br>    <span class="hljs-comment"># required model output based on the input X</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> self.out(F.relu(self.hidden(X)))<br></code></pre></td></tr></table></figure><h4 id="1-回调「Hook」函数介绍"><a href="#1-回调「Hook」函数介绍" class="headerlink" title="1. 回调「Hook」函数介绍"></a>1. 回调「Hook」函数介绍</h4><p><strong>hook 函数机制</strong>：不改变主体，实现额外功能，像一个挂件、挂钩 ➡️ hook</p><h5 id="1-1-为什么会有-hook-函数这个机制：参考文章1"><a href="#1-1-为什么会有-hook-函数这个机制：参考文章1" class="headerlink" title="1.1 为什么会有 hook 函数这个机制：参考文章1"></a>1.1 <strong>为什么会有 hook 函数这个机制：</strong><a href="https://blog.csdn.net/weixin_44878336/article/details/133859089">参考文章1</a></h5><blockquote><p>这与 <strong>PyTorch 动态图运行机制</strong>有关:</p><p>在动态图运行机制中，当运算结束后，<font color="red">一些中间变量是会被释放掉的，比如特征图、非叶子节点的梯度。</font>但有时候我们又想要继续关注这些中间变量，那么就可以使用 hook 函数在主体代码中提取中间变量。</p><p>主体代码主要是模型的<strong>前向传播「forward」和反向传播「backward」</strong>，额外的功能就是对模型的中间变量进行操作如：</p><ol><li>提取&#x2F;修改张量梯度</li><li>提取&#x2F;保留非叶子张量的梯度</li><li>查看模型的层与层之间的数据传递情况（数据维度、数据大小等）</li><li>在不修改原始模型代码的基础上可视化各个卷积特征图</li><li>……</li></ol></blockquote><h5 id="1-2-演示Hook的作用：参考文章2"><a href="#1-2-演示Hook的作用：参考文章2" class="headerlink" title="1.2 演示Hook的作用：参考文章2"></a>1.2 <strong>演示Hook的作用</strong>：<a href="https://cloud.tencent.com/developer/article/1745455">参考文章2</a></h5><blockquote><p>一般来说，“hook”是在特定事件之后自动执行的函数。</p><p>PyTorch 为nn.Module 对象 &#x2F; 每个张量<font color="red"><strong>注册</strong></font> hook。hook 由对象的向前或向后传播触发。它们具有以下函数签名:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn, Tensor<br><br><span class="hljs-comment"># For nn.Module objects only.</span><br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">module_hook</span>(<span class="hljs-params">module: nn.Module, <span class="hljs-built_in">input</span>: Tensor, output: Tensor</span>):<br>    <br><span class="hljs-comment"># For Tensor objects only.</span><br>   <span class="hljs-comment"># Only executed during the *backward* pass!</span><br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">tensor_hook</span>(<span class="hljs-params">grad: Tensor</span>):<br></code></pre></td></tr></table></figure></blockquote><ul><li><p><strong>例子1</strong>：假如你想知道每个层输出的形状。我们可以创建一个简单的 wrapper，使用 hook 打印输出形状:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn, Tensor<br><span class="hljs-keyword">from</span> torchvision.models <span class="hljs-keyword">import</span> resnet50<br><span class="hljs-keyword">import</span> warnings<br><br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VerboseExecution</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model: nn.Module</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 传入resnet50模型</span><br>        self.model = model<br><br>        <span class="hljs-keyword">for</span> name, layer <span class="hljs-keyword">in</span> self.model.named_children():<br>            layer.__name__ = name<br>            <span class="hljs-comment"># Register a hook for each layer</span><br>            layer.register_forward_hook(<br>                <span class="hljs-keyword">lambda</span> layer, _, output: <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;layer.__name__&#125;</span>: <span class="hljs-subst">&#123;output.shape&#125;</span>&quot;</span>)<br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x: Tensor</span>) -&gt; Tensor:<br>        <span class="hljs-keyword">return</span> self.model(x)<br><br><br>verbose_resnet = VerboseExecution(resnet50())<br>dummy_input = torch.ones(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)<br><br>_ = verbose_resnet(dummy_input)<br><br><span class="hljs-comment"># --------输出</span><br><span class="hljs-comment"># conv1: torch.Size([10, 64, 112, 112])</span><br><span class="hljs-comment"># bn1: torch.Size([10, 64, 112, 112])</span><br><span class="hljs-comment"># relu: torch.Size([10, 64, 112, 112])</span><br><span class="hljs-comment"># maxpool: torch.Size([10, 64, 56, 56])</span><br><span class="hljs-comment"># layer1: torch.Size([10, 256, 56, 56])</span><br><span class="hljs-comment"># layer2: torch.Size([10, 512, 28, 28])</span><br><span class="hljs-comment"># layer3: torch.Size([10, 1024, 14, 14])</span><br><span class="hljs-comment"># layer4: torch.Size([10, 2048, 7, 7])</span><br><span class="hljs-comment"># avgpool: torch.Size([10, 2048, 1, 1])</span><br><span class="hljs-comment"># fc: torch.Size([10, 1000])</span><br></code></pre></td></tr></table></figure></li><li><p><strong>例子2</strong>：<strong>特征提取</strong>：通常，我们希望从一个预先训练好的网络中生成特性，然后用它们来完成另一个任务(例如分类等)。使用 hook，我们可以提取特征，而不需要重新创建现有模型或以任何方式修改它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn, Tensor<br><span class="hljs-keyword">from</span> torchvision.models <span class="hljs-keyword">import</span> resnet50<br><span class="hljs-keyword">import</span> warnings<br><br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br><br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Dict</span>, Iterable, <span class="hljs-type">Callable</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeatureExtractor</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model: nn.Module, layers: Iterable[<span class="hljs-built_in">str</span>]</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.model = model<br>        self.layers = layers<br>        self._features = &#123;layer: torch.empty(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> layers&#125;<br><br>        <span class="hljs-keyword">for</span> layer_id <span class="hljs-keyword">in</span> layers:<br>            layer = <span class="hljs-built_in">dict</span>([*self.model.named_modules()])[layer_id]<br>            <span class="hljs-comment"># Register a hook</span><br>            layer.register_forward_hook(self.save_outputs_hook(layer_id))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_outputs_hook</span>(<span class="hljs-params">self, layer_id: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-type">Callable</span>:<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">fn</span>(<span class="hljs-params">_, __, output</span>):<br>            self._features[layer_id] = output<br>        <span class="hljs-keyword">return</span> fn<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x: Tensor</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, Tensor]:<br>        _ = self.model(x)<br>        <span class="hljs-keyword">return</span> self._features<br><br>resnet_features = FeatureExtractor(resnet50(), layers=[<span class="hljs-string">&quot;layer4&quot;</span>, <span class="hljs-string">&quot;avgpool&quot;</span>])<br>dummy_input = torch.ones(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)<br>features = resnet_features(dummy_input)<br><br><span class="hljs-built_in">print</span>(&#123;name: output.shape <span class="hljs-keyword">for</span> name, output <span class="hljs-keyword">in</span> features.items()&#125;)<br><br><span class="hljs-comment"># 输出</span><br><span class="hljs-comment"># &#123;&#x27;layer4&#x27;: torch.Size([10, 2048, 7, 7]), &#x27;avgpool&#x27;: torch.Size([10, 2048, 1, 1])&#125;</span><br></code></pre></td></tr></table></figure></li><li><p><strong>例子3：梯度裁剪</strong>：梯度裁剪是处理梯度爆炸的一种著名方法。PyTorch 已经提供了梯度裁剪的工具方法，但是我们也可以很容易地使用 hook 来实现它。其他任何用于梯度裁剪&#x2F;归一化&#x2F;修改的方法都可以用同样的方式实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient_clipper</span>(<span class="hljs-params">model: nn.Module, val: <span class="hljs-built_in">float</span></span>) -&gt; nn.Module:<br>    <span class="hljs-keyword">for</span> parameter <span class="hljs-keyword">in</span> model.parameters():<br>      <span class="hljs-comment"># Register a hook for each parameter</span><br>        <br>        <span class="hljs-comment"># register_hook 方法的主要作用是允许用户在张量的梯度计算中注册一个自定义函数</span><br>        <span class="hljs-comment"># 以便在反向传播期间对梯度进行操作或记录信息。</span><br>        <span class="hljs-comment"># 这对于实现自定义梯度处理、梯度剪裁、可视化梯度信息以及梯度的修改等任务非常有用。</span><br>        parameter.register_hook(<span class="hljs-keyword">lambda</span> grad: grad.clamp_(-val, val))<br>    <br>    <span class="hljs-keyword">return</span> model<br><br>clipped_resnet = gradient_clipper(resnet50(), <span class="hljs-number">0.01</span>)<br>pred = clipped_resnet(dummy_input)<br>loss = pred.log().mean()<br>loss.backward()<br><br><span class="hljs-built_in">print</span>(clipped_resnet.fc.bias.grad[:<span class="hljs-number">25</span>])<br><br><span class="hljs-comment"># 输出</span><br><span class="hljs-comment"># tensor([-0.0010, -0.0047, -0.0010, -0.0009, -0.0015,  0.0027,  0.0017, -0.0023,</span><br><span class="hljs-comment">#          0.0051, -0.0007, -0.0057, -0.0010, -0.0039, -0.0100, -0.0018,  0.0062,</span><br><span class="hljs-comment">#          0.0034, -0.0010,  0.0052,  0.0021,  0.0010,  0.0017, -0.0100,  0.0021,</span><br><span class="hljs-comment">#          0.0020])</span><br></code></pre></td></tr></table></figure></li></ul><h5 id="1-3一些常见的Hook函数："><a href="#1-3一些常见的Hook函数：" class="headerlink" title="1.3一些常见的Hook函数："></a>1.3<strong>一些常见的Hook函数</strong>：</h5><ul><li><p><code>register_forward_hook</code> 是 PyTorch 中用于在神经网络的<strong>前向传播过程中</strong>注册钩子的一个函数。这个钩子函数会在模块执行其 <code>forward</code> 方法时被调用，可以用来检查或修改中间输出。</p><p><code>module.register_forward_hook(hook)</code>:</p><p><img src="/2024/05/22/Pytorch/image-20240522161226987.png" alt="register_forward_hook"></p></li><li><p><code>register_hook</code> 是 PyTorch 中用于在神经网络的<strong>反向传播过程中</strong>注册钩子的函数。这个钩子函数会在张量的梯度计算过程中被调用，主要用于调试和修改梯度。</p><p><code>tensor.register_hook(hook)</code>:</p><p><img src="/2024/05/22/Pytorch/image-20240522161159145.png" alt="register_hook"></p></li><li><p>回调「Hook」函数注册</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 三个全局变量，dict类型，存储回调函数（即hook），用于net中的所有module</span><br><span class="hljs-comment"># 用于输入输出tensor</span><br>_global_buffer_registration_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] = OrderedDict()<br><span class="hljs-comment"># 用于module定义</span><br>_global_module_registration_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] = OrderedDict()<br><span class="hljs-comment"># 用于模型参数</span><br>_global_parameter_registration_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] = OrderedDict()<br><br><br><span class="hljs-string">r&quot;&quot;&quot;This tracks hooks common to all modules that are executed before/after</span><br><span class="hljs-string">calling forward and backward. This is global state used for debugging/profiling</span><br><span class="hljs-string">purposes&quot;&quot;&quot;</span> <br><span class="hljs-comment"># 用于在module的forward和backward接口前后注册回调函数，例如dump出每个op的输入输出结果</span><br>_global_backward_pre_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] = OrderedDict()<br>_global_backward_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] = OrderedDict()<br>_global_is_full_backward_hook: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span><br>_global_forward_pre_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] = OrderedDict()<br>_global_forward_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] = OrderedDict()<br><br><span class="hljs-comment"># 提供reg接口在完成回调函数注册</span><br>register_module_buffer_registration_hook()<br>register_module_module_registration_hook()<br>register_module_parameter_registration_hook()<br><br>register_module_forward_pre_hook()<br>register_module_forward_hook()<br>register_module_backward_hook()<br>register_module_full_backward_pre_hook()<br>register_module_full_backward_hook()<br></code></pre></td></tr></table></figure></li></ul><h5 id="1-4-Module成员变量分析"><a href="#1-4-Module成员变量分析" class="headerlink" title="1.4 Module成员变量分析"></a>1.4 Module成员变量分析</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 版本号，一个内部使用的属性，用于跟踪模块的版本。这一机制主要用于在序列化（serialization）和反序列化（deserialization）过程中管理模型的兼容性。</span><br>_version: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span> <br><span class="hljs-comment"># 一个布尔值，表示模块是否处于训练模式。可以通过 model.train() 和 model.eval() 方法切换。</span><br>training: <span class="hljs-built_in">bool</span> <br><span class="hljs-comment"># 存储模块的所有参数（Parameter 对象），类型为 OrderedDict，如conv的weight、bias等</span><br>_parameters: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Optional</span>[Parameter]] <br><span class="hljs-comment"># 存储模块中的所有缓冲区（Tensor 对象），类型为 OrderedDict。缓冲区是模型状态的一部分，但不是参数，比如 BatchNorm 的running mean 和 running variance。</span><br>_buffers: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Optional</span>[Tensor]] <br><span class="hljs-comment"># 存储模块的子模块，类型为 OrderedDict。每个子模块在模型中都有一个唯一的名称。</span><br>_modules: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Optional</span>[<span class="hljs-string">&#x27;Module&#x27;</span>]]<br><br><br><span class="hljs-comment"># 存储反向传播前的钩子，类型为 OrderedDict。这些钩子在反向传播前的过程中被调用。</span><br>_backward_pre_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>] <br><span class="hljs-comment"># 存储反向传播钩子，类型为 OrderedDict。这些钩子在反向传播过程中被调用。</span><br>_backward_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>]<br><span class="hljs-comment"># 存储前向传播钩子，类型为 OrderedDict。这些钩子在前向传播过程中被调用。</span><br>_forward_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>]<br><br><br><span class="hljs-comment"># 存储 state_dict 钩子，类型为 OrderedDict。这些钩子在调用 state_dict 时被调用。</span><br>_state_dict_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>]  // 模型加载时，op的参数加载相关的回调函数<br>_load_state_dict_pre_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>]<br>_state_dict_pre_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>]<br>_load_state_dict_post_hooks: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Callable</span>]<br><br></code></pre></td></tr></table></figure><h5 id="1-5-Module方法分析"><a href="#1-5-Module方法分析" class="headerlink" title="1.5 Module方法分析"></a>1.5 Module方法分析</h5>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fatser R-CNN Source Code</title>
    <link href="/2024/05/21/Faster-RCNN_SourceCode/"/>
    <url>/2024/05/21/Faster-RCNN_SourceCode/</url>
    
    <content type="html"><![CDATA[<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="/2024/05/21/Faster-RCNN_SourceCode/v2-4c23436f431a535e2cc2e9919b3ca10f_r.jpg" alt="Faster RCNN model"></p><h3 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h3><p>1. </p><h3 id="PyTorch代码学习"><a href="#PyTorch代码学习" class="headerlink" title="PyTorch代码学习"></a>PyTorch代码学习</h3><ol><li><p><code>torch._assert(False, &quot;XXX&quot;)</code></p><blockquote><p>assert函数叫做断言函数，它可以用于判断某个表达式的值，若是该值为真，那么程序就能够继续往下执行; 反之，会报出AssertionError错误。</p></blockquote></li><li><p><code>if isinstance(boxes, torch.Tensor)</code></p><blockquote><p>判断两个类型是否相同</p></blockquote></li></ol>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Object Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RCNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>YOLO Series</title>
    <link href="/2024/05/20/Yolo/"/>
    <url>/2024/05/20/Yolo/</url>
    
    <content type="html"><![CDATA[<h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h3>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Object Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>YOLO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PaddlePaddle-CPPD</title>
    <link href="/2024/05/19/PaddlePaddle_CPPD/"/>
    <url>/2024/05/19/PaddlePaddle_CPPD/</url>
    
    <content type="html"><![CDATA[<ul><li><h4 id="准备好训练数据集"><a href="#准备好训练数据集" class="headerlink" title="准备好训练数据集"></a>准备好训练数据集</h4><p>以PARSeq（lmdb数据格式）为例，在项目文件中如下：</p><p><img src="/2024/05/19/PaddlePaddle_CPPD/image-20240519204701863.png" alt="image-20240519204701863"></p></li><li><h4 id="报错解决"><a href="#报错解决" class="headerlink" title="报错解决"></a>报错解决</h4><ol><li><p>CUDA和CUDNN的配置问题</p><p><img src="/2024/05/19/PaddlePaddle_CPPD/image-20240519174249876.png" alt="image-20240519174249876"></p><p><img src="/2024/05/19/PaddlePaddle_CPPD/image-20240519202939423.png" alt="image-20240519202939423">解决办法：</p><blockquote><ol><li>下载正确CUDA版本的CUDA Toolkit</li><li>下载正确CUDA版本的cuDNN，将dll动态链接库文件复制到CUDA Toolkit中</li></ol></blockquote></li><li><p>其他报错解决</p></li></ol></li><li><h4 id="成功训练"><a href="#成功训练" class="headerlink" title="成功训练"></a>成功训练</h4><p><img src="/2024/05/19/PaddlePaddle_CPPD/image-20240519204959939.png" alt="image-20240519204959939"></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Optical Character Recognition</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CPPD</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker</title>
    <link href="/2024/05/15/Docker/"/>
    <url>/2024/05/15/Docker/</url>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><hr><p>Docker是一个用于<strong>构建(build)</strong>,<strong>运行(run)</strong>,<strong>传送(share)</strong> 应用程序的平台</p><ol><li><strong>将应用程序打包为一个个的集装箱</strong>，小鲸鱼(Docker)就会帮我们将它们运送到任何需要的地方</li></ol><p><img src="/2024/05/15/Docker/docker.png" alt="集装箱"></p><ol start="2"><li><strong>每个集装箱将下列文件打包在一起</strong>（应用程序和它运行时所需要的各种依赖包、第三方软件库、配置文件等），以便在任何环境中都可以正确的运行</li></ol><p><img src="/2024/05/15/Docker/image-20240515112551349.png" alt="集装箱内文件"></p><h3 id="为什么要使用Docker"><a href="#为什么要使用Docker" class="headerlink" title="为什么要使用Docker"></a>为什么要使用Docker</h3><hr><p>比如我们写了一个网站：</p><ol><li><p>前端使用Vue搭建界面</p></li><li><p>后端使用SpringBoot微服务框架来提供各种服务和接口</p></li><li><p>使用MySQL数据库来存储数据</p></li></ol><ul><li><strong>如果没有使用Docker</strong>，我们在生产环境和测试环境中都需要进行各种配置：</li></ul><p><img src="/2024/05/15/Docker/image-20240515113511454.png" alt="开发环境和测试环境"></p><ul><li><strong>如果有了Docker</strong>，我们可以将这些（前端、后端、数据库等）打包成一个个的集装箱，只要在开发环境中运行成功了，在其他环境中也一定可以成功。</li></ul><p><img src="/2024/05/15/Docker/image-20240515113744151.png" alt="开发环境和测试环境"></p><h3 id="Docker和虚拟机的区别"><a href="#Docker和虚拟机的区别" class="headerlink" title="Docker和虚拟机的区别"></a>Docker和虚拟机的区别</h3><hr><ol><li>架构差异：虚拟机是基于<strong>虚拟化技术</strong>（<strong>hypervisor</strong>）实现的，创建一个完整的虚拟硬件环境，模拟一台计算机，包括处理器、内存、硬盘等设备。而Docker则是基于<strong>容器化技术</strong>（<strong>Containerization</strong>），使用Docker引擎访问宿主机操作系统，将应用程序打包到容器中。因此，Docker的架构更加轻量级，启动速度也更快。</li><li>隔离原理：虚拟机通常隔离整个操作系统，进程无法直接访问宿主机资源和数据，需要通过网络或共享文件夹等方式交互。Docker使用Linux内核的namespace和cgroups功能，实现对不同容器中进程的隔离，允许它们共享宿主机资源同时独立运行。</li></ol><h3 id="基本原理和概念"><a href="#基本原理和概念" class="headerlink" title="基本原理和概念"></a>基本原理和概念</h3><hr><p><strong>Docker中的镜像、容器和仓库</strong></p><ul><li><p>镜像和容器的关系就像Java中的类和实例的关系一样：</p><ol><li><strong>镜像是一个只读的模版</strong>，它可以用来创建容器</li><li><strong>容器是Docker的运行实例</strong>，它提供了一个独立的可移植的环境，可以在这个环境中运行应用程序</li></ol></li><li><p>Docker仓库是用来存储Docker镜像的地方，最流行和最常用的仓库就是Dockerhub，实现镜像的共享和复用</p></li></ul><h3 id="Docker的安装"><a href="#Docker的安装" class="headerlink" title="Docker的安装"></a>Docker的安装</h3><hr><p><img src="/2024/05/15/Docker/image-20240515122544372.png" alt="Docker Daemon"></p><ol><li><p>Docker采用Client-Server架构模式</p><ul><li>Docker Client和Docker Daemon之间通过Socket或者RESTful API进行通信</li><li>Docker Daemon就是服务端的守护进程，他负责管理Docker的各种资源</li><li>Docker Client负责向Docker Daemon发送请求，Docker Daemon接收到请求后进行处理，然后将结果返回给Client。因此我们在终端中输入的各种命令，都是Client发送给Docker Daemon的</li></ul></li><li><p>容器化和Dockerfile</p><p><strong>容器化</strong>：顾名思义就是将应用程序打包为容器，在容器中运行应用程序的过程</p><p><strong>Dockerfile</strong>：是一个文本文件，里面包含了一条条的指令，用来告诉Docker如何来构建镜像，这个镜像中包含了我们应用程序执行的所有命令，也就是上边提到的依赖、第三方软件包、配置文件等</p><p>这个过程简单来说可以分成三个步骤:</p><ul><li>首先需要创建一个Dockerfile，来告诉Docker构建应用程序镜像所需要的步骤和配置</li><li>使用Dockerfile来构建镜像</li><li>使用镜像创建和运行容器</li></ul></li></ol><h3 id="实践环节"><a href="#实践环节" class="headerlink" title="实践环节"></a>实践环节</h3><hr><p><img src="/2024/05/15/Docker/image-20240515164517046.png" alt="测试程序_python: 在这里使用默认的ENTRYPOINT[&quot;top&quot;, &quot;-b&quot;]在build时会报错，先删除做测试"></p><hr><p><img src="/2024/05/15/Docker/image-20240515164536314.png" alt="打包为hello-docker镜像"></p><hr><p><img src="/2024/05/15/Docker/image-20240515164551800.png" alt="docker 镜像创建成功"></p>]]></content>
    
    
    <categories>
      
      <category>Develop</category>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>R-CNN Series</title>
    <link href="/2024/05/15/RCNN/"/>
    <url>/2024/05/15/RCNN/</url>
    
    <content type="html"><![CDATA[<h3 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h3><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs arcade">R-CNN源于<span class="hljs-number">2014</span>年伯克利大学的这篇论文<br>《Rich <span class="hljs-built_in">feature</span> hierarchies <span class="hljs-keyword">for</span> accurate object detection and semantic segmentation》<br>其架构和模型训练参数等借鉴了AlexNet，也和同时期的Overfeat也有很多共同之处。成为目标检测领域尤其是two stage模式的开山鼻祖。<br></code></pre></td></tr></table></figure><hr><p><img src="/2024/05/15/RCNN/R-CNN.png" alt="R-CNN网络结构"></p><ul><li><h4 id="R-CNN-算法流程分四个步骤："><a href="#R-CNN-算法流程分四个步骤：" class="headerlink" title="R-CNN 算法流程分四个步骤："></a>R-CNN 算法流程分四个步骤：</h4><ol><li><p>一张图像生成1k～2k个候选区域（使用 <strong><u>Selective Search</u></strong> 方法)[<a href="https://zhuanlan.zhihu.com/p/485727819]">https://zhuanlan.zhihu.com/p/485727819]</a></p><blockquote><p>候选区域生成 (Selective Search) 的主要思想：图像中物体可能存在的区域应该是有某些相似性或者连续性区域的。</p><ol><li>首先，对输入图像进行<strong>分割算法</strong>产生许多小的子区域。</li><li>其次，根据这些子区域之间相似性(相似性标准主要有颜色、纹理、大小等等)进行<strong>区域合并</strong>，不断的进行区域迭代合并。每次迭代过程中对这些合并的子区域做bounding boxes(外切矩形)，这些子区域外切矩形就是通常所说的候选框。</li></ol><p><img src="/2024/05/15/RCNN/SS%E7%AE%97%E6%B3%95.png" alt="SS算法"></p></blockquote></li><li><p>对每个候选区域，使用深度网络（图片分类网络）提取特征</p><blockquote><ol><li>通过SS算法可以在一张图片中生成大概2000个候选区域，将候选区域送到CNN网络之前先进行<strong>resize处理</strong>，将2000候选区域缩放到 227 × 227 (原文是不管候选框多大都resize到 227 × 227)</li><li>接着将候选区域输入事先训练好的AlexNet &#x2F; VGG CNN网络<strong>获取4096维的特征</strong>，得到2000×4096维矩阵。</li></ol></blockquote></li><li><p>特征送入每一类SVM分类器，判断是否属于该类</p><blockquote><ol><li><p>将<code>2000×4096</code>维特征与<code>20</code>个<strong>SVM</strong>组成的权值矩阵<code>4096×20</code>相乘，获得<code>2000×20</code>维矩阵表示每个建议框是某个目标类别的得分</p><p><img src="/2024/05/15/RCNN/SVM.png" alt="SVM分类器计算矩阵"></p></li><li><p>分别对上述<code>2000×20</code>维矩阵中每一列即每一类进行**NSM(非极大值抑制)**剔除重叠建议框，得到该列即该类中得分最高的一些建议框。</p></li></ol></blockquote></li><li><p>使用回归器精细修正候选框位置。（使用 Selective Search 算法得到的候选框并不是框得那么准）</p><blockquote><ol><li>对NMS处理后剩余的建议框进一步筛选</li><li>分别用20个<strong>回归器</strong>对上述20个类别中剩余的建议框进行回归操作，最终得到每个类别的修正后的得分最高的bounding box</li></ol><p>(原文)：</p><ol><li>保留与真实目标比IoU大于某一阈值的预测框，不满足的直接删除</li><li>接着再分别使用20个回归器对剩余的预测框进行回归操作，最终得到每个类别的修正后得分最高的预测框。这里的实现方法跟上面的SVM分类差不多，依旧是对卷积神经网络输出的特征向量进行预测，利用每个边界框得到4096维特征向量来预测的。通过回归分类器之后会得到四个参数分别对应着目标建议框的中心点的x,y偏移量和目标边界框的宽高缩放因子。通过预测的四个值对得到的建议框进行调整得到最终的预测边界框。</li></ol></blockquote></li></ol></li><li><h4 id="RCNN存在的问题"><a href="#RCNN存在的问题" class="headerlink" title="RCNN存在的问题"></a>RCNN存在的问题</h4><ol><li>检测速度慢，测试一张图片约53s (CPU)。用Selective Search算法提取候选框用时约2秒，一张图像内候选框之间存在大量重叠，提取特征操作冗余。</li><li>训练速度慢，并且训练过程极其复杂。</li><li>训练所需空间大，对于SVM和bbox回归训练，需要从每个图像中的每个目标候选框提取特征，并写入磁盘。对于非常深的网络，如VGG16，从VOC07训练集上的5k图像上提取的特征需要数百GB的存储空间。</li></ol></li></ul><h3 id="SPPNet"><a href="#SPPNet" class="headerlink" title="SPPNet"></a>SPPNet</h3><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mathematica">由提出<span class="hljs-variable">ResNet</span>的何凯明在论文《<span class="hljs-variable">Spatial</span> <span class="hljs-built_in">Pyramid</span> <span class="hljs-variable">Pooling</span> <span class="hljs-variable">in</span> <span class="hljs-variable">Deep</span> <span class="hljs-variable">Convolutional</span> <span class="hljs-variable">Networks</span> <span class="hljs-variable">for</span> <span class="hljs-variable">Visual</span> <span class="hljs-variable">Recognition</span>》中提出<br>主要就是可以解决<span class="hljs-variable">CNN</span>输入需要固定尺寸的问题，而且在分类和目标检测中都可以得到比较好的效果<br></code></pre></td></tr></table></figure><hr><p><img src="/2024/05/15/RCNN/SPPNet.png" alt="SPPNet网络结构"></p><ul><li><h4 id="针对R-CNN的缺点："><a href="#针对R-CNN的缺点：" class="headerlink" title="针对R-CNN的缺点："></a>针对R-CNN的缺点：</h4><p>1).对每个候选区域提取特征。2).输入CNN的候选区域大小固定，要经过Resize。SPPNet做出了以下改进，<strong>具体内容看表格</strong>：</p><ol><li><p>SPPNet让SS算法得到<strong>候选区域与feature map直接映射</strong>，得到候选区域的映射特征向量（这是映射来的，不需要每个候选区域都再经过CNN的计算，极大的减少了计算量）。</p></li><li><p>SPPNet引入一种空间<u><strong>金字塔池化( spatial pyramid pooling，SPP)层</strong></u>以移除网络对固定尺寸的限制（不需要候选区域经过crop&#x2F;wrap等操作变换成固定大小的图像）。</p><p><img src="/2024/05/15/RCNN/feature_map_regions.png" alt="image regions vs feature map regions"></p><table><thead><tr><th>R-CNN模型</th><th>SPPNet模型</th></tr></thead><tbody><tr><td>1.R-CNN让每个候选区域经过crop&#x2F;wrap等操作变换成固定大小的图像.  2. 固定大小的每个候选区域塞给CNN 传给后面的层做训练回归、分类操作</td><td>1.SPPNet把全图塞给CNN得到全图的feature map.  2.让SS算法得到候选区域与feature map直接映射，得到候选区域的映射特征向量(这是映射来的，不需要过CNN). 3.映射过来的特征向量大小不固定，所以这些特征向量塞给SPP层(空间金字塔变换层)，SPP层接收任何大小的输入，输出固定大小的特征向量，再塞给FC层. 4.经过映射+SPP转换，简化了计算，速度&#x2F;精确度也上去了</td></tr></tbody></table></li></ol></li><li><h4 id="两个关键问题："><a href="#两个关键问题：" class="headerlink" title="两个关键问题："></a>两个关键问题：</h4><ul><li><p><input checked disabled type="checkbox"> <strong>SPP层怎么可以接收任意大小的输入，输出固定的向量？</strong></p><p>spp layer会将<strong>每个候选区域分成1x1，2x2，4x4三张子图</strong>，对每个子图的每个区域作<strong>max pooling</strong>，得出的特征再<strong>连接</strong>到一起就是(16+4+1)x256&#x3D;21x256&#x3D;5376维向量，接着给全连接层做进一步处理。</p></li></ul><ul><li><p><input checked disabled type="checkbox"> <strong>SPPNet怎么就能把候选区域从全图的feature map 直接得到特征向量？</strong></p><p><img src="/2024/05/15/RCNN/Feature_map.png" alt="feature map">整个映射过程有具体的公式：$(x,y)&#x3D;(S<em>x’,S</em>y’)$ $(x’,y’)&#x3D;([x&#x2F;S]+1,[y&#x2F;S]+1)$（左上角+，右下角-)</p><p>其中 S 就是CNN中<strong>所有的strides的乘积</strong>，包含了池化、卷积的stride。论文中使用S的计算出来为2x2x2x2&#x3D;16,在ZF-5结构中。</p></li></ul></li></ul><h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h3><figure class="highlight objectivec"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs objectivec">Fast R-<span class="hljs-built_in">CNN</span> 是作者Ross Girshick继 R-<span class="hljs-built_in">CNN</span> 后的又一力作，论文名就叫做 Fast R-<span class="hljs-built_in">CNN</span>，<span class="hljs-number">2015</span>年发表。<br>同样使用VGG16作为网络的backbone，与 R-<span class="hljs-built_in">CNN</span> 相比训练时间快了 <span class="hljs-number">9</span> 倍，测试推理时间快了 <span class="hljs-number">213</span> 倍，准确率从 <span class="hljs-number">62</span>% 提升至了 <span class="hljs-number">66</span>%（在 Pascal VOC 数据集上）<br></code></pre></td></tr></table></figure><p>参考文章：</p><p><a href="https://zhuanlan.zhihu.com/p/165324194">Fast R-CNN</a></p><p><a href="https://www.cnblogs.com/yymn/articles/13629478.html">ROI Pooling</a></p><p><a href="https://www.jianshu.com/p/670a3e42107d">RoI Pooling及其改进</a></p><hr><p><img src="/2024/05/15/RCNN/Fast_R-CNN.png" alt="Fast R-CNN网络结构"></p><ul><li><h4 id="Fast-R-CNN-算法流程分四个步骤："><a href="#Fast-R-CNN-算法流程分四个步骤：" class="headerlink" title="Fast R-CNN 算法流程分四个步骤："></a>Fast R-CNN 算法流程分四个步骤：</h4><ol><li><p>一张图像生成1k～2k个候选区域（使用 Selective Search 方法）</p></li><li><p>将图像输入网络得到相应的特征图，将 Selective Search 算法生成的候选框投影到特征图上获得相应的特征矩阵</p></li><li><p>将每个特征矩阵通过 ROI pooling层缩放为$ 7 \times 7$大小的特征图  </p><blockquote><p>ROI「region of interests」：指的是矩形框框出的区域，可能是有目标的也可能没目标，概念上等价于proposal region。</p><p>ROI Pooling层的具体做法是：</p><p>对候选框所对应的特征矩阵，将其划分为7*7，也就是49等份。划分之后，对每一个区域做一个最大池化下采样操作，也就是MaxPooling操作。如此对49等分的候选区域操作，便得到了一个7*7的特征矩阵。</p><p>也就是说，无论候选框的特征矩阵是怎么样的尺寸，都被缩放到一个7*7的大小，这样就可以不去限制输入图像的尺寸了。</p></blockquote></li><li><p>接着将特征图展平通过一系列全连接层获得预测结果</p><blockquote><p>最后并联了两个全连接层分别对分类和bbox进行预测。</p><p>分类结点数为 N+1，因为需要增加上背景。bbox预测的全连接层则是$4*(N+1)$个结点，对每一类都要预测出来边界框回归参数。</p><p><img src="/2024/05/15/RCNN/%E8%BE%B9%E7%95%8C%E6%A1%86%E5%9B%9E%E5%BD%92%E5%99%A8.png" alt="边界框回归器"></p><p><img src="/2024/05/15/RCNN/image-20240519120214206.png" alt="边界框回归器"></p></blockquote></li></ol></li><li><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>Fast RCNN将分类与回归做到了一个网络里面，因此损失函数必定是多任务的：<br>$$<br>L(p,u,t^u,v)&#x3D;L_{cls}(p,u)+\lambda[u\geq1]L_{loc}(t^u,v)<br>$$</p><ol><li><p><strong>分类损失</strong>：$L_{cls}(p,u) $ </p><blockquote><p>使用交叉熵损失，<code>p</code>为预测结果的向量表示，<code>u</code>为真实类别的标签数据。</p></blockquote></li><li><p><strong>bbox回归器损失</strong>：$\lambda[u&gt;&#x3D;1]L_{loc}(t^u,v)$ ：</p><blockquote><ul><li><p>$\lambda$: 用于调节两部分损失函数的比例，一般取1</p></li><li><p>$[u\geq1]$: 是因为将<code>u</code>定义成了真实类别的索引，而且将background这一类定义成了0。所以如果标签是0的时候，这部分是不需要计算bbox的损失函数的，因为background不需要bbox。</p></li><li><p>$L_{loc}(t^u,v)$<br>$$<br>L_{loc}(t^u,v)&#x3D;\sum_{i\in{x,y,w,h}}smooth_{L1}(t_i^u-v_i)<br>$$</p><p>$$<br>smooth_{L1}(x)&#x3D;<br>\begin{cases}<br>0.5x^2 &amp;if\space|x|&lt;1\<br>|x|-0.5 &amp;otherwise<br>\end{cases}<br>$$</p><p>这一部分就是把预测框与groud truth中的x, y, w, h都单独拿出来进行相减（实际上就是L1的损失函数），然后计算smooth函数($Smooth_{L1}$)，最后把这四个的smooth进行相加就是bbox回归器的损失函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">smooth_l1</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    <span class="hljs-comment"># y_true和y_pred是特征位置的真实值和预测值</span><br>    abs_diff = K.<span class="hljs-built_in">abs</span>(y_true - y_pred)<br>    less_than_one = K.cast(K.less(abs_diff, <span class="hljs-number">1.0</span>), <span class="hljs-string">&quot;float32&quot;</span>)<br>    loss = (<span class="hljs-number">0.5</span> * abs_diff**<span class="hljs-number">2</span>) * less_than_one + (abs_diff - <span class="hljs-number">0.5</span>) * (<span class="hljs-number">1</span> - less_than_one)<br>    <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure></li></ul></blockquote></li></ol></li></ul><h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><figure class="highlight objectivec"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs objectivec">Faster R-<span class="hljs-built_in">CNN</span> 是作者 Ross Girshick 继 Fast R-<span class="hljs-built_in">CNN</span> 后的又一力作，<br>同样使用 VGG16 作为 backbone，推理速度在 GPU 上达到 <span class="hljs-number">5</span>fps（每秒检测五张图，包括候选区域生成），准确度也有一定的进步。核心在于 RPN 区域生成网络（Region Proposal Network）。<br></code></pre></td></tr></table></figure><p><a href="https://www.zhihu.com/tardis/bd/art/31426458">这篇文章</a>介绍Faster R-CNN写的非常非常详细！</p><hr><p><img src="/2024/05/15/RCNN/ff2cd606a307b927701694ec05d0f599.png" alt="Faster R-CNN网络结构"></p><ul><li><h4 id="Faster-R-CNN-算法流程分三个步骤："><a href="#Faster-R-CNN-算法流程分三个步骤：" class="headerlink" title="Faster R-CNN 算法流程分三个步骤："></a>Faster R-CNN 算法流程分三个步骤：</h4><ol><li>将图像输入网络得到相应的特征图-feature map</li><li>使用RPN网络（Region Proposal Network）生成候选框，将 RPN 生成的候选框投影到特征图上获得相应的特征矩阵</li><li>将每个特征矩阵通过 ROI pooling 层缩放为$7 \times 7$大小的特征图，接着将特征图展平通过一系列全连接层获得预测结果</li></ol></li><li><h4 id="RPN-网络结构"><a href="#RPN-网络结构" class="headerlink" title="RPN 网络结构"></a><font color="red">RPN 网络结构</font></h4><blockquote><p><strong>滑动窗口生成anchors -&gt; softmax分类器提取positvie anchors -&gt; bbox reg回归positive anchors -&gt; Proposal Layer生成proposals</strong></p></blockquote><p><img src="/2024/05/15/RCNN/RPN.png" alt="Faster R-CNN细节"></p><p>流程细节：</p><ol><li><p>在经过backbone之后生成Feature Map大小为：<strong>H*W*N</strong></p><blockquote><p>N是根据使用backbone的通道数来定的，比如VGG16为512个通道，而使用ZF网络则是256个通道。</p></blockquote></li><li><p>对特征图做了3x3卷积，但输出层数保持不变（<strong>N</strong>），相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？反正我没测试）</p></li><li><p>在上一步的结果上使用滑动窗口，每滑动到一个位置生成一个一维的向量， 在向量的基础上通过两个全连接层去输出目标概率（背景&#x2F;非背景）和边界框回归参数。</p><blockquote><p>256-d是指使用ZF model作为backbone，最终每个点都用一个256维的向量表示，即<code>N=256</code>。</p></blockquote></li><li><p>1*1卷积+Cls layer + Reg layer：</p><blockquote><p>在<strong>分类任务「Cls layer」</strong>中，经过1*1卷积后，输出图像大小为：<strong>H*W*18</strong>，正好对应9个anchor boxes的2分类结果。为何要在softmax前后都接一个reshape layer？其实只是为了便于softmax分类，具体原因这就要从caffe的实现形式说起。</p><p>在<strong>回归任务「Reg layer」</strong>，经过1*1卷积后，输出图像大小为：<strong>H*W*36</strong>，正好对应9个anchor boxes的边界框。</p><p>2k 中的 k 指的是 k 个 anchor boxes，<strong>2是指为背景的概率和为前景的概率</strong>。</p><p>4k 中的 k 指的是 k 个 anchor boxes，<strong>4是指每个 anchor 有 4 个<font color="red">边界框回归参数</font>。</strong></p></blockquote><p><img src="/2024/05/15/RCNN/u=1361402012,3659437005&fm=253&fmt=auto&app=138&f=JPEG-20240520150557582.jpeg" alt="RPN"></p></li><li><p><font color="red">Proposal层</font></p><p>Proposal层共有三个输入：分类器结果【1，2*9，H，W】、边界框回归参数【1，4*9，H，W】、img_info</p><p>Proposal Layer forward（caffe layer的前向函数）按照以下顺序依次处理：</p><ol><li><p>生成anchors，利用$d_x(A),d_y(A),d_w(A),d_h(a)$对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致)</p></li><li><p>按照输入的positive softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的positive anchors</p></li><li><p>判定超出图像边界的positive anchors为图像边界，防止后续roi pooling时proposal超出图像边界</p></li><li><p>剔除尺寸非常小的positive anchors</p></li><li><p>对剩余的positive anchors进行NMS（nonmaximum suppression）</p></li><li><p>对应的bbox reg的结果作为proposal输出</p><blockquote><p>由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的，这点在后续网络中有用。另外我认为，严格意义上的检测应该到此就结束了，后续部分应该属于识别了。</p></blockquote></li></ol></li></ol></li><li><h4 id="ROI层"><a href="#ROI层" class="headerlink" title="ROI层"></a><font color="red">ROI层</font></h4><p>同Fast R-CNN中的ROI层作用和用法</p></li><li><h4 id="FC层"><a href="#FC层" class="headerlink" title="FC层"></a>FC层</h4><p>同Fast R-CNN中的FC层作用和用法</p></li><li><h4 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h4><p><strong>RPN的损失</strong>也分为两个部分：分类损失和边界框回归损失<br>$$<br>L({p_i},{t_i})&#x3D;\frac{1}{N_{cls}}\sum_{i}L_{cls}(p_i,p_i^*)+\lambda\frac{1}{N_{reg}}\sum_{i}{p_i^*}L_{reg}(t_i,t_i^*)<br>$$</p></li></ul><h3 id="补充材料"><a href="#补充材料" class="headerlink" title="补充材料"></a>补充材料</h3><ul><li><p>Faster R-CNN</p><ol><li><p>实际上生成的那么多 anchors 并不是每个都用来训练 RPN 网络。对于每张图片我们从上万个 anchor 当中采样 256 个 anchor，这些 anchor 由正样本和负样本 1:1 组成的。如果正样本不足 128，就用负样本进行填充。两种定义正样本的方式：（1）anchor 与 ground-truth 的 iou 超过 0.7，（2）某个 anchor 与 ground-truth 拥有最大的 iou。负样本是与所有的 ground-truth 的 iou 都小于 0.3 的。</p></li><li><p>感受野</p><p><img src="/2024/05/15/RCNN/v2-6193e85eb99c691c051ef55840154f9e_r.jpg" alt="感受野"></p></li></ol></li></ul><p>​</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Object Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RCNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
